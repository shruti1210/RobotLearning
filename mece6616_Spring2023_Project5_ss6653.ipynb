{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp8PVN5sJ9pd"
      },
      "source": [
        "# ***Important***\n",
        "\n",
        "**Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/172081/pages/assignment-instructions) page on Courseworks2 to learn the workflow for completing this project.**\n",
        "\n",
        "- *Apart from the link to your notebook, you are also required to submit `q_network.pth` of Part 1 and `ppo_network.zip` (model checkpoints are loaded and saved by stable_baselines3 as zip files) of Part 2 to Coursework. You should put the link to your notebook in the comment entry.*\n",
        "- *Please name the revision you want us to grade \"Grade me\". We will only grade the revision that is correctly named. Late days will be applied according to this named revision.*\n",
        "- *Please make sure that \"anyone in LionMail with the link\" has Edit permissions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# Project Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPIiNSZ8hb8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e42308-48c5-450f-ad69-93c1bba5159a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp23_project5'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 43 (delta 19), reused 23 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), 13.83 KiB | 786.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp23_project3' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp23_project5.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ise8RAQhhs3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fe567a-d65b-4b32-e389-0387aff11780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/mecs6616_sp23_project5/arm_dynamics_base.py' -> '/content/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp23_project5/arm_dynamics.py' -> '/content/arm_dynamics.py'\n",
            "'/content/mecs6616_sp23_project5/arm_env.py' -> '/content/arm_env.py'\n",
            "'/content/mecs6616_sp23_project5/geometry.py' -> '/content/geometry.py'\n",
            "'/content/mecs6616_sp23_project5/render.py' -> '/content/render.py'\n",
            "'/content/mecs6616_sp23_project5/robot.py' -> '/content/robot.py'\n",
            "'/content/mecs6616_sp23_project5/score.py' -> '/content/score.py'\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# copy all needed files into the working directory. This is simply to make accessing files easier\n",
        "!cp -av /content/mecs6616_sp23_project5/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed setuptools-65.5.0\" at the end.\n",
        "\n",
        "!pip install setuptools==65.5.0"
      ],
      "metadata": {
        "id": "gYlcOXqbosEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144c8955-4fc9-4e8b-e039-95ff75e01ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==65.5.0\n",
            "  Downloading setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 65.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-65.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed gym-0.21.0 stable-baselines3-1.5.0\" at the end.\n",
        "\n",
        "!pip install gym==0.21.0 stable-baselines3==1.5.0"
      ],
      "metadata": {
        "id": "RChyEFYYqLGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a05b32-f224-419b-c9ca-883896402839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stable-baselines3==1.5.0\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.7/177.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->stable-baselines3==1.5.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->stable-baselines3==1.5.0) (16.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==1.5.0) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.5.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->stable-baselines3==1.5.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->stable-baselines3==1.5.0) (1.3.0)\n",
            "Building wheels for collected packages: gym\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gym\n",
            "Failed to build gym\n",
            "Installing collected packages: gym, stable-baselines3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed gym-0.21.0 stable-baselines3-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement DQN\n",
        "\n",
        "For this part, you will implement DQN from scratch. You SHOULD NOT use any RL libraries."
      ],
      "metadata": {
        "id": "-KNg9fzU5Un7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JvzRuwNsYz"
      },
      "source": [
        "## Starter Code Explanation\n",
        "In addition to code you are already familiar with from the previous project (i.e. arm dynamics, etc.) we are providing an \"Environment\" in the `ArmEnv` class. The environment \"wraps around\" the arm dynamics and provides the key functions that an RL algorithm expects: reset(...) and step(...). The implementation of `ArmEnv` follows the [OpenAI Gym](https://www.gymlibrary.dev/api/core/) API standard. It is a standard that is accepeted by many RL libraries and allows for our problem to be easily solved with various RL libraries. Take a moment to familiarize yourself with these functions! See [here](https://www.gymlibrary.dev/api/core/) for more information on the definition of the reset(...) and step(...) functions.\n",
        "\n",
        "Important notes:\n",
        "\n",
        "* The ArmEnv expects an action similar to the one used previously: a vector with a torque for every arm joint. Thus, the native action space for this environment is high-dimensional, and continuous. DQN will require an action space that is 1-dimensional and discrete. You will need to convert between these. For example, you can have an action space of [0, 1, 2,] where each number just represents the identity of an action candidate, and a conversion dictionary {0: [-0.1, -0.1], 1: [0.1, 0.1], 2: [0, 0]}. Then, when the Q network output an action 1, it will be converted into [0.1, 0.1] and used by the environment. Note that this is just an example method to implement the conversion and you do not have to follow the same procedure.\n",
        "* The observation provided by the environment will comprise the same state vector as before, to which we append the current position of the end_effector and the goal for the end-effector. Since your policy must learn to reach arbitrary goals, the goal must be provided as part of the observation. So the observation will consist of 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal.\n",
        "* The maximum episode length of the environment is 200 steps. Each step is simulated for 0.01 second. This should be used for both training and testing.\n",
        "* The reward function of this environment is by default r(s, a) = - dist(pos_ee, goal)^2 where represents the negative square of L2 distance between the current position of the end-effector and the goal position."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arm Environment Example\n",
        "You are encouraged to view the `arm_env.py` file to understand the `random_goal()`, `reset()` and `step()`  functions but do not modify the file.\n",
        "\n",
        "The `env.reset()` method, will reset the arm in the vertically downwards position and set a new random goal by calling the `random_goal()` method. By understanding how the goals are set you could guide your training in that direction. You can also provide your own goal as a (2,1) array to the reset function as an argument. This could come handy later when training the model.\n",
        "\n",
        "The `env.step()` function takes an action as a (2,1) shaped array and outputs the next observation, reward, done and info. `info` is a dictionary with pos_ee and vel_ee values. This can come handy if you attempt to do some reward engineering.\n",
        "\n",
        "The cell below provides an example of random policy interacting with the ArmEnv for 50 steps (0.5 seconds)"
      ],
      "metadata": {
        "id": "gw8H0PZcSv7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "80414257-92b6-41e9-cc54-388c1222f214",
        "id": "o6r9kJ5jpeds"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=True)\n",
        "\n",
        "# Passing our own defined goal to the reset function\n",
        "# goal = np.array([[0.5], [-1.5]])\n",
        "# obs = env.reset(goal)\n",
        "\n",
        "# Resetting the environment without the goal will set a random goal position\n",
        "obs = env.reset()\n",
        "\n",
        "for _ in range(50):\n",
        "  rand_action = np.random.uniform(-1.5, 1.5, (2,1))\n",
        "  obs, reward, done, info = env.step(rand_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmXTT_ngdqG"
      },
      "source": [
        "### QNetwork\n",
        "This class defines the architecture of your network. You must fill in the __init__(...) function which defines your network, and the forward(...) function which performs the forward pass.\n",
        "\n",
        "Your action space should be discrete, with whatever cardinality you decide. The size of the output layer of your Q-Network should thus be the same as the cardinality of your action space. When selecting an action, a policy must choose the one that has the highest estimated Q-value for the current state. As part of the QNetwork class, we are providing the function select_discrete_action(...) which does exactly that.\n",
        "\n",
        "The arm environment itself however expects a 2-dimensional, continuous action vector. Therefore, when it comes to send an action to the environment, you must provide the kind of action the environment expects. It is your job to determine how to convert between the discrete action space of your Q-Network and the continuous action space of the arm. You do this by filling in the action_discrete_to_continuous(...) function in your QNetwork. You can expect to call the step function of the environment like this:\n",
        "\n",
        "```\n",
        "self.env.step(self.q_network.action_discrete_to_continuous(discrete_action))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UyguLRKgf_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, env):\n",
        "    super(QNetwork, self).__init__()\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    self.layer1 = nn.Linear(env.observation_space.shape[0], 128)\n",
        "    self.layer2 = nn.Linear(128, 64)\n",
        "    self.layer3 = nn.Linear(64, 8*env.action_space.shape[0])\n",
        "    #---------------------------------------\n",
        "\n",
        "  def forward(self, x, device):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    x = torch.FloatTensor(x).to(device)\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = F.relu(self.layer2(x))\n",
        "    return self.layer3(x).squeeze(1)\n",
        "    #---------------------------------------\n",
        "\n",
        "\n",
        "  def select_discrete_action(self, obs, device):\n",
        "    # Put the observation through the network to estimate q values for all possible discrete actions\n",
        "    est_q_vals = self.forward(obs.reshape((1,) + obs.shape), device)\n",
        "    # Choose the discrete action with the highest estimated q value\n",
        "    discrete_action = torch.argmax(est_q_vals, dim=1).tolist()[0]\n",
        "    return discrete_action\n",
        "\n",
        "  def action_discrete_to_continuous(self, discrete_action):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    \"\"\"\n",
        "    trying to create a space to be reached by the arm,\n",
        "    with x and y ranging from 0.1 to 0.8 and -0.1 and -0.8 across four quadrants\n",
        "    Selected by trial and error\n",
        "    to increase the reach, values varied non-uniformly from first .1 to .2, and than .4 to .8\n",
        "    \"\"\"\n",
        "    mapping = {0: [0.1, 0.1], 1: [-0.1, -0.1], 2: [0.1, -0.1], 3: [-0.1, 0.1],\n",
        "               4: [0.2, 0.2], 5: [-0.2, -0.2], 6: [0.2, -0.2], 7: [-0.2, 0.2],\n",
        "               8: [0.4, 0.4], 9: [-0.4, -0.4], 10: [0.4, -0.4], 11: [-0.4, 0.4],\n",
        "              12: [0.8, 0.8], 13: [-0.8, -0.8], 14: [0.8, -0.8], 15: [-0.8, 0.8]}\n",
        "    return np.array(mapping[discrete_action]).reshape(2,-1)\n",
        "    #---------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with code to use the replay buffer in your RL implementation. You do not need to change the ReplayBuffer class.\n",
        "```\n",
        "rb = ReplayBuffer()\n",
        "```\n",
        "After creating a ReplayBuffer object you can add samples in the buffer using `put()`:\n",
        "```\n",
        "rb.put((obs, action, reward, next_obs, done))\n",
        "```\n",
        "Take random samples from the buffer using:\n",
        "```\n",
        "obs, actions, rewards, next_obses, dones = rb.sample(batch_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "IUjAeQcPdsGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7NytRAXtYkE"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append(done_mask)\n",
        "\n",
        "        return np.array(s_lst), np.array(a_lst), \\\n",
        "               np.array(r_lst), np.array(s_prime_lst), \\\n",
        "               np.array(done_mask_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainDQN\n",
        "Here, you must fill in the train(...) function that actually trains your network.\n",
        "\n",
        "We are providing a helper function called save_model(...) that will save the current Q-network. Use this as you see fit.\n",
        "\n",
        "To set one network equal to another one, you can use code like this:\n",
        "```\n",
        "target_network.load_state_dict(self.q_network.state_dict())\n",
        "```\n",
        "\n",
        "If you would like to be graded with a specific seed for the random number generators, make sure to change the default seed in the initialization of the TrainDQN class.\n",
        "\n",
        "The time taken to train the model will depend mainly on how big is your model architecture and the number of episodes you run the training for. As a reference, the time taken to train a model on 1500 episodes, which passed all evaluation metrics was about an hour.\n",
        "* Reference value for clipping the gradient value as mentioned in class: 0.2\n",
        "* Reference value for a typical size of Replay Buffer: >10k\n",
        "* Reference value for batch size while training: 64 - 512\n",
        "\n",
        "Note that these are just reference values and larger is not always better as it may slow things down.\n",
        "\n",
        "It is good practice in RL to ensure simpler things are working before complicating environments or training techniques.\n",
        "\n",
        "If you think your training method is not working at all, you could pass a fixed goal to the `env.reset()` method during the training loop to ensure that your model is learning."
      ],
      "metadata": {
        "id": "pxVawoBLe3bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwS8xVR7tbeQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "\n",
        "class TrainDQN:\n",
        "\n",
        "  def __init__(self, env, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    self.env = env\n",
        "    self.device = torch.device('cpu')\n",
        "    self.q_network = QNetwork(env).to(self.device)\n",
        "    self.target_network = QNetwork(env).to(self.device)\n",
        "    self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "  def save_model(self, episode_num, save_dir='models'):\n",
        "    timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    model_dir = os.path.join(save_dir, timestr)\n",
        "    if not os.path.exists(os.path.join(model_dir)):\n",
        "      os.makedirs(os.path.join(model_dir))\n",
        "    savepath = os.path.join(model_dir, f'q_network_ep_{episode_num:04d}.pth')\n",
        "    torch.save(self.q_network.state_dict(), savepath)\n",
        "    print(f'model saved to {savepath}\\n')\n",
        "\n",
        "  def select_action(self, epsilon, obs):\n",
        "    if random.random()<epsilon:\n",
        "      action = np.random.randint(0, env.action_space.shape[0])\n",
        "    else:\n",
        "      action = self.q_network.select_discrete_action(obs, self.device)\n",
        "    return action\n",
        "\n",
        "  def optimize(self, optimizer, r_b, batch_size, loss_func, gamma):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    batch_state, batch_action, batch_reward, batch_next_sate, _  = r_b.sample(batch_size)\n",
        "\n",
        "    batch_state = torch.from_numpy(batch_state).float()\n",
        "    batch_action = torch.from_numpy(batch_action).long()\n",
        "    batch_reward = torch.from_numpy(batch_reward).float()\n",
        "    batch_next_state = torch.from_numpy(batch_next_sate).float()\n",
        "\n",
        "    q_eval = self.q_network.forward(batch_state, self.device).gather(1, batch_action.view(-1, 1)).squeeze()\n",
        "\n",
        "    q_eval_t = self.target_network.forward(batch_next_state, self.device)\n",
        "    q_eval_t_max = torch.max(q_eval_t.detach(), dim=1)[0]\n",
        "    q_t = batch_reward + gamma*q_eval_t_max\n",
        "\n",
        "    loss = loss_func(q_eval, q_t)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for parameters in self.q_network.parameters():\n",
        "      parameters.grad.data.clamp_(-0.2, 0.2)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.001\n",
        "    epsilon_start = 0.9\n",
        "    epsilon_end = 0.05\n",
        "    epsilon_decay = 1000\n",
        "    gamma = 0.9\n",
        "\n",
        "    buffer_limit = 10000\n",
        "    r_b = ReplayBuffer(buffer_limit)\n",
        "\n",
        "    n_actions = env.action_space.shape[0]\n",
        "    n_states = env.observation_space.shape[0]\n",
        "\n",
        "    optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    best_return = -100000\n",
        "    total_episodes = 1000\n",
        "    steps_done = 0\n",
        "    for episode_i in range(total_episodes):\n",
        "      episodic_return = 0\n",
        "      state = env.reset()\n",
        "\n",
        "      while True:\n",
        "        \"\"\"\n",
        "        reference for implementing epislon-decay -\n",
        "        https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "        \"\"\"\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_done / epsilon_decay)\n",
        "        steps_done+=1\n",
        "        discrete_action = self.select_action(epsilon, state)\n",
        "        action = self.q_network.action_discrete_to_continuous(discrete_action)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "          break\n",
        "        r_b.put((state, discrete_action, reward, next_state, done))\n",
        "        episodic_return += reward\n",
        "\n",
        "        if len(r_b.buffer) >= buffer_limit:\n",
        "          loss = self.optimize(optimizer, r_b, batch_size, loss_func, gamma)\n",
        "        state = next_state\n",
        "\n",
        "      if episode_i%28 == 0:\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "      print(\"episode \", episode_i, \" reward \", episodic_return)\n",
        "\n",
        "      if(episodic_return >= best_return):\n",
        "        best_return = episodic_return\n",
        "        print(\"best return so far is \", episodic_return, \" in episode \", episode_i)\n",
        "      self.save_model(episode_i, save_dir='models_dqn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEHSV1Q1BT1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fa88de-55fd-48a5-b994-d0505aed7a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0  reward  -151.16354758167023\n",
            "best return so far is  -151.16354758167023  in episode  0\n",
            "model saved to models_dqn/2023-05-05_18-13-27/q_network_ep_0000.pth\n",
            "\n",
            "episode  1  reward  -160.74872230404512\n",
            "model saved to models_dqn/2023-05-05_18-13-27/q_network_ep_0001.pth\n",
            "\n",
            "episode  2  reward  -393.30337644915795\n",
            "model saved to models_dqn/2023-05-05_18-13-28/q_network_ep_0002.pth\n",
            "\n",
            "episode  3  reward  -253.20896057942673\n",
            "model saved to models_dqn/2023-05-05_18-13-28/q_network_ep_0003.pth\n",
            "\n",
            "episode  4  reward  -262.67308515096\n",
            "model saved to models_dqn/2023-05-05_18-13-28/q_network_ep_0004.pth\n",
            "\n",
            "episode  5  reward  -1276.0756554345585\n",
            "model saved to models_dqn/2023-05-05_18-13-28/q_network_ep_0005.pth\n",
            "\n",
            "episode  6  reward  -220.1849218289636\n",
            "model saved to models_dqn/2023-05-05_18-13-28/q_network_ep_0006.pth\n",
            "\n",
            "episode  7  reward  -1117.9604714349032\n",
            "model saved to models_dqn/2023-05-05_18-13-28/q_network_ep_0007.pth\n",
            "\n",
            "episode  8  reward  -421.10889735919193\n",
            "model saved to models_dqn/2023-05-05_18-13-29/q_network_ep_0008.pth\n",
            "\n",
            "episode  9  reward  -1290.2260267644208\n",
            "model saved to models_dqn/2023-05-05_18-13-29/q_network_ep_0009.pth\n",
            "\n",
            "episode  10  reward  -360.57993328494547\n",
            "model saved to models_dqn/2023-05-05_18-13-29/q_network_ep_0010.pth\n",
            "\n",
            "episode  11  reward  -1527.8164826689754\n",
            "model saved to models_dqn/2023-05-05_18-13-29/q_network_ep_0011.pth\n",
            "\n",
            "episode  12  reward  -291.3612152172372\n",
            "model saved to models_dqn/2023-05-05_18-13-29/q_network_ep_0012.pth\n",
            "\n",
            "episode  13  reward  -464.34503996991197\n",
            "model saved to models_dqn/2023-05-05_18-13-30/q_network_ep_0013.pth\n",
            "\n",
            "episode  14  reward  -386.67249401594256\n",
            "model saved to models_dqn/2023-05-05_18-13-30/q_network_ep_0014.pth\n",
            "\n",
            "episode  15  reward  -320.5084193095168\n",
            "model saved to models_dqn/2023-05-05_18-13-30/q_network_ep_0015.pth\n",
            "\n",
            "episode  16  reward  -448.6916378752103\n",
            "model saved to models_dqn/2023-05-05_18-13-30/q_network_ep_0016.pth\n",
            "\n",
            "episode  17  reward  -288.7920223989432\n",
            "model saved to models_dqn/2023-05-05_18-13-30/q_network_ep_0017.pth\n",
            "\n",
            "episode  18  reward  -368.55711539171693\n",
            "model saved to models_dqn/2023-05-05_18-13-31/q_network_ep_0018.pth\n",
            "\n",
            "episode  19  reward  -332.97093539290665\n",
            "model saved to models_dqn/2023-05-05_18-13-31/q_network_ep_0019.pth\n",
            "\n",
            "episode  20  reward  -388.60748185637516\n",
            "model saved to models_dqn/2023-05-05_18-13-31/q_network_ep_0020.pth\n",
            "\n",
            "episode  21  reward  -420.42808189531695\n",
            "model saved to models_dqn/2023-05-05_18-13-31/q_network_ep_0021.pth\n",
            "\n",
            "episode  22  reward  -269.66926366031976\n",
            "model saved to models_dqn/2023-05-05_18-13-32/q_network_ep_0022.pth\n",
            "\n",
            "episode  23  reward  -343.9272953741591\n",
            "model saved to models_dqn/2023-05-05_18-13-32/q_network_ep_0023.pth\n",
            "\n",
            "episode  24  reward  -355.4318375560896\n",
            "model saved to models_dqn/2023-05-05_18-13-32/q_network_ep_0024.pth\n",
            "\n",
            "episode  25  reward  -569.0952444671511\n",
            "model saved to models_dqn/2023-05-05_18-13-33/q_network_ep_0025.pth\n",
            "\n",
            "episode  26  reward  -326.4761611579976\n",
            "model saved to models_dqn/2023-05-05_18-13-33/q_network_ep_0026.pth\n",
            "\n",
            "episode  27  reward  -264.0719286291558\n",
            "model saved to models_dqn/2023-05-05_18-13-33/q_network_ep_0027.pth\n",
            "\n",
            "episode  28  reward  -458.0637420130501\n",
            "model saved to models_dqn/2023-05-05_18-13-33/q_network_ep_0028.pth\n",
            "\n",
            "episode  29  reward  -1313.1413711910495\n",
            "model saved to models_dqn/2023-05-05_18-13-34/q_network_ep_0029.pth\n",
            "\n",
            "episode  30  reward  -265.10818363083087\n",
            "model saved to models_dqn/2023-05-05_18-13-34/q_network_ep_0030.pth\n",
            "\n",
            "episode  31  reward  -404.677934341968\n",
            "model saved to models_dqn/2023-05-05_18-13-34/q_network_ep_0031.pth\n",
            "\n",
            "episode  32  reward  -1448.4931775283876\n",
            "model saved to models_dqn/2023-05-05_18-13-35/q_network_ep_0032.pth\n",
            "\n",
            "episode  33  reward  -1150.6327153630743\n",
            "model saved to models_dqn/2023-05-05_18-13-35/q_network_ep_0033.pth\n",
            "\n",
            "episode  34  reward  -1328.356818760336\n",
            "model saved to models_dqn/2023-05-05_18-13-35/q_network_ep_0034.pth\n",
            "\n",
            "episode  35  reward  -1121.159393203126\n",
            "model saved to models_dqn/2023-05-05_18-13-35/q_network_ep_0035.pth\n",
            "\n",
            "episode  36  reward  -1317.1895870022515\n",
            "model saved to models_dqn/2023-05-05_18-13-36/q_network_ep_0036.pth\n",
            "\n",
            "episode  37  reward  -422.10065782440375\n",
            "model saved to models_dqn/2023-05-05_18-13-36/q_network_ep_0037.pth\n",
            "\n",
            "episode  38  reward  -493.81109822042066\n",
            "model saved to models_dqn/2023-05-05_18-13-36/q_network_ep_0038.pth\n",
            "\n",
            "episode  39  reward  -784.5765788135112\n",
            "model saved to models_dqn/2023-05-05_18-13-36/q_network_ep_0039.pth\n",
            "\n",
            "episode  40  reward  -268.36298472538095\n",
            "model saved to models_dqn/2023-05-05_18-13-36/q_network_ep_0040.pth\n",
            "\n",
            "episode  41  reward  -338.0389868268388\n",
            "model saved to models_dqn/2023-05-05_18-13-37/q_network_ep_0041.pth\n",
            "\n",
            "episode  42  reward  -284.8039715124871\n",
            "model saved to models_dqn/2023-05-05_18-13-37/q_network_ep_0042.pth\n",
            "\n",
            "episode  43  reward  -374.586203885847\n",
            "model saved to models_dqn/2023-05-05_18-13-37/q_network_ep_0043.pth\n",
            "\n",
            "episode  44  reward  -1486.5595172043318\n",
            "model saved to models_dqn/2023-05-05_18-13-37/q_network_ep_0044.pth\n",
            "\n",
            "episode  45  reward  -247.00046378380256\n",
            "model saved to models_dqn/2023-05-05_18-13-37/q_network_ep_0045.pth\n",
            "\n",
            "episode  46  reward  -225.5562863754944\n",
            "model saved to models_dqn/2023-05-05_18-13-38/q_network_ep_0046.pth\n",
            "\n",
            "episode  47  reward  -439.0392139730831\n",
            "model saved to models_dqn/2023-05-05_18-13-38/q_network_ep_0047.pth\n",
            "\n",
            "episode  48  reward  -276.07812091615074\n",
            "model saved to models_dqn/2023-05-05_18-13-38/q_network_ep_0048.pth\n",
            "\n",
            "episode  49  reward  -408.6130395272574\n",
            "model saved to models_dqn/2023-05-05_18-13-38/q_network_ep_0049.pth\n",
            "\n",
            "episode  50  reward  -374.0256696114848\n",
            "model saved to models_dqn/2023-05-05_18-13-39/q_network_ep_0050.pth\n",
            "\n",
            "episode  51  reward  -312.48007138466926\n",
            "model saved to models_dqn/2023-05-05_18-13-39/q_network_ep_0051.pth\n",
            "\n",
            "episode  52  reward  -129.83456137799143\n",
            "best return so far is  -129.83456137799143  in episode  52\n",
            "model saved to models_dqn/2023-05-05_18-13-40/q_network_ep_0052.pth\n",
            "\n",
            "episode  53  reward  -520.1738012862141\n",
            "model saved to models_dqn/2023-05-05_18-13-41/q_network_ep_0053.pth\n",
            "\n",
            "episode  54  reward  -1219.7867889606002\n",
            "model saved to models_dqn/2023-05-05_18-13-41/q_network_ep_0054.pth\n",
            "\n",
            "episode  55  reward  -1113.531256774958\n",
            "model saved to models_dqn/2023-05-05_18-13-42/q_network_ep_0055.pth\n",
            "\n",
            "episode  56  reward  -497.39959052001876\n",
            "model saved to models_dqn/2023-05-05_18-13-43/q_network_ep_0056.pth\n",
            "\n",
            "episode  57  reward  -1052.641828508005\n",
            "model saved to models_dqn/2023-05-05_18-13-43/q_network_ep_0057.pth\n",
            "\n",
            "episode  58  reward  -689.9779167143955\n",
            "model saved to models_dqn/2023-05-05_18-13-44/q_network_ep_0058.pth\n",
            "\n",
            "episode  59  reward  -904.1391666698016\n",
            "model saved to models_dqn/2023-05-05_18-13-44/q_network_ep_0059.pth\n",
            "\n",
            "episode  60  reward  -1157.6981391137629\n",
            "model saved to models_dqn/2023-05-05_18-13-45/q_network_ep_0060.pth\n",
            "\n",
            "episode  61  reward  -1057.9608655771624\n",
            "model saved to models_dqn/2023-05-05_18-13-46/q_network_ep_0061.pth\n",
            "\n",
            "episode  62  reward  -1327.9357670078514\n",
            "model saved to models_dqn/2023-05-05_18-13-47/q_network_ep_0062.pth\n",
            "\n",
            "episode  63  reward  -1080.1510699960095\n",
            "model saved to models_dqn/2023-05-05_18-13-48/q_network_ep_0063.pth\n",
            "\n",
            "episode  64  reward  -1063.0402446941184\n",
            "model saved to models_dqn/2023-05-05_18-13-49/q_network_ep_0064.pth\n",
            "\n",
            "episode  65  reward  -1111.8003495603261\n",
            "model saved to models_dqn/2023-05-05_18-13-49/q_network_ep_0065.pth\n",
            "\n",
            "episode  66  reward  -591.4330675889032\n",
            "model saved to models_dqn/2023-05-05_18-13-50/q_network_ep_0066.pth\n",
            "\n",
            "episode  67  reward  -910.6951833596004\n",
            "model saved to models_dqn/2023-05-05_18-13-51/q_network_ep_0067.pth\n",
            "\n",
            "episode  68  reward  -864.3952910206705\n",
            "model saved to models_dqn/2023-05-05_18-13-51/q_network_ep_0068.pth\n",
            "\n",
            "episode  69  reward  -939.4865894072879\n",
            "model saved to models_dqn/2023-05-05_18-13-52/q_network_ep_0069.pth\n",
            "\n",
            "episode  70  reward  -1063.9665176688509\n",
            "model saved to models_dqn/2023-05-05_18-13-52/q_network_ep_0070.pth\n",
            "\n",
            "episode  71  reward  -452.9985371355512\n",
            "model saved to models_dqn/2023-05-05_18-13-53/q_network_ep_0071.pth\n",
            "\n",
            "episode  72  reward  -785.8990747374932\n",
            "model saved to models_dqn/2023-05-05_18-13-54/q_network_ep_0072.pth\n",
            "\n",
            "episode  73  reward  -1068.423740254854\n",
            "model saved to models_dqn/2023-05-05_18-13-54/q_network_ep_0073.pth\n",
            "\n",
            "episode  74  reward  -1363.200424602246\n",
            "model saved to models_dqn/2023-05-05_18-13-55/q_network_ep_0074.pth\n",
            "\n",
            "episode  75  reward  -950.4434492094399\n",
            "model saved to models_dqn/2023-05-05_18-13-55/q_network_ep_0075.pth\n",
            "\n",
            "episode  76  reward  -942.714583028998\n",
            "model saved to models_dqn/2023-05-05_18-13-56/q_network_ep_0076.pth\n",
            "\n",
            "episode  77  reward  -1086.3703959199754\n",
            "model saved to models_dqn/2023-05-05_18-13-57/q_network_ep_0077.pth\n",
            "\n",
            "episode  78  reward  -1187.5369289475975\n",
            "model saved to models_dqn/2023-05-05_18-13-57/q_network_ep_0078.pth\n",
            "\n",
            "episode  79  reward  -1380.2909769863863\n",
            "model saved to models_dqn/2023-05-05_18-13-58/q_network_ep_0079.pth\n",
            "\n",
            "episode  80  reward  -881.0244450922622\n",
            "model saved to models_dqn/2023-05-05_18-13-59/q_network_ep_0080.pth\n",
            "\n",
            "episode  81  reward  -1040.5546989071413\n",
            "model saved to models_dqn/2023-05-05_18-14-00/q_network_ep_0081.pth\n",
            "\n",
            "episode  82  reward  -1216.6868070903781\n",
            "model saved to models_dqn/2023-05-05_18-14-01/q_network_ep_0082.pth\n",
            "\n",
            "episode  83  reward  -800.026633076957\n",
            "model saved to models_dqn/2023-05-05_18-14-01/q_network_ep_0083.pth\n",
            "\n",
            "episode  84  reward  -1034.3594808408038\n",
            "model saved to models_dqn/2023-05-05_18-14-02/q_network_ep_0084.pth\n",
            "\n",
            "episode  85  reward  -819.6127302625671\n",
            "model saved to models_dqn/2023-05-05_18-14-03/q_network_ep_0085.pth\n",
            "\n",
            "episode  86  reward  -767.7634834504024\n",
            "model saved to models_dqn/2023-05-05_18-14-04/q_network_ep_0086.pth\n",
            "\n",
            "episode  87  reward  -653.4917552567327\n",
            "model saved to models_dqn/2023-05-05_18-14-04/q_network_ep_0087.pth\n",
            "\n",
            "episode  88  reward  -693.0247826487381\n",
            "model saved to models_dqn/2023-05-05_18-14-05/q_network_ep_0088.pth\n",
            "\n",
            "episode  89  reward  -838.3127662511463\n",
            "model saved to models_dqn/2023-05-05_18-14-06/q_network_ep_0089.pth\n",
            "\n",
            "episode  90  reward  -918.2034769232016\n",
            "model saved to models_dqn/2023-05-05_18-14-06/q_network_ep_0090.pth\n",
            "\n",
            "episode  91  reward  -986.7275041762229\n",
            "model saved to models_dqn/2023-05-05_18-14-07/q_network_ep_0091.pth\n",
            "\n",
            "episode  92  reward  -1318.417399629063\n",
            "model saved to models_dqn/2023-05-05_18-14-08/q_network_ep_0092.pth\n",
            "\n",
            "episode  93  reward  -734.2492575917142\n",
            "model saved to models_dqn/2023-05-05_18-14-08/q_network_ep_0093.pth\n",
            "\n",
            "episode  94  reward  -867.677513326864\n",
            "model saved to models_dqn/2023-05-05_18-14-09/q_network_ep_0094.pth\n",
            "\n",
            "episode  95  reward  -779.7984011839933\n",
            "model saved to models_dqn/2023-05-05_18-14-09/q_network_ep_0095.pth\n",
            "\n",
            "episode  96  reward  -333.72535448559984\n",
            "model saved to models_dqn/2023-05-05_18-14-10/q_network_ep_0096.pth\n",
            "\n",
            "episode  97  reward  -1038.6298229535944\n",
            "model saved to models_dqn/2023-05-05_18-14-11/q_network_ep_0097.pth\n",
            "\n",
            "episode  98  reward  -293.63737079298545\n",
            "model saved to models_dqn/2023-05-05_18-14-11/q_network_ep_0098.pth\n",
            "\n",
            "episode  99  reward  -277.35920617261223\n",
            "model saved to models_dqn/2023-05-05_18-14-12/q_network_ep_0099.pth\n",
            "\n",
            "episode  100  reward  -863.2639554096083\n",
            "model saved to models_dqn/2023-05-05_18-14-13/q_network_ep_0100.pth\n",
            "\n",
            "episode  101  reward  -1240.3566533977905\n",
            "model saved to models_dqn/2023-05-05_18-14-14/q_network_ep_0101.pth\n",
            "\n",
            "episode  102  reward  -500.4248567695686\n",
            "model saved to models_dqn/2023-05-05_18-14-15/q_network_ep_0102.pth\n",
            "\n",
            "episode  103  reward  -1247.6503966088055\n",
            "model saved to models_dqn/2023-05-05_18-14-16/q_network_ep_0103.pth\n",
            "\n",
            "episode  104  reward  -858.3707728706099\n",
            "model saved to models_dqn/2023-05-05_18-14-16/q_network_ep_0104.pth\n",
            "\n",
            "episode  105  reward  -464.5686602725368\n",
            "model saved to models_dqn/2023-05-05_18-14-17/q_network_ep_0105.pth\n",
            "\n",
            "episode  106  reward  -575.2624483750776\n",
            "model saved to models_dqn/2023-05-05_18-14-17/q_network_ep_0106.pth\n",
            "\n",
            "episode  107  reward  -601.898671994257\n",
            "model saved to models_dqn/2023-05-05_18-14-18/q_network_ep_0107.pth\n",
            "\n",
            "episode  108  reward  -905.9386923500407\n",
            "model saved to models_dqn/2023-05-05_18-14-19/q_network_ep_0108.pth\n",
            "\n",
            "episode  109  reward  -1080.111843435237\n",
            "model saved to models_dqn/2023-05-05_18-14-19/q_network_ep_0109.pth\n",
            "\n",
            "episode  110  reward  -557.9577200556872\n",
            "model saved to models_dqn/2023-05-05_18-14-20/q_network_ep_0110.pth\n",
            "\n",
            "episode  111  reward  -691.6461186013372\n",
            "model saved to models_dqn/2023-05-05_18-14-21/q_network_ep_0111.pth\n",
            "\n",
            "episode  112  reward  -883.5001317580044\n",
            "model saved to models_dqn/2023-05-05_18-14-21/q_network_ep_0112.pth\n",
            "\n",
            "episode  113  reward  -1072.1066743334793\n",
            "model saved to models_dqn/2023-05-05_18-14-22/q_network_ep_0113.pth\n",
            "\n",
            "episode  114  reward  -1408.9634856404653\n",
            "model saved to models_dqn/2023-05-05_18-14-22/q_network_ep_0114.pth\n",
            "\n",
            "episode  115  reward  -429.86334388089364\n",
            "model saved to models_dqn/2023-05-05_18-14-23/q_network_ep_0115.pth\n",
            "\n",
            "episode  116  reward  -1402.0864907288938\n",
            "model saved to models_dqn/2023-05-05_18-14-24/q_network_ep_0116.pth\n",
            "\n",
            "episode  117  reward  -1057.3188169158466\n",
            "model saved to models_dqn/2023-05-05_18-14-24/q_network_ep_0117.pth\n",
            "\n",
            "episode  118  reward  -1143.4373648846274\n",
            "model saved to models_dqn/2023-05-05_18-14-25/q_network_ep_0118.pth\n",
            "\n",
            "episode  119  reward  -433.83374629126865\n",
            "model saved to models_dqn/2023-05-05_18-14-26/q_network_ep_0119.pth\n",
            "\n",
            "episode  120  reward  -380.2795715518552\n",
            "model saved to models_dqn/2023-05-05_18-14-26/q_network_ep_0120.pth\n",
            "\n",
            "episode  121  reward  -737.2764235699864\n",
            "model saved to models_dqn/2023-05-05_18-14-27/q_network_ep_0121.pth\n",
            "\n",
            "episode  122  reward  -238.74263976895776\n",
            "model saved to models_dqn/2023-05-05_18-14-28/q_network_ep_0122.pth\n",
            "\n",
            "episode  123  reward  -355.41178619302065\n",
            "model saved to models_dqn/2023-05-05_18-14-29/q_network_ep_0123.pth\n",
            "\n",
            "episode  124  reward  -843.8878570950185\n",
            "model saved to models_dqn/2023-05-05_18-14-30/q_network_ep_0124.pth\n",
            "\n",
            "episode  125  reward  -330.13736257111447\n",
            "model saved to models_dqn/2023-05-05_18-14-30/q_network_ep_0125.pth\n",
            "\n",
            "episode  126  reward  -347.68399979553726\n",
            "model saved to models_dqn/2023-05-05_18-14-31/q_network_ep_0126.pth\n",
            "\n",
            "episode  127  reward  -949.4796260600257\n",
            "model saved to models_dqn/2023-05-05_18-14-32/q_network_ep_0127.pth\n",
            "\n",
            "episode  128  reward  -547.8697608078257\n",
            "model saved to models_dqn/2023-05-05_18-14-32/q_network_ep_0128.pth\n",
            "\n",
            "episode  129  reward  -138.85844909273845\n",
            "model saved to models_dqn/2023-05-05_18-14-33/q_network_ep_0129.pth\n",
            "\n",
            "episode  130  reward  -941.6268279272417\n",
            "model saved to models_dqn/2023-05-05_18-14-34/q_network_ep_0130.pth\n",
            "\n",
            "episode  131  reward  -859.5294109886113\n",
            "model saved to models_dqn/2023-05-05_18-14-34/q_network_ep_0131.pth\n",
            "\n",
            "episode  132  reward  -578.6403091884542\n",
            "model saved to models_dqn/2023-05-05_18-14-35/q_network_ep_0132.pth\n",
            "\n",
            "episode  133  reward  -1368.1183120921003\n",
            "model saved to models_dqn/2023-05-05_18-14-36/q_network_ep_0133.pth\n",
            "\n",
            "episode  134  reward  -307.85855625465695\n",
            "model saved to models_dqn/2023-05-05_18-14-36/q_network_ep_0134.pth\n",
            "\n",
            "episode  135  reward  -854.0641894871616\n",
            "model saved to models_dqn/2023-05-05_18-14-37/q_network_ep_0135.pth\n",
            "\n",
            "episode  136  reward  -1068.8411757748747\n",
            "model saved to models_dqn/2023-05-05_18-14-37/q_network_ep_0136.pth\n",
            "\n",
            "episode  137  reward  -888.066431136714\n",
            "model saved to models_dqn/2023-05-05_18-14-38/q_network_ep_0137.pth\n",
            "\n",
            "episode  138  reward  -810.0962780132402\n",
            "model saved to models_dqn/2023-05-05_18-14-39/q_network_ep_0138.pth\n",
            "\n",
            "episode  139  reward  -1262.9634291970149\n",
            "model saved to models_dqn/2023-05-05_18-14-39/q_network_ep_0139.pth\n",
            "\n",
            "episode  140  reward  -687.5055234257256\n",
            "model saved to models_dqn/2023-05-05_18-14-40/q_network_ep_0140.pth\n",
            "\n",
            "episode  141  reward  -620.5266061400716\n",
            "model saved to models_dqn/2023-05-05_18-14-41/q_network_ep_0141.pth\n",
            "\n",
            "episode  142  reward  -421.36847993783454\n",
            "model saved to models_dqn/2023-05-05_18-14-42/q_network_ep_0142.pth\n",
            "\n",
            "episode  143  reward  -1280.2036621266311\n",
            "model saved to models_dqn/2023-05-05_18-14-43/q_network_ep_0143.pth\n",
            "\n",
            "episode  144  reward  -852.1429783201108\n",
            "model saved to models_dqn/2023-05-05_18-14-44/q_network_ep_0144.pth\n",
            "\n",
            "episode  145  reward  -1570.6031238236308\n",
            "model saved to models_dqn/2023-05-05_18-14-44/q_network_ep_0145.pth\n",
            "\n",
            "episode  146  reward  -746.6756102872658\n",
            "model saved to models_dqn/2023-05-05_18-14-45/q_network_ep_0146.pth\n",
            "\n",
            "episode  147  reward  -49.326803784260846\n",
            "best return so far is  -49.326803784260846  in episode  147\n",
            "model saved to models_dqn/2023-05-05_18-14-46/q_network_ep_0147.pth\n",
            "\n",
            "episode  148  reward  -583.2788230792489\n",
            "model saved to models_dqn/2023-05-05_18-14-46/q_network_ep_0148.pth\n",
            "\n",
            "episode  149  reward  -937.7153054039615\n",
            "model saved to models_dqn/2023-05-05_18-14-47/q_network_ep_0149.pth\n",
            "\n",
            "episode  150  reward  -596.8036106925756\n",
            "model saved to models_dqn/2023-05-05_18-14-47/q_network_ep_0150.pth\n",
            "\n",
            "episode  151  reward  -228.32992498695666\n",
            "model saved to models_dqn/2023-05-05_18-14-48/q_network_ep_0151.pth\n",
            "\n",
            "episode  152  reward  -792.9501305323715\n",
            "model saved to models_dqn/2023-05-05_18-14-49/q_network_ep_0152.pth\n",
            "\n",
            "episode  153  reward  -29.946825241412004\n",
            "best return so far is  -29.946825241412004  in episode  153\n",
            "model saved to models_dqn/2023-05-05_18-14-49/q_network_ep_0153.pth\n",
            "\n",
            "episode  154  reward  -864.8804913494586\n",
            "model saved to models_dqn/2023-05-05_18-14-50/q_network_ep_0154.pth\n",
            "\n",
            "episode  155  reward  -586.054364004783\n",
            "model saved to models_dqn/2023-05-05_18-14-51/q_network_ep_0155.pth\n",
            "\n",
            "episode  156  reward  -50.7919161462509\n",
            "model saved to models_dqn/2023-05-05_18-14-51/q_network_ep_0156.pth\n",
            "\n",
            "episode  157  reward  -221.7296541976441\n",
            "model saved to models_dqn/2023-05-05_18-14-52/q_network_ep_0157.pth\n",
            "\n",
            "episode  158  reward  -554.4188346434688\n",
            "model saved to models_dqn/2023-05-05_18-14-53/q_network_ep_0158.pth\n",
            "\n",
            "episode  159  reward  -1024.6979820459328\n",
            "model saved to models_dqn/2023-05-05_18-14-54/q_network_ep_0159.pth\n",
            "\n",
            "episode  160  reward  -387.6934506733168\n",
            "model saved to models_dqn/2023-05-05_18-14-55/q_network_ep_0160.pth\n",
            "\n",
            "episode  161  reward  -1273.3012846996678\n",
            "model saved to models_dqn/2023-05-05_18-14-56/q_network_ep_0161.pth\n",
            "\n",
            "episode  162  reward  -798.0978119128407\n",
            "model saved to models_dqn/2023-05-05_18-14-56/q_network_ep_0162.pth\n",
            "\n",
            "episode  163  reward  -942.4678056750004\n",
            "model saved to models_dqn/2023-05-05_18-14-57/q_network_ep_0163.pth\n",
            "\n",
            "episode  164  reward  -738.0794276080192\n",
            "model saved to models_dqn/2023-05-05_18-14-58/q_network_ep_0164.pth\n",
            "\n",
            "episode  165  reward  -873.4315789567827\n",
            "model saved to models_dqn/2023-05-05_18-14-58/q_network_ep_0165.pth\n",
            "\n",
            "episode  166  reward  -206.96124314517067\n",
            "model saved to models_dqn/2023-05-05_18-14-59/q_network_ep_0166.pth\n",
            "\n",
            "episode  167  reward  -548.9860072544228\n",
            "model saved to models_dqn/2023-05-05_18-15-00/q_network_ep_0167.pth\n",
            "\n",
            "episode  168  reward  -727.5778990244869\n",
            "model saved to models_dqn/2023-05-05_18-15-00/q_network_ep_0168.pth\n",
            "\n",
            "episode  169  reward  -1157.0548026764225\n",
            "model saved to models_dqn/2023-05-05_18-15-01/q_network_ep_0169.pth\n",
            "\n",
            "episode  170  reward  -328.84480470137464\n",
            "model saved to models_dqn/2023-05-05_18-15-02/q_network_ep_0170.pth\n",
            "\n",
            "episode  171  reward  -98.05192127082243\n",
            "model saved to models_dqn/2023-05-05_18-15-02/q_network_ep_0171.pth\n",
            "\n",
            "episode  172  reward  -82.11449883377347\n",
            "model saved to models_dqn/2023-05-05_18-15-03/q_network_ep_0172.pth\n",
            "\n",
            "episode  173  reward  -70.26638992616526\n",
            "model saved to models_dqn/2023-05-05_18-15-04/q_network_ep_0173.pth\n",
            "\n",
            "episode  174  reward  -918.8375340626689\n",
            "model saved to models_dqn/2023-05-05_18-15-04/q_network_ep_0174.pth\n",
            "\n",
            "episode  175  reward  -689.1113312080766\n",
            "model saved to models_dqn/2023-05-05_18-15-05/q_network_ep_0175.pth\n",
            "\n",
            "episode  176  reward  -584.0681761495701\n",
            "model saved to models_dqn/2023-05-05_18-15-06/q_network_ep_0176.pth\n",
            "\n",
            "episode  177  reward  -783.7950359710406\n",
            "model saved to models_dqn/2023-05-05_18-15-06/q_network_ep_0177.pth\n",
            "\n",
            "episode  178  reward  -813.4191790996016\n",
            "model saved to models_dqn/2023-05-05_18-15-07/q_network_ep_0178.pth\n",
            "\n",
            "episode  179  reward  -851.488018396129\n",
            "model saved to models_dqn/2023-05-05_18-15-08/q_network_ep_0179.pth\n",
            "\n",
            "episode  180  reward  -235.2727700500881\n",
            "model saved to models_dqn/2023-05-05_18-15-09/q_network_ep_0180.pth\n",
            "\n",
            "episode  181  reward  -420.53026269434434\n",
            "model saved to models_dqn/2023-05-05_18-15-10/q_network_ep_0181.pth\n",
            "\n",
            "episode  182  reward  -670.8100018713519\n",
            "model saved to models_dqn/2023-05-05_18-15-11/q_network_ep_0182.pth\n",
            "\n",
            "episode  183  reward  -659.3052097205333\n",
            "model saved to models_dqn/2023-05-05_18-15-11/q_network_ep_0183.pth\n",
            "\n",
            "episode  184  reward  -352.3387715577728\n",
            "model saved to models_dqn/2023-05-05_18-15-12/q_network_ep_0184.pth\n",
            "\n",
            "episode  185  reward  -1019.4372569265573\n",
            "model saved to models_dqn/2023-05-05_18-15-13/q_network_ep_0185.pth\n",
            "\n",
            "episode  186  reward  -502.81049463931345\n",
            "model saved to models_dqn/2023-05-05_18-15-13/q_network_ep_0186.pth\n",
            "\n",
            "episode  187  reward  -702.2393426376127\n",
            "model saved to models_dqn/2023-05-05_18-15-14/q_network_ep_0187.pth\n",
            "\n",
            "episode  188  reward  -342.38907493379776\n",
            "model saved to models_dqn/2023-05-05_18-15-15/q_network_ep_0188.pth\n",
            "\n",
            "episode  189  reward  -378.970766936445\n",
            "model saved to models_dqn/2023-05-05_18-15-15/q_network_ep_0189.pth\n",
            "\n",
            "episode  190  reward  -648.8828899677894\n",
            "model saved to models_dqn/2023-05-05_18-15-16/q_network_ep_0190.pth\n",
            "\n",
            "episode  191  reward  -723.5189096456293\n",
            "model saved to models_dqn/2023-05-05_18-15-16/q_network_ep_0191.pth\n",
            "\n",
            "episode  192  reward  -433.8272828479473\n",
            "model saved to models_dqn/2023-05-05_18-15-17/q_network_ep_0192.pth\n",
            "\n",
            "episode  193  reward  -430.7695517069524\n",
            "model saved to models_dqn/2023-05-05_18-15-18/q_network_ep_0193.pth\n",
            "\n",
            "episode  194  reward  -374.15528510688534\n",
            "model saved to models_dqn/2023-05-05_18-15-18/q_network_ep_0194.pth\n",
            "\n",
            "episode  195  reward  -511.78651527862905\n",
            "model saved to models_dqn/2023-05-05_18-15-19/q_network_ep_0195.pth\n",
            "\n",
            "episode  196  reward  -578.7665294067943\n",
            "model saved to models_dqn/2023-05-05_18-15-20/q_network_ep_0196.pth\n",
            "\n",
            "episode  197  reward  -1203.1356646147956\n",
            "model saved to models_dqn/2023-05-05_18-15-21/q_network_ep_0197.pth\n",
            "\n",
            "episode  198  reward  -498.88592016374787\n",
            "model saved to models_dqn/2023-05-05_18-15-21/q_network_ep_0198.pth\n",
            "\n",
            "episode  199  reward  -418.24093555830905\n",
            "model saved to models_dqn/2023-05-05_18-15-22/q_network_ep_0199.pth\n",
            "\n",
            "episode  200  reward  -376.5805350355696\n",
            "model saved to models_dqn/2023-05-05_18-15-23/q_network_ep_0200.pth\n",
            "\n",
            "episode  201  reward  -695.0755875525864\n",
            "model saved to models_dqn/2023-05-05_18-15-24/q_network_ep_0201.pth\n",
            "\n",
            "episode  202  reward  -952.426624659269\n",
            "model saved to models_dqn/2023-05-05_18-15-25/q_network_ep_0202.pth\n",
            "\n",
            "episode  203  reward  -518.3802750512577\n",
            "model saved to models_dqn/2023-05-05_18-15-25/q_network_ep_0203.pth\n",
            "\n",
            "episode  204  reward  -1298.7963496123932\n",
            "model saved to models_dqn/2023-05-05_18-15-26/q_network_ep_0204.pth\n",
            "\n",
            "episode  205  reward  -139.37575907682884\n",
            "model saved to models_dqn/2023-05-05_18-15-27/q_network_ep_0205.pth\n",
            "\n",
            "episode  206  reward  -481.62375415609347\n",
            "model saved to models_dqn/2023-05-05_18-15-27/q_network_ep_0206.pth\n",
            "\n",
            "episode  207  reward  -178.74871549159408\n",
            "model saved to models_dqn/2023-05-05_18-15-28/q_network_ep_0207.pth\n",
            "\n",
            "episode  208  reward  -334.15667092765494\n",
            "model saved to models_dqn/2023-05-05_18-15-29/q_network_ep_0208.pth\n",
            "\n",
            "episode  209  reward  -1156.5707123256586\n",
            "model saved to models_dqn/2023-05-05_18-15-29/q_network_ep_0209.pth\n",
            "\n",
            "episode  210  reward  -482.1681670683401\n",
            "model saved to models_dqn/2023-05-05_18-15-30/q_network_ep_0210.pth\n",
            "\n",
            "episode  211  reward  -606.6325074935905\n",
            "model saved to models_dqn/2023-05-05_18-15-30/q_network_ep_0211.pth\n",
            "\n",
            "episode  212  reward  -304.1104010997757\n",
            "model saved to models_dqn/2023-05-05_18-15-31/q_network_ep_0212.pth\n",
            "\n",
            "episode  213  reward  -682.6201652255006\n",
            "model saved to models_dqn/2023-05-05_18-15-32/q_network_ep_0213.pth\n",
            "\n",
            "episode  214  reward  -547.0684391243308\n",
            "model saved to models_dqn/2023-05-05_18-15-32/q_network_ep_0214.pth\n",
            "\n",
            "episode  215  reward  -460.2816158643857\n",
            "model saved to models_dqn/2023-05-05_18-15-33/q_network_ep_0215.pth\n",
            "\n",
            "episode  216  reward  -769.0674573453509\n",
            "model saved to models_dqn/2023-05-05_18-15-34/q_network_ep_0216.pth\n",
            "\n",
            "episode  217  reward  -396.6074883887982\n",
            "model saved to models_dqn/2023-05-05_18-15-35/q_network_ep_0217.pth\n",
            "\n",
            "episode  218  reward  -389.7468520030421\n",
            "model saved to models_dqn/2023-05-05_18-15-35/q_network_ep_0218.pth\n",
            "\n",
            "episode  219  reward  -805.0217860261713\n",
            "model saved to models_dqn/2023-05-05_18-15-36/q_network_ep_0219.pth\n",
            "\n",
            "episode  220  reward  -666.7759145341236\n",
            "model saved to models_dqn/2023-05-05_18-15-37/q_network_ep_0220.pth\n",
            "\n",
            "episode  221  reward  -770.4017774583838\n",
            "model saved to models_dqn/2023-05-05_18-15-38/q_network_ep_0221.pth\n",
            "\n",
            "episode  222  reward  -366.0475982276513\n",
            "model saved to models_dqn/2023-05-05_18-15-39/q_network_ep_0222.pth\n",
            "\n",
            "episode  223  reward  -636.4655354914812\n",
            "model saved to models_dqn/2023-05-05_18-15-39/q_network_ep_0223.pth\n",
            "\n",
            "episode  224  reward  -696.0312431895925\n",
            "model saved to models_dqn/2023-05-05_18-15-40/q_network_ep_0224.pth\n",
            "\n",
            "episode  225  reward  -503.91673214466465\n",
            "model saved to models_dqn/2023-05-05_18-15-40/q_network_ep_0225.pth\n",
            "\n",
            "episode  226  reward  -549.3084219861437\n",
            "model saved to models_dqn/2023-05-05_18-15-41/q_network_ep_0226.pth\n",
            "\n",
            "episode  227  reward  -243.1251441590486\n",
            "model saved to models_dqn/2023-05-05_18-15-42/q_network_ep_0227.pth\n",
            "\n",
            "episode  228  reward  -543.8318105569067\n",
            "model saved to models_dqn/2023-05-05_18-15-42/q_network_ep_0228.pth\n",
            "\n",
            "episode  229  reward  -687.1411570421251\n",
            "model saved to models_dqn/2023-05-05_18-15-43/q_network_ep_0229.pth\n",
            "\n",
            "episode  230  reward  -723.760343367629\n",
            "model saved to models_dqn/2023-05-05_18-15-44/q_network_ep_0230.pth\n",
            "\n",
            "episode  231  reward  -53.439919428119524\n",
            "model saved to models_dqn/2023-05-05_18-15-44/q_network_ep_0231.pth\n",
            "\n",
            "episode  232  reward  -935.9441217304903\n",
            "model saved to models_dqn/2023-05-05_18-15-45/q_network_ep_0232.pth\n",
            "\n",
            "episode  233  reward  -1254.421256039525\n",
            "model saved to models_dqn/2023-05-05_18-15-46/q_network_ep_0233.pth\n",
            "\n",
            "episode  234  reward  -536.8859217183368\n",
            "model saved to models_dqn/2023-05-05_18-15-46/q_network_ep_0234.pth\n",
            "\n",
            "episode  235  reward  -554.7442431606403\n",
            "model saved to models_dqn/2023-05-05_18-15-47/q_network_ep_0235.pth\n",
            "\n",
            "episode  236  reward  -545.5492487987565\n",
            "model saved to models_dqn/2023-05-05_18-15-48/q_network_ep_0236.pth\n",
            "\n",
            "episode  237  reward  -367.23553074511915\n",
            "model saved to models_dqn/2023-05-05_18-15-48/q_network_ep_0237.pth\n",
            "\n",
            "episode  238  reward  -286.3971480906728\n",
            "model saved to models_dqn/2023-05-05_18-15-49/q_network_ep_0238.pth\n",
            "\n",
            "episode  239  reward  -574.7097907674014\n",
            "model saved to models_dqn/2023-05-05_18-15-50/q_network_ep_0239.pth\n",
            "\n",
            "episode  240  reward  -732.4852750801814\n",
            "model saved to models_dqn/2023-05-05_18-15-51/q_network_ep_0240.pth\n",
            "\n",
            "episode  241  reward  -588.2078997141178\n",
            "model saved to models_dqn/2023-05-05_18-15-52/q_network_ep_0241.pth\n",
            "\n",
            "episode  242  reward  -535.7001776505663\n",
            "model saved to models_dqn/2023-05-05_18-15-53/q_network_ep_0242.pth\n",
            "\n",
            "episode  243  reward  -705.1551811145839\n",
            "model saved to models_dqn/2023-05-05_18-15-53/q_network_ep_0243.pth\n",
            "\n",
            "episode  244  reward  -983.539456063849\n",
            "model saved to models_dqn/2023-05-05_18-15-54/q_network_ep_0244.pth\n",
            "\n",
            "episode  245  reward  -727.889551908187\n",
            "model saved to models_dqn/2023-05-05_18-15-54/q_network_ep_0245.pth\n",
            "\n",
            "episode  246  reward  -357.43210130761366\n",
            "model saved to models_dqn/2023-05-05_18-15-55/q_network_ep_0246.pth\n",
            "\n",
            "episode  247  reward  -325.0580398149645\n",
            "model saved to models_dqn/2023-05-05_18-15-56/q_network_ep_0247.pth\n",
            "\n",
            "episode  248  reward  -785.2407436076367\n",
            "model saved to models_dqn/2023-05-05_18-15-56/q_network_ep_0248.pth\n",
            "\n",
            "episode  249  reward  -575.3870377525874\n",
            "model saved to models_dqn/2023-05-05_18-15-57/q_network_ep_0249.pth\n",
            "\n",
            "episode  250  reward  -1247.5583173451785\n",
            "model saved to models_dqn/2023-05-05_18-15-58/q_network_ep_0250.pth\n",
            "\n",
            "episode  251  reward  -410.8625495379027\n",
            "model saved to models_dqn/2023-05-05_18-15-58/q_network_ep_0251.pth\n",
            "\n",
            "episode  252  reward  -230.0021664264452\n",
            "model saved to models_dqn/2023-05-05_18-15-59/q_network_ep_0252.pth\n",
            "\n",
            "episode  253  reward  -260.3166635649395\n",
            "model saved to models_dqn/2023-05-05_18-16-00/q_network_ep_0253.pth\n",
            "\n",
            "episode  254  reward  -961.9964071896759\n",
            "model saved to models_dqn/2023-05-05_18-16-00/q_network_ep_0254.pth\n",
            "\n",
            "episode  255  reward  -174.84778902972562\n",
            "model saved to models_dqn/2023-05-05_18-16-01/q_network_ep_0255.pth\n",
            "\n",
            "episode  256  reward  -103.42922072287966\n",
            "model saved to models_dqn/2023-05-05_18-16-02/q_network_ep_0256.pth\n",
            "\n",
            "episode  257  reward  -239.81521565263844\n",
            "model saved to models_dqn/2023-05-05_18-16-03/q_network_ep_0257.pth\n",
            "\n",
            "episode  258  reward  -14.65696207242866\n",
            "best return so far is  -14.65696207242866  in episode  258\n",
            "model saved to models_dqn/2023-05-05_18-16-04/q_network_ep_0258.pth\n",
            "\n",
            "episode  259  reward  -342.56038339502123\n",
            "model saved to models_dqn/2023-05-05_18-16-05/q_network_ep_0259.pth\n",
            "\n",
            "episode  260  reward  -894.7999165811057\n",
            "model saved to models_dqn/2023-05-05_18-16-05/q_network_ep_0260.pth\n",
            "\n",
            "episode  261  reward  -416.98654284389005\n",
            "model saved to models_dqn/2023-05-05_18-16-06/q_network_ep_0261.pth\n",
            "\n",
            "episode  262  reward  -651.0929329821256\n",
            "model saved to models_dqn/2023-05-05_18-16-06/q_network_ep_0262.pth\n",
            "\n",
            "episode  263  reward  -89.04946007218712\n",
            "model saved to models_dqn/2023-05-05_18-16-07/q_network_ep_0263.pth\n",
            "\n",
            "episode  264  reward  -714.5221903211013\n",
            "model saved to models_dqn/2023-05-05_18-16-08/q_network_ep_0264.pth\n",
            "\n",
            "episode  265  reward  -495.2529760448022\n",
            "model saved to models_dqn/2023-05-05_18-16-08/q_network_ep_0265.pth\n",
            "\n",
            "episode  266  reward  -812.1138264146263\n",
            "model saved to models_dqn/2023-05-05_18-16-09/q_network_ep_0266.pth\n",
            "\n",
            "episode  267  reward  -911.7215317880448\n",
            "model saved to models_dqn/2023-05-05_18-16-10/q_network_ep_0267.pth\n",
            "\n",
            "episode  268  reward  -567.092166581676\n",
            "model saved to models_dqn/2023-05-05_18-16-10/q_network_ep_0268.pth\n",
            "\n",
            "episode  269  reward  -101.84865318766721\n",
            "model saved to models_dqn/2023-05-05_18-16-11/q_network_ep_0269.pth\n",
            "\n",
            "episode  270  reward  -244.97132087548468\n",
            "model saved to models_dqn/2023-05-05_18-16-12/q_network_ep_0270.pth\n",
            "\n",
            "episode  271  reward  -200.21450235956877\n",
            "model saved to models_dqn/2023-05-05_18-16-12/q_network_ep_0271.pth\n",
            "\n",
            "episode  272  reward  -329.7758314340141\n",
            "model saved to models_dqn/2023-05-05_18-16-13/q_network_ep_0272.pth\n",
            "\n",
            "episode  273  reward  -84.06595878430507\n",
            "model saved to models_dqn/2023-05-05_18-16-14/q_network_ep_0273.pth\n",
            "\n",
            "episode  274  reward  -459.2417162430186\n",
            "model saved to models_dqn/2023-05-05_18-16-14/q_network_ep_0274.pth\n",
            "\n",
            "episode  275  reward  -367.0969150510405\n",
            "model saved to models_dqn/2023-05-05_18-16-15/q_network_ep_0275.pth\n",
            "\n",
            "episode  276  reward  -366.0272232430443\n",
            "model saved to models_dqn/2023-05-05_18-16-16/q_network_ep_0276.pth\n",
            "\n",
            "episode  277  reward  -306.54282659372086\n",
            "model saved to models_dqn/2023-05-05_18-16-17/q_network_ep_0277.pth\n",
            "\n",
            "episode  278  reward  -196.01590681970356\n",
            "model saved to models_dqn/2023-05-05_18-16-18/q_network_ep_0278.pth\n",
            "\n",
            "episode  279  reward  -1157.2084773755319\n",
            "model saved to models_dqn/2023-05-05_18-16-19/q_network_ep_0279.pth\n",
            "\n",
            "episode  280  reward  -474.0905770384749\n",
            "model saved to models_dqn/2023-05-05_18-16-19/q_network_ep_0280.pth\n",
            "\n",
            "episode  281  reward  -161.82199621869754\n",
            "model saved to models_dqn/2023-05-05_18-16-20/q_network_ep_0281.pth\n",
            "\n",
            "episode  282  reward  -96.24073219968884\n",
            "model saved to models_dqn/2023-05-05_18-16-21/q_network_ep_0282.pth\n",
            "\n",
            "episode  283  reward  -169.4757000673302\n",
            "model saved to models_dqn/2023-05-05_18-16-21/q_network_ep_0283.pth\n",
            "\n",
            "episode  284  reward  -134.30525489486303\n",
            "model saved to models_dqn/2023-05-05_18-16-22/q_network_ep_0284.pth\n",
            "\n",
            "episode  285  reward  -292.4949669224594\n",
            "model saved to models_dqn/2023-05-05_18-16-23/q_network_ep_0285.pth\n",
            "\n",
            "episode  286  reward  -226.92604803424504\n",
            "model saved to models_dqn/2023-05-05_18-16-23/q_network_ep_0286.pth\n",
            "\n",
            "episode  287  reward  -87.82619802452336\n",
            "model saved to models_dqn/2023-05-05_18-16-24/q_network_ep_0287.pth\n",
            "\n",
            "episode  288  reward  -270.45689987173455\n",
            "model saved to models_dqn/2023-05-05_18-16-25/q_network_ep_0288.pth\n",
            "\n",
            "episode  289  reward  -98.0830302996552\n",
            "model saved to models_dqn/2023-05-05_18-16-25/q_network_ep_0289.pth\n",
            "\n",
            "episode  290  reward  -115.53643422746094\n",
            "model saved to models_dqn/2023-05-05_18-16-26/q_network_ep_0290.pth\n",
            "\n",
            "episode  291  reward  -266.30036917383546\n",
            "model saved to models_dqn/2023-05-05_18-16-26/q_network_ep_0291.pth\n",
            "\n",
            "episode  292  reward  -458.8480623927455\n",
            "model saved to models_dqn/2023-05-05_18-16-27/q_network_ep_0292.pth\n",
            "\n",
            "episode  293  reward  -468.51316457796446\n",
            "model saved to models_dqn/2023-05-05_18-16-28/q_network_ep_0293.pth\n",
            "\n",
            "episode  294  reward  -13.15553122919464\n",
            "best return so far is  -13.15553122919464  in episode  294\n",
            "model saved to models_dqn/2023-05-05_18-16-28/q_network_ep_0294.pth\n",
            "\n",
            "episode  295  reward  -49.360264002742106\n",
            "model saved to models_dqn/2023-05-05_18-16-29/q_network_ep_0295.pth\n",
            "\n",
            "episode  296  reward  -748.3031040974896\n",
            "model saved to models_dqn/2023-05-05_18-16-30/q_network_ep_0296.pth\n",
            "\n",
            "episode  297  reward  -24.18484902704911\n",
            "model saved to models_dqn/2023-05-05_18-16-31/q_network_ep_0297.pth\n",
            "\n",
            "episode  298  reward  -283.3996566289274\n",
            "model saved to models_dqn/2023-05-05_18-16-32/q_network_ep_0298.pth\n",
            "\n",
            "episode  299  reward  -51.64215585784768\n",
            "model saved to models_dqn/2023-05-05_18-16-33/q_network_ep_0299.pth\n",
            "\n",
            "episode  300  reward  -71.69764210487268\n",
            "model saved to models_dqn/2023-05-05_18-16-33/q_network_ep_0300.pth\n",
            "\n",
            "episode  301  reward  -45.237179602963025\n",
            "model saved to models_dqn/2023-05-05_18-16-34/q_network_ep_0301.pth\n",
            "\n",
            "episode  302  reward  -343.1872986176377\n",
            "model saved to models_dqn/2023-05-05_18-16-35/q_network_ep_0302.pth\n",
            "\n",
            "episode  303  reward  -459.2806894368171\n",
            "model saved to models_dqn/2023-05-05_18-16-35/q_network_ep_0303.pth\n",
            "\n",
            "episode  304  reward  -39.36793579259247\n",
            "model saved to models_dqn/2023-05-05_18-16-36/q_network_ep_0304.pth\n",
            "\n",
            "episode  305  reward  -10.19797015791935\n",
            "best return so far is  -10.19797015791935  in episode  305\n",
            "model saved to models_dqn/2023-05-05_18-16-37/q_network_ep_0305.pth\n",
            "\n",
            "episode  306  reward  -167.22616474106943\n",
            "model saved to models_dqn/2023-05-05_18-16-37/q_network_ep_0306.pth\n",
            "\n",
            "episode  307  reward  -298.2936638845977\n",
            "model saved to models_dqn/2023-05-05_18-16-38/q_network_ep_0307.pth\n",
            "\n",
            "episode  308  reward  -257.31215103543235\n",
            "model saved to models_dqn/2023-05-05_18-16-39/q_network_ep_0308.pth\n",
            "\n",
            "episode  309  reward  -68.48863238563413\n",
            "model saved to models_dqn/2023-05-05_18-16-39/q_network_ep_0309.pth\n",
            "\n",
            "episode  310  reward  -365.873434026346\n",
            "model saved to models_dqn/2023-05-05_18-16-40/q_network_ep_0310.pth\n",
            "\n",
            "episode  311  reward  -164.36004005776078\n",
            "model saved to models_dqn/2023-05-05_18-16-41/q_network_ep_0311.pth\n",
            "\n",
            "episode  312  reward  -23.429420359362997\n",
            "model saved to models_dqn/2023-05-05_18-16-41/q_network_ep_0312.pth\n",
            "\n",
            "episode  313  reward  -74.90996726236993\n",
            "model saved to models_dqn/2023-05-05_18-16-42/q_network_ep_0313.pth\n",
            "\n",
            "episode  314  reward  -83.86326807499543\n",
            "model saved to models_dqn/2023-05-05_18-16-43/q_network_ep_0314.pth\n",
            "\n",
            "episode  315  reward  -246.23482146682892\n",
            "model saved to models_dqn/2023-05-05_18-16-44/q_network_ep_0315.pth\n",
            "\n",
            "episode  316  reward  -25.99437246889119\n",
            "model saved to models_dqn/2023-05-05_18-16-45/q_network_ep_0316.pth\n",
            "\n",
            "episode  317  reward  -341.66084796794416\n",
            "model saved to models_dqn/2023-05-05_18-16-46/q_network_ep_0317.pth\n",
            "\n",
            "episode  318  reward  -160.6810266723534\n",
            "model saved to models_dqn/2023-05-05_18-16-46/q_network_ep_0318.pth\n",
            "\n",
            "episode  319  reward  -46.47608528126275\n",
            "model saved to models_dqn/2023-05-05_18-16-47/q_network_ep_0319.pth\n",
            "\n",
            "episode  320  reward  -29.773877749953265\n",
            "model saved to models_dqn/2023-05-05_18-16-47/q_network_ep_0320.pth\n",
            "\n",
            "episode  321  reward  -143.72653894299648\n",
            "model saved to models_dqn/2023-05-05_18-16-48/q_network_ep_0321.pth\n",
            "\n",
            "episode  322  reward  -237.3693331572329\n",
            "model saved to models_dqn/2023-05-05_18-16-49/q_network_ep_0322.pth\n",
            "\n",
            "episode  323  reward  -19.130176671970137\n",
            "model saved to models_dqn/2023-05-05_18-16-49/q_network_ep_0323.pth\n",
            "\n",
            "episode  324  reward  -103.70148814330237\n",
            "model saved to models_dqn/2023-05-05_18-16-50/q_network_ep_0324.pth\n",
            "\n",
            "episode  325  reward  -396.8915017490192\n",
            "model saved to models_dqn/2023-05-05_18-16-51/q_network_ep_0325.pth\n",
            "\n",
            "episode  326  reward  -55.23815353328365\n",
            "model saved to models_dqn/2023-05-05_18-16-51/q_network_ep_0326.pth\n",
            "\n",
            "episode  327  reward  -147.54405876493067\n",
            "model saved to models_dqn/2023-05-05_18-16-52/q_network_ep_0327.pth\n",
            "\n",
            "episode  328  reward  -28.09107658201402\n",
            "model saved to models_dqn/2023-05-05_18-16-53/q_network_ep_0328.pth\n",
            "\n",
            "episode  329  reward  -31.567831876480483\n",
            "model saved to models_dqn/2023-05-05_18-16-53/q_network_ep_0329.pth\n",
            "\n",
            "episode  330  reward  -148.74736924579165\n",
            "model saved to models_dqn/2023-05-05_18-16-54/q_network_ep_0330.pth\n",
            "\n",
            "episode  331  reward  -279.97551140977106\n",
            "model saved to models_dqn/2023-05-05_18-16-55/q_network_ep_0331.pth\n",
            "\n",
            "episode  332  reward  -77.04648955927897\n",
            "model saved to models_dqn/2023-05-05_18-16-55/q_network_ep_0332.pth\n",
            "\n",
            "episode  333  reward  -688.7254468660858\n",
            "model saved to models_dqn/2023-05-05_18-16-56/q_network_ep_0333.pth\n",
            "\n",
            "episode  334  reward  -270.1799614774998\n",
            "model saved to models_dqn/2023-05-05_18-16-57/q_network_ep_0334.pth\n",
            "\n",
            "episode  335  reward  -409.58803685967143\n",
            "model saved to models_dqn/2023-05-05_18-16-58/q_network_ep_0335.pth\n",
            "\n",
            "episode  336  reward  -491.24959586763794\n",
            "model saved to models_dqn/2023-05-05_18-16-59/q_network_ep_0336.pth\n",
            "\n",
            "episode  337  reward  -205.21988982563062\n",
            "model saved to models_dqn/2023-05-05_18-17-00/q_network_ep_0337.pth\n",
            "\n",
            "episode  338  reward  -67.93990372604563\n",
            "model saved to models_dqn/2023-05-05_18-17-00/q_network_ep_0338.pth\n",
            "\n",
            "episode  339  reward  -16.16602454063941\n",
            "model saved to models_dqn/2023-05-05_18-17-01/q_network_ep_0339.pth\n",
            "\n",
            "episode  340  reward  -268.68080890067\n",
            "model saved to models_dqn/2023-05-05_18-17-02/q_network_ep_0340.pth\n",
            "\n",
            "episode  341  reward  -71.88575938149042\n",
            "model saved to models_dqn/2023-05-05_18-17-02/q_network_ep_0341.pth\n",
            "\n",
            "episode  342  reward  -10.945048296200786\n",
            "model saved to models_dqn/2023-05-05_18-17-03/q_network_ep_0342.pth\n",
            "\n",
            "episode  343  reward  -24.993287888346764\n",
            "model saved to models_dqn/2023-05-05_18-17-03/q_network_ep_0343.pth\n",
            "\n",
            "episode  344  reward  -21.34818753966819\n",
            "model saved to models_dqn/2023-05-05_18-17-04/q_network_ep_0344.pth\n",
            "\n",
            "episode  345  reward  -60.24736840884158\n",
            "model saved to models_dqn/2023-05-05_18-17-05/q_network_ep_0345.pth\n",
            "\n",
            "episode  346  reward  -263.3342006881088\n",
            "model saved to models_dqn/2023-05-05_18-17-05/q_network_ep_0346.pth\n",
            "\n",
            "episode  347  reward  -124.61023669946555\n",
            "model saved to models_dqn/2023-05-05_18-17-06/q_network_ep_0347.pth\n",
            "\n",
            "episode  348  reward  -51.63706724132114\n",
            "model saved to models_dqn/2023-05-05_18-17-07/q_network_ep_0348.pth\n",
            "\n",
            "episode  349  reward  -83.73155715512212\n",
            "model saved to models_dqn/2023-05-05_18-17-07/q_network_ep_0349.pth\n",
            "\n",
            "episode  350  reward  -19.60230884227371\n",
            "model saved to models_dqn/2023-05-05_18-17-08/q_network_ep_0350.pth\n",
            "\n",
            "episode  351  reward  -130.2929425044086\n",
            "model saved to models_dqn/2023-05-05_18-17-09/q_network_ep_0351.pth\n",
            "\n",
            "episode  352  reward  -20.919378541013224\n",
            "model saved to models_dqn/2023-05-05_18-17-10/q_network_ep_0352.pth\n",
            "\n",
            "episode  353  reward  -30.762055924118094\n",
            "model saved to models_dqn/2023-05-05_18-17-10/q_network_ep_0353.pth\n",
            "\n",
            "episode  354  reward  -25.311384848753946\n",
            "model saved to models_dqn/2023-05-05_18-17-11/q_network_ep_0354.pth\n",
            "\n",
            "episode  355  reward  -33.0585575817037\n",
            "model saved to models_dqn/2023-05-05_18-17-12/q_network_ep_0355.pth\n",
            "\n",
            "episode  356  reward  -54.73804645383067\n",
            "model saved to models_dqn/2023-05-05_18-17-13/q_network_ep_0356.pth\n",
            "\n",
            "episode  357  reward  -19.68781398204233\n",
            "model saved to models_dqn/2023-05-05_18-17-14/q_network_ep_0357.pth\n",
            "\n",
            "episode  358  reward  -143.99089242724858\n",
            "model saved to models_dqn/2023-05-05_18-17-14/q_network_ep_0358.pth\n",
            "\n",
            "episode  359  reward  -190.68371570606465\n",
            "model saved to models_dqn/2023-05-05_18-17-15/q_network_ep_0359.pth\n",
            "\n",
            "episode  360  reward  -16.83532542504836\n",
            "model saved to models_dqn/2023-05-05_18-17-16/q_network_ep_0360.pth\n",
            "\n",
            "episode  361  reward  -76.92609926811902\n",
            "model saved to models_dqn/2023-05-05_18-17-16/q_network_ep_0361.pth\n",
            "\n",
            "episode  362  reward  -81.90131415927983\n",
            "model saved to models_dqn/2023-05-05_18-17-17/q_network_ep_0362.pth\n",
            "\n",
            "episode  363  reward  -184.17508779279314\n",
            "model saved to models_dqn/2023-05-05_18-17-18/q_network_ep_0363.pth\n",
            "\n",
            "episode  364  reward  -9.853276416922546\n",
            "best return so far is  -9.853276416922546  in episode  364\n",
            "model saved to models_dqn/2023-05-05_18-17-18/q_network_ep_0364.pth\n",
            "\n",
            "episode  365  reward  -33.54517271486775\n",
            "model saved to models_dqn/2023-05-05_18-17-19/q_network_ep_0365.pth\n",
            "\n",
            "episode  366  reward  -74.02672381221171\n",
            "model saved to models_dqn/2023-05-05_18-17-20/q_network_ep_0366.pth\n",
            "\n",
            "episode  367  reward  -21.291448005300232\n",
            "model saved to models_dqn/2023-05-05_18-17-20/q_network_ep_0367.pth\n",
            "\n",
            "episode  368  reward  -301.1218570460753\n",
            "model saved to models_dqn/2023-05-05_18-17-21/q_network_ep_0368.pth\n",
            "\n",
            "episode  369  reward  -15.194163920040895\n",
            "model saved to models_dqn/2023-05-05_18-17-22/q_network_ep_0369.pth\n",
            "\n",
            "episode  370  reward  -8.296808967913474\n",
            "best return so far is  -8.296808967913474  in episode  370\n",
            "model saved to models_dqn/2023-05-05_18-17-22/q_network_ep_0370.pth\n",
            "\n",
            "episode  371  reward  -39.60795363057347\n",
            "model saved to models_dqn/2023-05-05_18-17-23/q_network_ep_0371.pth\n",
            "\n",
            "episode  372  reward  -57.073785481443025\n",
            "model saved to models_dqn/2023-05-05_18-17-24/q_network_ep_0372.pth\n",
            "\n",
            "episode  373  reward  -15.261968558653177\n",
            "model saved to models_dqn/2023-05-05_18-17-25/q_network_ep_0373.pth\n",
            "\n",
            "episode  374  reward  -32.51586398437295\n",
            "model saved to models_dqn/2023-05-05_18-17-26/q_network_ep_0374.pth\n",
            "\n",
            "episode  375  reward  -22.137694337105646\n",
            "model saved to models_dqn/2023-05-05_18-17-27/q_network_ep_0375.pth\n",
            "\n",
            "episode  376  reward  -40.57467079670162\n",
            "model saved to models_dqn/2023-05-05_18-17-27/q_network_ep_0376.pth\n",
            "\n",
            "episode  377  reward  -14.129335435982437\n",
            "model saved to models_dqn/2023-05-05_18-17-28/q_network_ep_0377.pth\n",
            "\n",
            "episode  378  reward  -25.60634343501249\n",
            "model saved to models_dqn/2023-05-05_18-17-28/q_network_ep_0378.pth\n",
            "\n",
            "episode  379  reward  -212.7329137063014\n",
            "model saved to models_dqn/2023-05-05_18-17-29/q_network_ep_0379.pth\n",
            "\n",
            "episode  380  reward  -26.520074064593285\n",
            "model saved to models_dqn/2023-05-05_18-17-30/q_network_ep_0380.pth\n",
            "\n",
            "episode  381  reward  -7.764710161804291\n",
            "best return so far is  -7.764710161804291  in episode  381\n",
            "model saved to models_dqn/2023-05-05_18-17-30/q_network_ep_0381.pth\n",
            "\n",
            "episode  382  reward  -4.088630920192917\n",
            "best return so far is  -4.088630920192917  in episode  382\n",
            "model saved to models_dqn/2023-05-05_18-17-31/q_network_ep_0382.pth\n",
            "\n",
            "episode  383  reward  -85.09159034868195\n",
            "model saved to models_dqn/2023-05-05_18-17-32/q_network_ep_0383.pth\n",
            "\n",
            "episode  384  reward  -27.274777123899582\n",
            "model saved to models_dqn/2023-05-05_18-17-32/q_network_ep_0384.pth\n",
            "\n",
            "episode  385  reward  -16.163398822476456\n",
            "model saved to models_dqn/2023-05-05_18-17-33/q_network_ep_0385.pth\n",
            "\n",
            "episode  386  reward  -7.183531100027622\n",
            "model saved to models_dqn/2023-05-05_18-17-34/q_network_ep_0386.pth\n",
            "\n",
            "episode  387  reward  -23.379400114529165\n",
            "model saved to models_dqn/2023-05-05_18-17-34/q_network_ep_0387.pth\n",
            "\n",
            "episode  388  reward  -380.93960080783177\n",
            "model saved to models_dqn/2023-05-05_18-17-35/q_network_ep_0388.pth\n",
            "\n",
            "episode  389  reward  -15.126021465436814\n",
            "model saved to models_dqn/2023-05-05_18-17-36/q_network_ep_0389.pth\n",
            "\n",
            "episode  390  reward  -30.94052810651163\n",
            "model saved to models_dqn/2023-05-05_18-17-37/q_network_ep_0390.pth\n",
            "\n",
            "episode  391  reward  -163.79513319491033\n",
            "model saved to models_dqn/2023-05-05_18-17-37/q_network_ep_0391.pth\n",
            "\n",
            "episode  392  reward  -34.16584609180013\n",
            "model saved to models_dqn/2023-05-05_18-17-38/q_network_ep_0392.pth\n",
            "\n",
            "episode  393  reward  -31.904870677721718\n",
            "model saved to models_dqn/2023-05-05_18-17-39/q_network_ep_0393.pth\n",
            "\n",
            "episode  394  reward  -406.8092034395115\n",
            "model saved to models_dqn/2023-05-05_18-17-40/q_network_ep_0394.pth\n",
            "\n",
            "episode  395  reward  -874.5819422813811\n",
            "model saved to models_dqn/2023-05-05_18-17-41/q_network_ep_0395.pth\n",
            "\n",
            "episode  396  reward  -476.7463711552035\n",
            "model saved to models_dqn/2023-05-05_18-17-41/q_network_ep_0396.pth\n",
            "\n",
            "episode  397  reward  -18.495744304286156\n",
            "model saved to models_dqn/2023-05-05_18-17-42/q_network_ep_0397.pth\n",
            "\n",
            "episode  398  reward  -6.863560661258782\n",
            "model saved to models_dqn/2023-05-05_18-17-43/q_network_ep_0398.pth\n",
            "\n",
            "episode  399  reward  -92.91767685156012\n",
            "model saved to models_dqn/2023-05-05_18-17-43/q_network_ep_0399.pth\n",
            "\n",
            "episode  400  reward  -28.05459335755184\n",
            "model saved to models_dqn/2023-05-05_18-17-44/q_network_ep_0400.pth\n",
            "\n",
            "episode  401  reward  -591.5553096215856\n",
            "model saved to models_dqn/2023-05-05_18-17-45/q_network_ep_0401.pth\n",
            "\n",
            "episode  402  reward  -7.937512441140167\n",
            "model saved to models_dqn/2023-05-05_18-17-45/q_network_ep_0402.pth\n",
            "\n",
            "episode  403  reward  -9.334545699817273\n",
            "model saved to models_dqn/2023-05-05_18-17-46/q_network_ep_0403.pth\n",
            "\n",
            "episode  404  reward  -28.76467461848581\n",
            "model saved to models_dqn/2023-05-05_18-17-47/q_network_ep_0404.pth\n",
            "\n",
            "episode  405  reward  -4.069942072218866\n",
            "best return so far is  -4.069942072218866  in episode  405\n",
            "model saved to models_dqn/2023-05-05_18-17-47/q_network_ep_0405.pth\n",
            "\n",
            "episode  406  reward  -131.23131438396067\n",
            "model saved to models_dqn/2023-05-05_18-17-48/q_network_ep_0406.pth\n",
            "\n",
            "episode  407  reward  -127.99957102270983\n",
            "model saved to models_dqn/2023-05-05_18-17-49/q_network_ep_0407.pth\n",
            "\n",
            "episode  408  reward  -5.447714566646384\n",
            "model saved to models_dqn/2023-05-05_18-17-49/q_network_ep_0408.pth\n",
            "\n",
            "episode  409  reward  -58.47520772718764\n",
            "model saved to models_dqn/2023-05-05_18-17-50/q_network_ep_0409.pth\n",
            "\n",
            "episode  410  reward  -27.075578959072576\n",
            "model saved to models_dqn/2023-05-05_18-17-51/q_network_ep_0410.pth\n",
            "\n",
            "episode  411  reward  -36.01015583784669\n",
            "model saved to models_dqn/2023-05-05_18-17-52/q_network_ep_0411.pth\n",
            "\n",
            "episode  412  reward  -14.608035507171508\n",
            "model saved to models_dqn/2023-05-05_18-17-53/q_network_ep_0412.pth\n",
            "\n",
            "episode  413  reward  -3.433200273181803\n",
            "best return so far is  -3.433200273181803  in episode  413\n",
            "model saved to models_dqn/2023-05-05_18-17-54/q_network_ep_0413.pth\n",
            "\n",
            "episode  414  reward  -24.371111108723227\n",
            "model saved to models_dqn/2023-05-05_18-17-54/q_network_ep_0414.pth\n",
            "\n",
            "episode  415  reward  -360.1285603197814\n",
            "model saved to models_dqn/2023-05-05_18-17-55/q_network_ep_0415.pth\n",
            "\n",
            "episode  416  reward  -15.559819148662315\n",
            "model saved to models_dqn/2023-05-05_18-17-56/q_network_ep_0416.pth\n",
            "\n",
            "episode  417  reward  -111.88454838152167\n",
            "model saved to models_dqn/2023-05-05_18-17-56/q_network_ep_0417.pth\n",
            "\n",
            "episode  418  reward  -10.970083385369094\n",
            "model saved to models_dqn/2023-05-05_18-17-57/q_network_ep_0418.pth\n",
            "\n",
            "episode  419  reward  -8.267952845335664\n",
            "model saved to models_dqn/2023-05-05_18-17-58/q_network_ep_0419.pth\n",
            "\n",
            "episode  420  reward  -19.212549152255676\n",
            "model saved to models_dqn/2023-05-05_18-17-58/q_network_ep_0420.pth\n",
            "\n",
            "episode  421  reward  -20.63350948438609\n",
            "model saved to models_dqn/2023-05-05_18-17-59/q_network_ep_0421.pth\n",
            "\n",
            "episode  422  reward  -11.604259764525628\n",
            "model saved to models_dqn/2023-05-05_18-18-00/q_network_ep_0422.pth\n",
            "\n",
            "episode  423  reward  -2.3821980825167164\n",
            "best return so far is  -2.3821980825167164  in episode  423\n",
            "model saved to models_dqn/2023-05-05_18-18-00/q_network_ep_0423.pth\n",
            "\n",
            "episode  424  reward  -19.192577585561477\n",
            "model saved to models_dqn/2023-05-05_18-18-01/q_network_ep_0424.pth\n",
            "\n",
            "episode  425  reward  -2.654136689560065\n",
            "model saved to models_dqn/2023-05-05_18-18-02/q_network_ep_0425.pth\n",
            "\n",
            "episode  426  reward  -31.619540196001402\n",
            "model saved to models_dqn/2023-05-05_18-18-02/q_network_ep_0426.pth\n",
            "\n",
            "episode  427  reward  -130.94221388197522\n",
            "model saved to models_dqn/2023-05-05_18-18-03/q_network_ep_0427.pth\n",
            "\n",
            "episode  428  reward  -19.78798903449144\n",
            "model saved to models_dqn/2023-05-05_18-18-04/q_network_ep_0428.pth\n",
            "\n",
            "episode  429  reward  -24.141747143915524\n",
            "model saved to models_dqn/2023-05-05_18-18-05/q_network_ep_0429.pth\n",
            "\n",
            "episode  430  reward  -15.968697986175895\n",
            "model saved to models_dqn/2023-05-05_18-18-06/q_network_ep_0430.pth\n",
            "\n",
            "episode  431  reward  -1.1246346896121773\n",
            "best return so far is  -1.1246346896121773  in episode  431\n",
            "model saved to models_dqn/2023-05-05_18-18-07/q_network_ep_0431.pth\n",
            "\n",
            "episode  432  reward  -36.69443334115358\n",
            "model saved to models_dqn/2023-05-05_18-18-07/q_network_ep_0432.pth\n",
            "\n",
            "episode  433  reward  -2.2946965104116606\n",
            "model saved to models_dqn/2023-05-05_18-18-08/q_network_ep_0433.pth\n",
            "\n",
            "episode  434  reward  -18.752684785334253\n",
            "model saved to models_dqn/2023-05-05_18-18-09/q_network_ep_0434.pth\n",
            "\n",
            "episode  435  reward  -7.30485429252313\n",
            "model saved to models_dqn/2023-05-05_18-18-09/q_network_ep_0435.pth\n",
            "\n",
            "episode  436  reward  -17.193648897352972\n",
            "model saved to models_dqn/2023-05-05_18-18-10/q_network_ep_0436.pth\n",
            "\n",
            "episode  437  reward  -33.44507891301291\n",
            "model saved to models_dqn/2023-05-05_18-18-11/q_network_ep_0437.pth\n",
            "\n",
            "episode  438  reward  -3.148843096325622\n",
            "model saved to models_dqn/2023-05-05_18-18-11/q_network_ep_0438.pth\n",
            "\n",
            "episode  439  reward  -0.2692806549995741\n",
            "best return so far is  -0.2692806549995741  in episode  439\n",
            "model saved to models_dqn/2023-05-05_18-18-12/q_network_ep_0439.pth\n",
            "\n",
            "episode  440  reward  -18.199466018558656\n",
            "model saved to models_dqn/2023-05-05_18-18-13/q_network_ep_0440.pth\n",
            "\n",
            "episode  441  reward  -11.805314167507813\n",
            "model saved to models_dqn/2023-05-05_18-18-13/q_network_ep_0441.pth\n",
            "\n",
            "episode  442  reward  -314.5207076142365\n",
            "model saved to models_dqn/2023-05-05_18-18-14/q_network_ep_0442.pth\n",
            "\n",
            "episode  443  reward  -1.5236430812859483\n",
            "model saved to models_dqn/2023-05-05_18-18-15/q_network_ep_0443.pth\n",
            "\n",
            "episode  444  reward  -6.607929063456065\n",
            "model saved to models_dqn/2023-05-05_18-18-15/q_network_ep_0444.pth\n",
            "\n",
            "episode  445  reward  -405.7019463208251\n",
            "model saved to models_dqn/2023-05-05_18-18-16/q_network_ep_0445.pth\n",
            "\n",
            "episode  446  reward  -6.213692744686728\n",
            "model saved to models_dqn/2023-05-05_18-18-17/q_network_ep_0446.pth\n",
            "\n",
            "episode  447  reward  -15.09896688079939\n",
            "model saved to models_dqn/2023-05-05_18-18-18/q_network_ep_0447.pth\n",
            "\n",
            "episode  448  reward  -3.839456305625906\n",
            "model saved to models_dqn/2023-05-05_18-18-19/q_network_ep_0448.pth\n",
            "\n",
            "episode  449  reward  -9.179954403765366\n",
            "model saved to models_dqn/2023-05-05_18-18-20/q_network_ep_0449.pth\n",
            "\n",
            "episode  450  reward  -1.3689332817690258\n",
            "model saved to models_dqn/2023-05-05_18-18-21/q_network_ep_0450.pth\n",
            "\n",
            "episode  451  reward  -46.46152481853924\n",
            "model saved to models_dqn/2023-05-05_18-18-21/q_network_ep_0451.pth\n",
            "\n",
            "episode  452  reward  -13.709889617573417\n",
            "model saved to models_dqn/2023-05-05_18-18-22/q_network_ep_0452.pth\n",
            "\n",
            "episode  453  reward  -300.5515021692656\n",
            "model saved to models_dqn/2023-05-05_18-18-23/q_network_ep_0453.pth\n",
            "\n",
            "episode  454  reward  -9.060058426898268\n",
            "model saved to models_dqn/2023-05-05_18-18-23/q_network_ep_0454.pth\n",
            "\n",
            "episode  455  reward  -2.993878244453972\n",
            "model saved to models_dqn/2023-05-05_18-18-24/q_network_ep_0455.pth\n",
            "\n",
            "episode  456  reward  -2.7459528361336685\n",
            "model saved to models_dqn/2023-05-05_18-18-25/q_network_ep_0456.pth\n",
            "\n",
            "episode  457  reward  -12.493561659210826\n",
            "model saved to models_dqn/2023-05-05_18-18-25/q_network_ep_0457.pth\n",
            "\n",
            "episode  458  reward  -11.086803971661086\n",
            "model saved to models_dqn/2023-05-05_18-18-26/q_network_ep_0458.pth\n",
            "\n",
            "episode  459  reward  -2.050892427598618\n",
            "model saved to models_dqn/2023-05-05_18-18-27/q_network_ep_0459.pth\n",
            "\n",
            "episode  460  reward  -7.368796159576359\n",
            "model saved to models_dqn/2023-05-05_18-18-28/q_network_ep_0460.pth\n",
            "\n",
            "episode  461  reward  -19.082947782554363\n",
            "model saved to models_dqn/2023-05-05_18-18-28/q_network_ep_0461.pth\n",
            "\n",
            "episode  462  reward  -4.261460690520312\n",
            "model saved to models_dqn/2023-05-05_18-18-29/q_network_ep_0462.pth\n",
            "\n",
            "episode  463  reward  -26.280767360316872\n",
            "model saved to models_dqn/2023-05-05_18-18-30/q_network_ep_0463.pth\n",
            "\n",
            "episode  464  reward  -28.815008226012935\n",
            "model saved to models_dqn/2023-05-05_18-18-30/q_network_ep_0464.pth\n",
            "\n",
            "episode  465  reward  -51.99801218387457\n",
            "model saved to models_dqn/2023-05-05_18-18-31/q_network_ep_0465.pth\n",
            "\n",
            "episode  466  reward  -6.382274400522681\n",
            "model saved to models_dqn/2023-05-05_18-18-32/q_network_ep_0466.pth\n",
            "\n",
            "episode  467  reward  -10.535079875426636\n",
            "model saved to models_dqn/2023-05-05_18-18-33/q_network_ep_0467.pth\n",
            "\n",
            "episode  468  reward  -12.452743351984786\n",
            "model saved to models_dqn/2023-05-05_18-18-34/q_network_ep_0468.pth\n",
            "\n",
            "episode  469  reward  -2.660475843281437\n",
            "model saved to models_dqn/2023-05-05_18-18-35/q_network_ep_0469.pth\n",
            "\n",
            "episode  470  reward  -5.479727811278496\n",
            "model saved to models_dqn/2023-05-05_18-18-36/q_network_ep_0470.pth\n",
            "\n",
            "episode  471  reward  -12.393747450694262\n",
            "model saved to models_dqn/2023-05-05_18-18-36/q_network_ep_0471.pth\n",
            "\n",
            "episode  472  reward  -8.607971676834524\n",
            "model saved to models_dqn/2023-05-05_18-18-37/q_network_ep_0472.pth\n",
            "\n",
            "episode  473  reward  -13.785554827481041\n",
            "model saved to models_dqn/2023-05-05_18-18-38/q_network_ep_0473.pth\n",
            "\n",
            "episode  474  reward  -9.769418566864482\n",
            "model saved to models_dqn/2023-05-05_18-18-38/q_network_ep_0474.pth\n",
            "\n",
            "episode  475  reward  -51.06747062789867\n",
            "model saved to models_dqn/2023-05-05_18-18-39/q_network_ep_0475.pth\n",
            "\n",
            "episode  476  reward  -8.060726124608903\n",
            "model saved to models_dqn/2023-05-05_18-18-40/q_network_ep_0476.pth\n",
            "\n",
            "episode  477  reward  -13.769464102060605\n",
            "model saved to models_dqn/2023-05-05_18-18-41/q_network_ep_0477.pth\n",
            "\n",
            "episode  478  reward  -20.384498788794232\n",
            "model saved to models_dqn/2023-05-05_18-18-41/q_network_ep_0478.pth\n",
            "\n",
            "episode  479  reward  -7.950804238043574\n",
            "model saved to models_dqn/2023-05-05_18-18-42/q_network_ep_0479.pth\n",
            "\n",
            "episode  480  reward  -264.9768213968368\n",
            "model saved to models_dqn/2023-05-05_18-18-43/q_network_ep_0480.pth\n",
            "\n",
            "episode  481  reward  -6.461053087251358\n",
            "model saved to models_dqn/2023-05-05_18-18-43/q_network_ep_0481.pth\n",
            "\n",
            "episode  482  reward  -4.0569815879008875\n",
            "model saved to models_dqn/2023-05-05_18-18-44/q_network_ep_0482.pth\n",
            "\n",
            "episode  483  reward  -3.55391351093185\n",
            "model saved to models_dqn/2023-05-05_18-18-45/q_network_ep_0483.pth\n",
            "\n",
            "episode  484  reward  -14.629591498398806\n",
            "model saved to models_dqn/2023-05-05_18-18-46/q_network_ep_0484.pth\n",
            "\n",
            "episode  485  reward  -19.64327149238188\n",
            "model saved to models_dqn/2023-05-05_18-18-47/q_network_ep_0485.pth\n",
            "\n",
            "episode  486  reward  -103.89719415208005\n",
            "model saved to models_dqn/2023-05-05_18-18-48/q_network_ep_0486.pth\n",
            "\n",
            "episode  487  reward  -4.101900403487018\n",
            "model saved to models_dqn/2023-05-05_18-18-49/q_network_ep_0487.pth\n",
            "\n",
            "episode  488  reward  -14.8809027590653\n",
            "model saved to models_dqn/2023-05-05_18-18-49/q_network_ep_0488.pth\n",
            "\n",
            "episode  489  reward  -5.574206898064963\n",
            "model saved to models_dqn/2023-05-05_18-18-50/q_network_ep_0489.pth\n",
            "\n",
            "episode  490  reward  -8.164451421565019\n",
            "model saved to models_dqn/2023-05-05_18-18-51/q_network_ep_0490.pth\n",
            "\n",
            "episode  491  reward  -2.1454148395524415\n",
            "model saved to models_dqn/2023-05-05_18-18-51/q_network_ep_0491.pth\n",
            "\n",
            "episode  492  reward  -5.873164670584474\n",
            "model saved to models_dqn/2023-05-05_18-18-52/q_network_ep_0492.pth\n",
            "\n",
            "episode  493  reward  -5.273473357723353\n",
            "model saved to models_dqn/2023-05-05_18-18-53/q_network_ep_0493.pth\n",
            "\n",
            "episode  494  reward  -13.820371203791066\n",
            "model saved to models_dqn/2023-05-05_18-18-53/q_network_ep_0494.pth\n",
            "\n",
            "episode  495  reward  -5.266684295284171\n",
            "model saved to models_dqn/2023-05-05_18-18-54/q_network_ep_0495.pth\n",
            "\n",
            "episode  496  reward  -5.230018313670346\n",
            "model saved to models_dqn/2023-05-05_18-18-55/q_network_ep_0496.pth\n",
            "\n",
            "episode  497  reward  -3.9444599071289868\n",
            "model saved to models_dqn/2023-05-05_18-18-56/q_network_ep_0497.pth\n",
            "\n",
            "episode  498  reward  -7.655706250190359\n",
            "model saved to models_dqn/2023-05-05_18-18-56/q_network_ep_0498.pth\n",
            "\n",
            "episode  499  reward  -4.78829381210956\n",
            "model saved to models_dqn/2023-05-05_18-18-57/q_network_ep_0499.pth\n",
            "\n",
            "episode  500  reward  -2.556816286673389\n",
            "model saved to models_dqn/2023-05-05_18-18-58/q_network_ep_0500.pth\n",
            "\n",
            "episode  501  reward  -4.026230630982005\n",
            "model saved to models_dqn/2023-05-05_18-18-59/q_network_ep_0501.pth\n",
            "\n",
            "episode  502  reward  -16.33004302037091\n",
            "model saved to models_dqn/2023-05-05_18-19-00/q_network_ep_0502.pth\n",
            "\n",
            "episode  503  reward  -9.197231543516756\n",
            "model saved to models_dqn/2023-05-05_18-19-01/q_network_ep_0503.pth\n",
            "\n",
            "episode  504  reward  -11.265815818047841\n",
            "model saved to models_dqn/2023-05-05_18-19-01/q_network_ep_0504.pth\n",
            "\n",
            "episode  505  reward  -11.72007293665844\n",
            "model saved to models_dqn/2023-05-05_18-19-02/q_network_ep_0505.pth\n",
            "\n",
            "episode  506  reward  -14.512616493993088\n",
            "model saved to models_dqn/2023-05-05_18-19-03/q_network_ep_0506.pth\n",
            "\n",
            "episode  507  reward  -2.979648796840175\n",
            "model saved to models_dqn/2023-05-05_18-19-04/q_network_ep_0507.pth\n",
            "\n",
            "episode  508  reward  -9.826257770234317\n",
            "model saved to models_dqn/2023-05-05_18-19-04/q_network_ep_0508.pth\n",
            "\n",
            "episode  509  reward  -25.326087340096258\n",
            "model saved to models_dqn/2023-05-05_18-19-05/q_network_ep_0509.pth\n",
            "\n",
            "episode  510  reward  -4.415121760765215\n",
            "model saved to models_dqn/2023-05-05_18-19-06/q_network_ep_0510.pth\n",
            "\n",
            "episode  511  reward  -2.8131397732094743\n",
            "model saved to models_dqn/2023-05-05_18-19-06/q_network_ep_0511.pth\n",
            "\n",
            "episode  512  reward  -15.826435510305682\n",
            "model saved to models_dqn/2023-05-05_18-19-07/q_network_ep_0512.pth\n",
            "\n",
            "episode  513  reward  -4.638093616749792\n",
            "model saved to models_dqn/2023-05-05_18-19-08/q_network_ep_0513.pth\n",
            "\n",
            "episode  514  reward  -11.61792706612853\n",
            "model saved to models_dqn/2023-05-05_18-19-08/q_network_ep_0514.pth\n",
            "\n",
            "episode  515  reward  -15.56765678449743\n",
            "model saved to models_dqn/2023-05-05_18-19-09/q_network_ep_0515.pth\n",
            "\n",
            "episode  516  reward  -2.074021985518106\n",
            "model saved to models_dqn/2023-05-05_18-19-10/q_network_ep_0516.pth\n",
            "\n",
            "episode  517  reward  -2.50666847214475\n",
            "model saved to models_dqn/2023-05-05_18-19-10/q_network_ep_0517.pth\n",
            "\n",
            "episode  518  reward  -6.324051553758895\n",
            "model saved to models_dqn/2023-05-05_18-19-11/q_network_ep_0518.pth\n",
            "\n",
            "episode  519  reward  -22.313379563727324\n",
            "model saved to models_dqn/2023-05-05_18-19-12/q_network_ep_0519.pth\n",
            "\n",
            "episode  520  reward  -17.247175286641188\n",
            "model saved to models_dqn/2023-05-05_18-19-13/q_network_ep_0520.pth\n",
            "\n",
            "episode  521  reward  -19.663086974626825\n",
            "model saved to models_dqn/2023-05-05_18-19-14/q_network_ep_0521.pth\n",
            "\n",
            "episode  522  reward  -2.3079291849236268\n",
            "model saved to models_dqn/2023-05-05_18-19-15/q_network_ep_0522.pth\n",
            "\n",
            "episode  523  reward  -4.2551977563745575\n",
            "model saved to models_dqn/2023-05-05_18-19-15/q_network_ep_0523.pth\n",
            "\n",
            "episode  524  reward  -1.6366782281485845\n",
            "model saved to models_dqn/2023-05-05_18-19-16/q_network_ep_0524.pth\n",
            "\n",
            "episode  525  reward  -12.458994379523277\n",
            "model saved to models_dqn/2023-05-05_18-19-17/q_network_ep_0525.pth\n",
            "\n",
            "episode  526  reward  -5.146577836258239\n",
            "model saved to models_dqn/2023-05-05_18-19-17/q_network_ep_0526.pth\n",
            "\n",
            "episode  527  reward  -8.433890026831248\n",
            "model saved to models_dqn/2023-05-05_18-19-18/q_network_ep_0527.pth\n",
            "\n",
            "episode  528  reward  -4.907840428083706\n",
            "model saved to models_dqn/2023-05-05_18-19-19/q_network_ep_0528.pth\n",
            "\n",
            "episode  529  reward  -5.43245642129621\n",
            "model saved to models_dqn/2023-05-05_18-19-19/q_network_ep_0529.pth\n",
            "\n",
            "episode  530  reward  -2.8616493160345438\n",
            "model saved to models_dqn/2023-05-05_18-19-20/q_network_ep_0530.pth\n",
            "\n",
            "episode  531  reward  -6.39724831508507\n",
            "model saved to models_dqn/2023-05-05_18-19-21/q_network_ep_0531.pth\n",
            "\n",
            "episode  532  reward  -10.170168250478632\n",
            "model saved to models_dqn/2023-05-05_18-19-22/q_network_ep_0532.pth\n",
            "\n",
            "episode  533  reward  -5.778121093523558\n",
            "model saved to models_dqn/2023-05-05_18-19-22/q_network_ep_0533.pth\n",
            "\n",
            "episode  534  reward  -2.372658379395401\n",
            "model saved to models_dqn/2023-05-05_18-19-23/q_network_ep_0534.pth\n",
            "\n",
            "episode  535  reward  -4.4516788934882525\n",
            "model saved to models_dqn/2023-05-05_18-19-24/q_network_ep_0535.pth\n",
            "\n",
            "episode  536  reward  -23.548603848113093\n",
            "model saved to models_dqn/2023-05-05_18-19-24/q_network_ep_0536.pth\n",
            "\n",
            "episode  537  reward  -4.699942510877534\n",
            "model saved to models_dqn/2023-05-05_18-19-25/q_network_ep_0537.pth\n",
            "\n",
            "episode  538  reward  -6.033009503888201\n",
            "model saved to models_dqn/2023-05-05_18-19-26/q_network_ep_0538.pth\n",
            "\n",
            "episode  539  reward  -1.8161526122363714\n",
            "model saved to models_dqn/2023-05-05_18-19-27/q_network_ep_0539.pth\n",
            "\n",
            "episode  540  reward  -3.6011117100326957\n",
            "model saved to models_dqn/2023-05-05_18-19-28/q_network_ep_0540.pth\n",
            "\n",
            "episode  541  reward  -5.8286506091742485\n",
            "model saved to models_dqn/2023-05-05_18-19-29/q_network_ep_0541.pth\n",
            "\n",
            "episode  542  reward  -3.993860793611285\n",
            "model saved to models_dqn/2023-05-05_18-19-29/q_network_ep_0542.pth\n",
            "\n",
            "episode  543  reward  -10.794600419961615\n",
            "model saved to models_dqn/2023-05-05_18-19-30/q_network_ep_0543.pth\n",
            "\n",
            "episode  544  reward  -1.0202702310245904\n",
            "model saved to models_dqn/2023-05-05_18-19-31/q_network_ep_0544.pth\n",
            "\n",
            "episode  545  reward  -4.0595131625706555\n",
            "model saved to models_dqn/2023-05-05_18-19-32/q_network_ep_0545.pth\n",
            "\n",
            "episode  546  reward  -2.6639195373960063\n",
            "model saved to models_dqn/2023-05-05_18-19-32/q_network_ep_0546.pth\n",
            "\n",
            "episode  547  reward  -12.043198105663404\n",
            "model saved to models_dqn/2023-05-05_18-19-33/q_network_ep_0547.pth\n",
            "\n",
            "episode  548  reward  -4.237845822423856\n",
            "model saved to models_dqn/2023-05-05_18-19-34/q_network_ep_0548.pth\n",
            "\n",
            "episode  549  reward  -3.7461293803132287\n",
            "model saved to models_dqn/2023-05-05_18-19-34/q_network_ep_0549.pth\n",
            "\n",
            "episode  550  reward  -1.1865735934900725\n",
            "model saved to models_dqn/2023-05-05_18-19-35/q_network_ep_0550.pth\n",
            "\n",
            "episode  551  reward  -0.8688816913719042\n",
            "model saved to models_dqn/2023-05-05_18-19-36/q_network_ep_0551.pth\n",
            "\n",
            "episode  552  reward  -5.0551328371205555\n",
            "model saved to models_dqn/2023-05-05_18-19-36/q_network_ep_0552.pth\n",
            "\n",
            "episode  553  reward  -2.0574907283362918\n",
            "model saved to models_dqn/2023-05-05_18-19-37/q_network_ep_0553.pth\n",
            "\n",
            "episode  554  reward  -9.973919683067606\n",
            "model saved to models_dqn/2023-05-05_18-19-38/q_network_ep_0554.pth\n",
            "\n",
            "episode  555  reward  -2.326486607750731\n",
            "model saved to models_dqn/2023-05-05_18-19-39/q_network_ep_0555.pth\n",
            "\n",
            "episode  556  reward  -2.4006865290568014\n",
            "model saved to models_dqn/2023-05-05_18-19-39/q_network_ep_0556.pth\n",
            "\n",
            "episode  557  reward  -15.303542458131243\n",
            "model saved to models_dqn/2023-05-05_18-19-41/q_network_ep_0557.pth\n",
            "\n",
            "episode  558  reward  -15.029744704939864\n",
            "model saved to models_dqn/2023-05-05_18-19-42/q_network_ep_0558.pth\n",
            "\n",
            "episode  559  reward  -3.9496919267186827\n",
            "model saved to models_dqn/2023-05-05_18-19-42/q_network_ep_0559.pth\n",
            "\n",
            "episode  560  reward  -3.923632536840167\n",
            "model saved to models_dqn/2023-05-05_18-19-43/q_network_ep_0560.pth\n",
            "\n",
            "episode  561  reward  -2.647992556288981\n",
            "model saved to models_dqn/2023-05-05_18-19-44/q_network_ep_0561.pth\n",
            "\n",
            "episode  562  reward  -1.6907824936446147\n",
            "model saved to models_dqn/2023-05-05_18-19-44/q_network_ep_0562.pth\n",
            "\n",
            "episode  563  reward  -6.281047395197094\n",
            "model saved to models_dqn/2023-05-05_18-19-45/q_network_ep_0563.pth\n",
            "\n",
            "episode  564  reward  -5.137404234868084\n",
            "model saved to models_dqn/2023-05-05_18-19-46/q_network_ep_0564.pth\n",
            "\n",
            "episode  565  reward  -1.3080428594239757\n",
            "model saved to models_dqn/2023-05-05_18-19-46/q_network_ep_0565.pth\n",
            "\n",
            "episode  566  reward  -2.5348303350029373\n",
            "model saved to models_dqn/2023-05-05_18-19-47/q_network_ep_0566.pth\n",
            "\n",
            "episode  567  reward  -7.527687768752973\n",
            "model saved to models_dqn/2023-05-05_18-19-48/q_network_ep_0567.pth\n",
            "\n",
            "episode  568  reward  -1.1226501513648335\n",
            "model saved to models_dqn/2023-05-05_18-19-48/q_network_ep_0568.pth\n",
            "\n",
            "episode  569  reward  -2.3699259153160828\n",
            "model saved to models_dqn/2023-05-05_18-19-49/q_network_ep_0569.pth\n",
            "\n",
            "episode  570  reward  -2.4966658315640475\n",
            "model saved to models_dqn/2023-05-05_18-19-50/q_network_ep_0570.pth\n",
            "\n",
            "episode  571  reward  -7.7935491491757425\n",
            "model saved to models_dqn/2023-05-05_18-19-51/q_network_ep_0571.pth\n",
            "\n",
            "episode  572  reward  -35.9309143509408\n",
            "model saved to models_dqn/2023-05-05_18-19-51/q_network_ep_0572.pth\n",
            "\n",
            "episode  573  reward  -25.62587658869858\n",
            "model saved to models_dqn/2023-05-05_18-19-52/q_network_ep_0573.pth\n",
            "\n",
            "episode  574  reward  -7.0648708795443\n",
            "model saved to models_dqn/2023-05-05_18-19-53/q_network_ep_0574.pth\n",
            "\n",
            "episode  575  reward  -8.048485285023744\n",
            "model saved to models_dqn/2023-05-05_18-19-54/q_network_ep_0575.pth\n",
            "\n",
            "episode  576  reward  -8.811421726185973\n",
            "model saved to models_dqn/2023-05-05_18-19-55/q_network_ep_0576.pth\n",
            "\n",
            "episode  577  reward  -11.948841389075445\n",
            "model saved to models_dqn/2023-05-05_18-19-56/q_network_ep_0577.pth\n",
            "\n",
            "episode  578  reward  -3.0326520233501415\n",
            "model saved to models_dqn/2023-05-05_18-19-56/q_network_ep_0578.pth\n",
            "\n",
            "episode  579  reward  -11.608889486567499\n",
            "model saved to models_dqn/2023-05-05_18-19-57/q_network_ep_0579.pth\n",
            "\n",
            "episode  580  reward  -4.078738488410011\n",
            "model saved to models_dqn/2023-05-05_18-19-58/q_network_ep_0580.pth\n",
            "\n",
            "episode  581  reward  -2.491038056106149\n",
            "model saved to models_dqn/2023-05-05_18-19-59/q_network_ep_0581.pth\n",
            "\n",
            "episode  582  reward  -2.3791715006493313\n",
            "model saved to models_dqn/2023-05-05_18-19-59/q_network_ep_0582.pth\n",
            "\n",
            "episode  583  reward  -3.2530857590614413\n",
            "model saved to models_dqn/2023-05-05_18-20-00/q_network_ep_0583.pth\n",
            "\n",
            "episode  584  reward  -8.759752694003469\n",
            "model saved to models_dqn/2023-05-05_18-20-01/q_network_ep_0584.pth\n",
            "\n",
            "episode  585  reward  -1.8271800958965851\n",
            "model saved to models_dqn/2023-05-05_18-20-01/q_network_ep_0585.pth\n",
            "\n",
            "episode  586  reward  -3.136144805739549\n",
            "model saved to models_dqn/2023-05-05_18-20-02/q_network_ep_0586.pth\n",
            "\n",
            "episode  587  reward  -13.484317195815924\n",
            "model saved to models_dqn/2023-05-05_18-20-03/q_network_ep_0587.pth\n",
            "\n",
            "episode  588  reward  -1.6818065985373534\n",
            "model saved to models_dqn/2023-05-05_18-20-03/q_network_ep_0588.pth\n",
            "\n",
            "episode  589  reward  -6.51407024721189\n",
            "model saved to models_dqn/2023-05-05_18-20-04/q_network_ep_0589.pth\n",
            "\n",
            "episode  590  reward  -4.360536564763546\n",
            "model saved to models_dqn/2023-05-05_18-20-05/q_network_ep_0590.pth\n",
            "\n",
            "episode  591  reward  -1.978325028199107\n",
            "model saved to models_dqn/2023-05-05_18-20-06/q_network_ep_0591.pth\n",
            "\n",
            "episode  592  reward  -3.6516810627975316\n",
            "model saved to models_dqn/2023-05-05_18-20-07/q_network_ep_0592.pth\n",
            "\n",
            "episode  593  reward  -7.69210157793967\n",
            "model saved to models_dqn/2023-05-05_18-20-08/q_network_ep_0593.pth\n",
            "\n",
            "episode  594  reward  -8.586101592562251\n",
            "model saved to models_dqn/2023-05-05_18-20-09/q_network_ep_0594.pth\n",
            "\n",
            "episode  595  reward  -0.8190010605977223\n",
            "model saved to models_dqn/2023-05-05_18-20-10/q_network_ep_0595.pth\n",
            "\n",
            "episode  596  reward  -0.9297720951654287\n",
            "model saved to models_dqn/2023-05-05_18-20-10/q_network_ep_0596.pth\n",
            "\n",
            "episode  597  reward  -3.1944489778013\n",
            "model saved to models_dqn/2023-05-05_18-20-11/q_network_ep_0597.pth\n",
            "\n",
            "episode  598  reward  -1.9837620717889426\n",
            "model saved to models_dqn/2023-05-05_18-20-12/q_network_ep_0598.pth\n",
            "\n",
            "episode  599  reward  -6.43521092175443\n",
            "model saved to models_dqn/2023-05-05_18-20-12/q_network_ep_0599.pth\n",
            "\n",
            "episode  600  reward  -8.288157694555053\n",
            "model saved to models_dqn/2023-05-05_18-20-13/q_network_ep_0600.pth\n",
            "\n",
            "episode  601  reward  -857.9714672865408\n",
            "model saved to models_dqn/2023-05-05_18-20-14/q_network_ep_0601.pth\n",
            "\n",
            "episode  602  reward  -5.708801347305411\n",
            "model saved to models_dqn/2023-05-05_18-20-14/q_network_ep_0602.pth\n",
            "\n",
            "episode  603  reward  -2.8788561800052874\n",
            "model saved to models_dqn/2023-05-05_18-20-15/q_network_ep_0603.pth\n",
            "\n",
            "episode  604  reward  -4.51517422671034\n",
            "model saved to models_dqn/2023-05-05_18-20-16/q_network_ep_0604.pth\n",
            "\n",
            "episode  605  reward  -4.65587180770667\n",
            "model saved to models_dqn/2023-05-05_18-20-16/q_network_ep_0605.pth\n",
            "\n",
            "episode  606  reward  -16.701140332542654\n",
            "model saved to models_dqn/2023-05-05_18-20-17/q_network_ep_0606.pth\n",
            "\n",
            "episode  607  reward  -7.550941058492053\n",
            "model saved to models_dqn/2023-05-05_18-20-18/q_network_ep_0607.pth\n",
            "\n",
            "episode  608  reward  -4.012085094077767\n",
            "model saved to models_dqn/2023-05-05_18-20-19/q_network_ep_0608.pth\n",
            "\n",
            "episode  609  reward  -12.88644655013656\n",
            "model saved to models_dqn/2023-05-05_18-20-20/q_network_ep_0609.pth\n",
            "\n",
            "episode  610  reward  -3.134011683351116\n",
            "model saved to models_dqn/2023-05-05_18-20-21/q_network_ep_0610.pth\n",
            "\n",
            "episode  611  reward  -2.1307169706303974\n",
            "model saved to models_dqn/2023-05-05_18-20-22/q_network_ep_0611.pth\n",
            "\n",
            "episode  612  reward  -447.5561657551598\n",
            "model saved to models_dqn/2023-05-05_18-20-23/q_network_ep_0612.pth\n",
            "\n",
            "episode  613  reward  -10.469848190436046\n",
            "model saved to models_dqn/2023-05-05_18-20-23/q_network_ep_0613.pth\n",
            "\n",
            "episode  614  reward  -8.699096369705424\n",
            "model saved to models_dqn/2023-05-05_18-20-24/q_network_ep_0614.pth\n",
            "\n",
            "episode  615  reward  -157.0106863424128\n",
            "model saved to models_dqn/2023-05-05_18-20-25/q_network_ep_0615.pth\n",
            "\n",
            "episode  616  reward  -29.366921075775533\n",
            "model saved to models_dqn/2023-05-05_18-20-25/q_network_ep_0616.pth\n",
            "\n",
            "episode  617  reward  -691.4340493517384\n",
            "model saved to models_dqn/2023-05-05_18-20-26/q_network_ep_0617.pth\n",
            "\n",
            "episode  618  reward  -33.544169055543605\n",
            "model saved to models_dqn/2023-05-05_18-20-27/q_network_ep_0618.pth\n",
            "\n",
            "episode  619  reward  -3.7781769419131046\n",
            "model saved to models_dqn/2023-05-05_18-20-28/q_network_ep_0619.pth\n",
            "\n",
            "episode  620  reward  -9.654252497244883\n",
            "model saved to models_dqn/2023-05-05_18-20-28/q_network_ep_0620.pth\n",
            "\n",
            "episode  621  reward  -88.41990658670719\n",
            "model saved to models_dqn/2023-05-05_18-20-29/q_network_ep_0621.pth\n",
            "\n",
            "episode  622  reward  -8.293149891809788\n",
            "model saved to models_dqn/2023-05-05_18-20-30/q_network_ep_0622.pth\n",
            "\n",
            "episode  623  reward  -6.392219948731103\n",
            "model saved to models_dqn/2023-05-05_18-20-30/q_network_ep_0623.pth\n",
            "\n",
            "episode  624  reward  -15.665170913586245\n",
            "model saved to models_dqn/2023-05-05_18-20-31/q_network_ep_0624.pth\n",
            "\n",
            "episode  625  reward  -5.744163193905937\n",
            "model saved to models_dqn/2023-05-05_18-20-32/q_network_ep_0625.pth\n",
            "\n",
            "episode  626  reward  -4.173463208849322\n",
            "model saved to models_dqn/2023-05-05_18-20-32/q_network_ep_0626.pth\n",
            "\n",
            "episode  627  reward  -8.638523503958762\n",
            "model saved to models_dqn/2023-05-05_18-20-33/q_network_ep_0627.pth\n",
            "\n",
            "episode  628  reward  -9.1318444187245\n",
            "model saved to models_dqn/2023-05-05_18-20-34/q_network_ep_0628.pth\n",
            "\n",
            "episode  629  reward  -1.0682291457025623\n",
            "model saved to models_dqn/2023-05-05_18-20-35/q_network_ep_0629.pth\n",
            "\n",
            "episode  630  reward  -12.406955628699212\n",
            "model saved to models_dqn/2023-05-05_18-20-36/q_network_ep_0630.pth\n",
            "\n",
            "episode  631  reward  -1.6657944565579113\n",
            "model saved to models_dqn/2023-05-05_18-20-37/q_network_ep_0631.pth\n",
            "\n",
            "episode  632  reward  -5.415919686426185\n",
            "model saved to models_dqn/2023-05-05_18-20-38/q_network_ep_0632.pth\n",
            "\n",
            "episode  633  reward  -17.03019374797658\n",
            "model saved to models_dqn/2023-05-05_18-20-38/q_network_ep_0633.pth\n",
            "\n",
            "episode  634  reward  -4.244195410924045\n",
            "model saved to models_dqn/2023-05-05_18-20-39/q_network_ep_0634.pth\n",
            "\n",
            "episode  635  reward  -2.9689917817316616\n",
            "model saved to models_dqn/2023-05-05_18-20-40/q_network_ep_0635.pth\n",
            "\n",
            "episode  636  reward  -1.3572556241914946\n",
            "model saved to models_dqn/2023-05-05_18-20-40/q_network_ep_0636.pth\n",
            "\n",
            "episode  637  reward  -11.728018185386446\n",
            "model saved to models_dqn/2023-05-05_18-20-41/q_network_ep_0637.pth\n",
            "\n",
            "episode  638  reward  -3.2238650329833014\n",
            "model saved to models_dqn/2023-05-05_18-20-42/q_network_ep_0638.pth\n",
            "\n",
            "episode  639  reward  -10.45067619923747\n",
            "model saved to models_dqn/2023-05-05_18-20-42/q_network_ep_0639.pth\n",
            "\n",
            "episode  640  reward  -4.679018692745011\n",
            "model saved to models_dqn/2023-05-05_18-20-43/q_network_ep_0640.pth\n",
            "\n",
            "episode  641  reward  -4.683540296417249\n",
            "model saved to models_dqn/2023-05-05_18-20-44/q_network_ep_0641.pth\n",
            "\n",
            "episode  642  reward  -4.734711178178829\n",
            "model saved to models_dqn/2023-05-05_18-20-44/q_network_ep_0642.pth\n",
            "\n",
            "episode  643  reward  -5.786848672372744\n",
            "model saved to models_dqn/2023-05-05_18-20-45/q_network_ep_0643.pth\n",
            "\n",
            "episode  644  reward  -7.265706663840427\n",
            "model saved to models_dqn/2023-05-05_18-20-46/q_network_ep_0644.pth\n",
            "\n",
            "episode  645  reward  -6.2006820271011645\n",
            "model saved to models_dqn/2023-05-05_18-20-47/q_network_ep_0645.pth\n",
            "\n",
            "episode  646  reward  -2.3448238387616036\n",
            "model saved to models_dqn/2023-05-05_18-20-48/q_network_ep_0646.pth\n",
            "\n",
            "episode  647  reward  -13.290387129393398\n",
            "model saved to models_dqn/2023-05-05_18-20-49/q_network_ep_0647.pth\n",
            "\n",
            "episode  648  reward  -4.337443747581012\n",
            "model saved to models_dqn/2023-05-05_18-20-50/q_network_ep_0648.pth\n",
            "\n",
            "episode  649  reward  -1.9816331996829666\n",
            "model saved to models_dqn/2023-05-05_18-20-50/q_network_ep_0649.pth\n",
            "\n",
            "episode  650  reward  -17.032183533975356\n",
            "model saved to models_dqn/2023-05-05_18-20-51/q_network_ep_0650.pth\n",
            "\n",
            "episode  651  reward  -4.380166911906545\n",
            "model saved to models_dqn/2023-05-05_18-20-52/q_network_ep_0651.pth\n",
            "\n",
            "episode  652  reward  -5.992443122920865\n",
            "model saved to models_dqn/2023-05-05_18-20-52/q_network_ep_0652.pth\n",
            "\n",
            "episode  653  reward  -0.9215417229147337\n",
            "model saved to models_dqn/2023-05-05_18-20-53/q_network_ep_0653.pth\n",
            "\n",
            "episode  654  reward  -8.100787476819361\n",
            "model saved to models_dqn/2023-05-05_18-20-54/q_network_ep_0654.pth\n",
            "\n",
            "episode  655  reward  -4.805244039467573\n",
            "model saved to models_dqn/2023-05-05_18-20-54/q_network_ep_0655.pth\n",
            "\n",
            "episode  656  reward  -5.350990754349231\n",
            "model saved to models_dqn/2023-05-05_18-20-55/q_network_ep_0656.pth\n",
            "\n",
            "episode  657  reward  -1.1501102105477998\n",
            "model saved to models_dqn/2023-05-05_18-20-56/q_network_ep_0657.pth\n",
            "\n",
            "episode  658  reward  -2.9570253868047436\n",
            "model saved to models_dqn/2023-05-05_18-20-56/q_network_ep_0658.pth\n",
            "\n",
            "episode  659  reward  -7.524325615510183\n",
            "model saved to models_dqn/2023-05-05_18-20-57/q_network_ep_0659.pth\n",
            "\n",
            "episode  660  reward  -2.874864543162142\n",
            "model saved to models_dqn/2023-05-05_18-20-58/q_network_ep_0660.pth\n",
            "\n",
            "episode  661  reward  -2.806039887056303\n",
            "model saved to models_dqn/2023-05-05_18-20-59/q_network_ep_0661.pth\n",
            "\n",
            "episode  662  reward  -4.7178365638954105\n",
            "model saved to models_dqn/2023-05-05_18-20-59/q_network_ep_0662.pth\n",
            "\n",
            "episode  663  reward  -3.3352601338528998\n",
            "model saved to models_dqn/2023-05-05_18-21-00/q_network_ep_0663.pth\n",
            "\n",
            "episode  664  reward  -3.2392762909829274\n",
            "model saved to models_dqn/2023-05-05_18-21-01/q_network_ep_0664.pth\n",
            "\n",
            "episode  665  reward  -5.890278086474509\n",
            "model saved to models_dqn/2023-05-05_18-21-02/q_network_ep_0665.pth\n",
            "\n",
            "episode  666  reward  -3.2813447783541174\n",
            "model saved to models_dqn/2023-05-05_18-21-03/q_network_ep_0666.pth\n",
            "\n",
            "episode  667  reward  -3.0234768694655045\n",
            "model saved to models_dqn/2023-05-05_18-21-04/q_network_ep_0667.pth\n",
            "\n",
            "episode  668  reward  -2.8474729989563543\n",
            "model saved to models_dqn/2023-05-05_18-21-04/q_network_ep_0668.pth\n",
            "\n",
            "episode  669  reward  -5.623763471735643\n",
            "model saved to models_dqn/2023-05-05_18-21-05/q_network_ep_0669.pth\n",
            "\n",
            "episode  670  reward  -6.675354958305162\n",
            "model saved to models_dqn/2023-05-05_18-21-06/q_network_ep_0670.pth\n",
            "\n",
            "episode  671  reward  -3.695448034245456\n",
            "model saved to models_dqn/2023-05-05_18-21-06/q_network_ep_0671.pth\n",
            "\n",
            "episode  672  reward  -8.68185221469576\n",
            "model saved to models_dqn/2023-05-05_18-21-07/q_network_ep_0672.pth\n",
            "\n",
            "episode  673  reward  -8.436655876714868\n",
            "model saved to models_dqn/2023-05-05_18-21-08/q_network_ep_0673.pth\n",
            "\n",
            "episode  674  reward  -5.324869740095816\n",
            "model saved to models_dqn/2023-05-05_18-21-09/q_network_ep_0674.pth\n",
            "\n",
            "episode  675  reward  -10.795058795076129\n",
            "model saved to models_dqn/2023-05-05_18-21-09/q_network_ep_0675.pth\n",
            "\n",
            "episode  676  reward  -7.065797499159315\n",
            "model saved to models_dqn/2023-05-05_18-21-10/q_network_ep_0676.pth\n",
            "\n",
            "episode  677  reward  -7.338602917564693\n",
            "model saved to models_dqn/2023-05-05_18-21-11/q_network_ep_0677.pth\n",
            "\n",
            "episode  678  reward  -14.023991156168293\n",
            "model saved to models_dqn/2023-05-05_18-21-11/q_network_ep_0678.pth\n",
            "\n",
            "episode  679  reward  -8.431472986884662\n",
            "model saved to models_dqn/2023-05-05_18-21-12/q_network_ep_0679.pth\n",
            "\n",
            "episode  680  reward  -6.120296640661731\n",
            "model saved to models_dqn/2023-05-05_18-21-13/q_network_ep_0680.pth\n",
            "\n",
            "episode  681  reward  -4.2940193041044505\n",
            "model saved to models_dqn/2023-05-05_18-21-13/q_network_ep_0681.pth\n",
            "\n",
            "episode  682  reward  -10.47583695863064\n",
            "model saved to models_dqn/2023-05-05_18-21-14/q_network_ep_0682.pth\n",
            "\n",
            "episode  683  reward  -3.2190061636382366\n",
            "model saved to models_dqn/2023-05-05_18-21-15/q_network_ep_0683.pth\n",
            "\n",
            "episode  684  reward  -2.5349876090273864\n",
            "model saved to models_dqn/2023-05-05_18-21-16/q_network_ep_0684.pth\n",
            "\n",
            "episode  685  reward  -1.1193586277363359\n",
            "model saved to models_dqn/2023-05-05_18-21-17/q_network_ep_0685.pth\n",
            "\n",
            "episode  686  reward  -9.33416631546544\n",
            "model saved to models_dqn/2023-05-05_18-21-18/q_network_ep_0686.pth\n",
            "\n",
            "episode  687  reward  -6.6988490034476555\n",
            "model saved to models_dqn/2023-05-05_18-21-19/q_network_ep_0687.pth\n",
            "\n",
            "episode  688  reward  -11.709641184089293\n",
            "model saved to models_dqn/2023-05-05_18-21-19/q_network_ep_0688.pth\n",
            "\n",
            "episode  689  reward  -7.721798297108262\n",
            "model saved to models_dqn/2023-05-05_18-21-20/q_network_ep_0689.pth\n",
            "\n",
            "episode  690  reward  -4.468339255231442\n",
            "model saved to models_dqn/2023-05-05_18-21-21/q_network_ep_0690.pth\n",
            "\n",
            "episode  691  reward  -3.3123419523403355\n",
            "model saved to models_dqn/2023-05-05_18-21-21/q_network_ep_0691.pth\n",
            "\n",
            "episode  692  reward  -1.1701494914993742\n",
            "model saved to models_dqn/2023-05-05_18-21-22/q_network_ep_0692.pth\n",
            "\n",
            "episode  693  reward  -38.07671651189123\n",
            "model saved to models_dqn/2023-05-05_18-21-23/q_network_ep_0693.pth\n",
            "\n",
            "episode  694  reward  -11.618877394295083\n",
            "model saved to models_dqn/2023-05-05_18-21-23/q_network_ep_0694.pth\n",
            "\n",
            "episode  695  reward  -0.7603941200724251\n",
            "model saved to models_dqn/2023-05-05_18-21-24/q_network_ep_0695.pth\n",
            "\n",
            "episode  696  reward  -0.8513959315921569\n",
            "model saved to models_dqn/2023-05-05_18-21-25/q_network_ep_0696.pth\n",
            "\n",
            "episode  697  reward  -11.72471535302738\n",
            "model saved to models_dqn/2023-05-05_18-21-26/q_network_ep_0697.pth\n",
            "\n",
            "episode  698  reward  -2.3067472023374065\n",
            "model saved to models_dqn/2023-05-05_18-21-26/q_network_ep_0698.pth\n",
            "\n",
            "episode  699  reward  -2.4769516741982347\n",
            "model saved to models_dqn/2023-05-05_18-21-27/q_network_ep_0699.pth\n",
            "\n",
            "episode  700  reward  -0.704999178111391\n",
            "model saved to models_dqn/2023-05-05_18-21-28/q_network_ep_0700.pth\n",
            "\n",
            "episode  701  reward  -4.483449446936672\n",
            "model saved to models_dqn/2023-05-05_18-21-29/q_network_ep_0701.pth\n",
            "\n",
            "episode  702  reward  -1.3383168976701647\n",
            "model saved to models_dqn/2023-05-05_18-21-30/q_network_ep_0702.pth\n",
            "\n",
            "episode  703  reward  -20.26745936240222\n",
            "model saved to models_dqn/2023-05-05_18-21-31/q_network_ep_0703.pth\n",
            "\n",
            "episode  704  reward  -2.807265459621305\n",
            "model saved to models_dqn/2023-05-05_18-21-31/q_network_ep_0704.pth\n",
            "\n",
            "episode  705  reward  -1.2015036927306886\n",
            "model saved to models_dqn/2023-05-05_18-21-32/q_network_ep_0705.pth\n",
            "\n",
            "episode  706  reward  -3.5088063865061803\n",
            "model saved to models_dqn/2023-05-05_18-21-33/q_network_ep_0706.pth\n",
            "\n",
            "episode  707  reward  -6.389506978593728\n",
            "model saved to models_dqn/2023-05-05_18-21-34/q_network_ep_0707.pth\n",
            "\n",
            "episode  708  reward  -8.148474834015381\n",
            "model saved to models_dqn/2023-05-05_18-21-34/q_network_ep_0708.pth\n",
            "\n",
            "episode  709  reward  -1.852875623152749\n",
            "model saved to models_dqn/2023-05-05_18-21-35/q_network_ep_0709.pth\n",
            "\n",
            "episode  710  reward  -1.2754644885479762\n",
            "model saved to models_dqn/2023-05-05_18-21-36/q_network_ep_0710.pth\n",
            "\n",
            "episode  711  reward  -6.4246041399882\n",
            "model saved to models_dqn/2023-05-05_18-21-36/q_network_ep_0711.pth\n",
            "\n",
            "episode  712  reward  -1.9254322628873186\n",
            "model saved to models_dqn/2023-05-05_18-21-37/q_network_ep_0712.pth\n",
            "\n",
            "episode  713  reward  -0.5954256744789148\n",
            "model saved to models_dqn/2023-05-05_18-21-38/q_network_ep_0713.pth\n",
            "\n",
            "episode  714  reward  -1.179130218328016\n",
            "model saved to models_dqn/2023-05-05_18-21-38/q_network_ep_0714.pth\n",
            "\n",
            "episode  715  reward  -1.3067937566351797\n",
            "model saved to models_dqn/2023-05-05_18-21-39/q_network_ep_0715.pth\n",
            "\n",
            "episode  716  reward  -2.8692347856118317\n",
            "model saved to models_dqn/2023-05-05_18-21-40/q_network_ep_0716.pth\n",
            "\n",
            "episode  717  reward  -5.436571889250232\n",
            "model saved to models_dqn/2023-05-05_18-21-41/q_network_ep_0717.pth\n",
            "\n",
            "episode  718  reward  -10.185133600568083\n",
            "model saved to models_dqn/2023-05-05_18-21-42/q_network_ep_0718.pth\n",
            "\n",
            "episode  719  reward  -2.309601928368986\n",
            "model saved to models_dqn/2023-05-05_18-21-43/q_network_ep_0719.pth\n",
            "\n",
            "episode  720  reward  -1.360991577470864\n",
            "model saved to models_dqn/2023-05-05_18-21-44/q_network_ep_0720.pth\n",
            "\n",
            "episode  721  reward  -4.822156879649409\n",
            "model saved to models_dqn/2023-05-05_18-21-45/q_network_ep_0721.pth\n",
            "\n",
            "episode  722  reward  -944.6206172204827\n",
            "model saved to models_dqn/2023-05-05_18-21-45/q_network_ep_0722.pth\n",
            "\n",
            "episode  723  reward  -502.13889786154294\n",
            "model saved to models_dqn/2023-05-05_18-21-46/q_network_ep_0723.pth\n",
            "\n",
            "episode  724  reward  -21.31486145451535\n",
            "model saved to models_dqn/2023-05-05_18-21-47/q_network_ep_0724.pth\n",
            "\n",
            "episode  725  reward  -740.9989832043469\n",
            "model saved to models_dqn/2023-05-05_18-21-47/q_network_ep_0725.pth\n",
            "\n",
            "episode  726  reward  -26.251155414159733\n",
            "model saved to models_dqn/2023-05-05_18-21-48/q_network_ep_0726.pth\n",
            "\n",
            "episode  727  reward  -38.41996852219375\n",
            "model saved to models_dqn/2023-05-05_18-21-49/q_network_ep_0727.pth\n",
            "\n",
            "episode  728  reward  -404.0347023395839\n",
            "model saved to models_dqn/2023-05-05_18-21-50/q_network_ep_0728.pth\n",
            "\n",
            "episode  729  reward  -47.41826516544113\n",
            "model saved to models_dqn/2023-05-05_18-21-50/q_network_ep_0729.pth\n",
            "\n",
            "episode  730  reward  -205.83174378342355\n",
            "model saved to models_dqn/2023-05-05_18-21-51/q_network_ep_0730.pth\n",
            "\n",
            "episode  731  reward  -25.626791444863663\n",
            "model saved to models_dqn/2023-05-05_18-21-52/q_network_ep_0731.pth\n",
            "\n",
            "episode  732  reward  -562.9196924706216\n",
            "model saved to models_dqn/2023-05-05_18-21-52/q_network_ep_0732.pth\n",
            "\n",
            "episode  733  reward  -22.386172827632297\n",
            "model saved to models_dqn/2023-05-05_18-21-53/q_network_ep_0733.pth\n",
            "\n",
            "episode  734  reward  -845.521185774963\n",
            "model saved to models_dqn/2023-05-05_18-21-54/q_network_ep_0734.pth\n",
            "\n",
            "episode  735  reward  -48.66177268409699\n",
            "model saved to models_dqn/2023-05-05_18-21-55/q_network_ep_0735.pth\n",
            "\n",
            "episode  736  reward  -109.32105426467272\n",
            "model saved to models_dqn/2023-05-05_18-21-56/q_network_ep_0736.pth\n",
            "\n",
            "episode  737  reward  -670.241668548883\n",
            "model saved to models_dqn/2023-05-05_18-21-57/q_network_ep_0737.pth\n",
            "\n",
            "episode  738  reward  -136.86640292416675\n",
            "model saved to models_dqn/2023-05-05_18-21-58/q_network_ep_0738.pth\n",
            "\n",
            "episode  739  reward  -202.50995683671331\n",
            "model saved to models_dqn/2023-05-05_18-21-58/q_network_ep_0739.pth\n",
            "\n",
            "episode  740  reward  -397.56869187640575\n",
            "model saved to models_dqn/2023-05-05_18-21-59/q_network_ep_0740.pth\n",
            "\n",
            "episode  741  reward  -40.332842077433064\n",
            "model saved to models_dqn/2023-05-05_18-22-00/q_network_ep_0741.pth\n",
            "\n",
            "episode  742  reward  -35.804151464760665\n",
            "model saved to models_dqn/2023-05-05_18-22-00/q_network_ep_0742.pth\n",
            "\n",
            "episode  743  reward  -33.04478007133528\n",
            "model saved to models_dqn/2023-05-05_18-22-01/q_network_ep_0743.pth\n",
            "\n",
            "episode  744  reward  -372.21337730605376\n",
            "model saved to models_dqn/2023-05-05_18-22-02/q_network_ep_0744.pth\n",
            "\n",
            "episode  745  reward  -23.772640017975263\n",
            "model saved to models_dqn/2023-05-05_18-22-02/q_network_ep_0745.pth\n",
            "\n",
            "episode  746  reward  -10.263016167203425\n",
            "model saved to models_dqn/2023-05-05_18-22-03/q_network_ep_0746.pth\n",
            "\n",
            "episode  747  reward  -27.61374714147067\n",
            "model saved to models_dqn/2023-05-05_18-22-04/q_network_ep_0747.pth\n",
            "\n",
            "episode  748  reward  -19.618815482775535\n",
            "model saved to models_dqn/2023-05-05_18-22-04/q_network_ep_0748.pth\n",
            "\n",
            "episode  749  reward  -15.221771884786287\n",
            "model saved to models_dqn/2023-05-05_18-22-05/q_network_ep_0749.pth\n",
            "\n",
            "episode  750  reward  -29.837947864944837\n",
            "model saved to models_dqn/2023-05-05_18-22-06/q_network_ep_0750.pth\n",
            "\n",
            "episode  751  reward  -15.611944265343874\n",
            "model saved to models_dqn/2023-05-05_18-22-07/q_network_ep_0751.pth\n",
            "\n",
            "episode  752  reward  -4.90887200781026\n",
            "model saved to models_dqn/2023-05-05_18-22-07/q_network_ep_0752.pth\n",
            "\n",
            "episode  753  reward  -11.239478525202715\n",
            "model saved to models_dqn/2023-05-05_18-22-08/q_network_ep_0753.pth\n",
            "\n",
            "episode  754  reward  -15.356546945462913\n",
            "model saved to models_dqn/2023-05-05_18-22-09/q_network_ep_0754.pth\n",
            "\n",
            "episode  755  reward  -15.31504756184305\n",
            "model saved to models_dqn/2023-05-05_18-22-10/q_network_ep_0755.pth\n",
            "\n",
            "episode  756  reward  -29.568691100953895\n",
            "model saved to models_dqn/2023-05-05_18-22-11/q_network_ep_0756.pth\n",
            "\n",
            "episode  757  reward  -28.960500802740892\n",
            "model saved to models_dqn/2023-05-05_18-22-12/q_network_ep_0757.pth\n",
            "\n",
            "episode  758  reward  -13.31639345474067\n",
            "model saved to models_dqn/2023-05-05_18-22-13/q_network_ep_0758.pth\n",
            "\n",
            "episode  759  reward  -11.583610277813209\n",
            "model saved to models_dqn/2023-05-05_18-22-13/q_network_ep_0759.pth\n",
            "\n",
            "episode  760  reward  -271.1649825714611\n",
            "model saved to models_dqn/2023-05-05_18-22-14/q_network_ep_0760.pth\n",
            "\n",
            "episode  761  reward  -14.2176091717561\n",
            "model saved to models_dqn/2023-05-05_18-22-15/q_network_ep_0761.pth\n",
            "\n",
            "episode  762  reward  -16.508175273393054\n",
            "model saved to models_dqn/2023-05-05_18-22-15/q_network_ep_0762.pth\n",
            "\n",
            "episode  763  reward  -10.742412392829728\n",
            "model saved to models_dqn/2023-05-05_18-22-16/q_network_ep_0763.pth\n",
            "\n",
            "episode  764  reward  -10.177127945911352\n",
            "model saved to models_dqn/2023-05-05_18-22-17/q_network_ep_0764.pth\n",
            "\n",
            "episode  765  reward  -19.767684294837164\n",
            "model saved to models_dqn/2023-05-05_18-22-17/q_network_ep_0765.pth\n",
            "\n",
            "episode  766  reward  -14.504978339055537\n",
            "model saved to models_dqn/2023-05-05_18-22-18/q_network_ep_0766.pth\n",
            "\n",
            "episode  767  reward  -21.26314221488835\n",
            "model saved to models_dqn/2023-05-05_18-22-19/q_network_ep_0767.pth\n",
            "\n",
            "episode  768  reward  -66.04832647094123\n",
            "model saved to models_dqn/2023-05-05_18-22-20/q_network_ep_0768.pth\n",
            "\n",
            "episode  769  reward  -14.557500619443493\n",
            "model saved to models_dqn/2023-05-05_18-22-20/q_network_ep_0769.pth\n",
            "\n",
            "episode  770  reward  -7.60838334226732\n",
            "model saved to models_dqn/2023-05-05_18-22-21/q_network_ep_0770.pth\n",
            "\n",
            "episode  771  reward  -10.714881588303658\n",
            "model saved to models_dqn/2023-05-05_18-22-22/q_network_ep_0771.pth\n",
            "\n",
            "episode  772  reward  -5.219442025301057\n",
            "model saved to models_dqn/2023-05-05_18-22-23/q_network_ep_0772.pth\n",
            "\n",
            "episode  773  reward  -5.587994546157754\n",
            "model saved to models_dqn/2023-05-05_18-22-24/q_network_ep_0773.pth\n",
            "\n",
            "episode  774  reward  -7.494404279039746\n",
            "model saved to models_dqn/2023-05-05_18-22-25/q_network_ep_0774.pth\n",
            "\n",
            "episode  775  reward  -17.290580465178767\n",
            "model saved to models_dqn/2023-05-05_18-22-26/q_network_ep_0775.pth\n",
            "\n",
            "episode  776  reward  -6.181266600992098\n",
            "model saved to models_dqn/2023-05-05_18-22-26/q_network_ep_0776.pth\n",
            "\n",
            "episode  777  reward  -4.754115724534217\n",
            "model saved to models_dqn/2023-05-05_18-22-27/q_network_ep_0777.pth\n",
            "\n",
            "episode  778  reward  -11.511930647108654\n",
            "model saved to models_dqn/2023-05-05_18-22-28/q_network_ep_0778.pth\n",
            "\n",
            "episode  779  reward  -31.294711295539678\n",
            "model saved to models_dqn/2023-05-05_18-22-28/q_network_ep_0779.pth\n",
            "\n",
            "episode  780  reward  -6.386812125958343\n",
            "model saved to models_dqn/2023-05-05_18-22-29/q_network_ep_0780.pth\n",
            "\n",
            "episode  781  reward  -18.886502683104332\n",
            "model saved to models_dqn/2023-05-05_18-22-30/q_network_ep_0781.pth\n",
            "\n",
            "episode  782  reward  -11.69331352240439\n",
            "model saved to models_dqn/2023-05-05_18-22-30/q_network_ep_0782.pth\n",
            "\n",
            "episode  783  reward  -8.86740760039577\n",
            "model saved to models_dqn/2023-05-05_18-22-31/q_network_ep_0783.pth\n",
            "\n",
            "episode  784  reward  -36.601525048357985\n",
            "model saved to models_dqn/2023-05-05_18-22-32/q_network_ep_0784.pth\n",
            "\n",
            "episode  785  reward  -10.490509936853012\n",
            "model saved to models_dqn/2023-05-05_18-22-33/q_network_ep_0785.pth\n",
            "\n",
            "episode  786  reward  -7.851290504688576\n",
            "model saved to models_dqn/2023-05-05_18-22-33/q_network_ep_0786.pth\n",
            "\n",
            "episode  787  reward  -8.569281949760548\n",
            "model saved to models_dqn/2023-05-05_18-22-34/q_network_ep_0787.pth\n",
            "\n",
            "episode  788  reward  -68.06455572939454\n",
            "model saved to models_dqn/2023-05-05_18-22-35/q_network_ep_0788.pth\n",
            "\n",
            "episode  789  reward  -8.862600668122152\n",
            "model saved to models_dqn/2023-05-05_18-22-36/q_network_ep_0789.pth\n",
            "\n",
            "episode  790  reward  -10.678881668564188\n",
            "model saved to models_dqn/2023-05-05_18-22-37/q_network_ep_0790.pth\n",
            "\n",
            "episode  791  reward  -8.75643576621646\n",
            "model saved to models_dqn/2023-05-05_18-22-37/q_network_ep_0791.pth\n",
            "\n",
            "episode  792  reward  -20.259014519160925\n",
            "model saved to models_dqn/2023-05-05_18-22-38/q_network_ep_0792.pth\n",
            "\n",
            "episode  793  reward  -4.047805878232089\n",
            "model saved to models_dqn/2023-05-05_18-22-39/q_network_ep_0793.pth\n",
            "\n",
            "episode  794  reward  -6.499928663420693\n",
            "model saved to models_dqn/2023-05-05_18-22-40/q_network_ep_0794.pth\n",
            "\n",
            "episode  795  reward  -8.69167430488632\n",
            "model saved to models_dqn/2023-05-05_18-22-40/q_network_ep_0795.pth\n",
            "\n",
            "episode  796  reward  -7.856864320381637\n",
            "model saved to models_dqn/2023-05-05_18-22-41/q_network_ep_0796.pth\n",
            "\n",
            "episode  797  reward  -8.965061298375543\n",
            "model saved to models_dqn/2023-05-05_18-22-42/q_network_ep_0797.pth\n",
            "\n",
            "episode  798  reward  -27.577505184320792\n",
            "model saved to models_dqn/2023-05-05_18-22-43/q_network_ep_0798.pth\n",
            "\n",
            "episode  799  reward  -4.060682472488301\n",
            "model saved to models_dqn/2023-05-05_18-22-43/q_network_ep_0799.pth\n",
            "\n",
            "episode  800  reward  -5.245360824494759\n",
            "model saved to models_dqn/2023-05-05_18-22-44/q_network_ep_0800.pth\n",
            "\n",
            "episode  801  reward  -9.026303982039462\n",
            "model saved to models_dqn/2023-05-05_18-22-45/q_network_ep_0801.pth\n",
            "\n",
            "episode  802  reward  -9.904643867664529\n",
            "model saved to models_dqn/2023-05-05_18-22-45/q_network_ep_0802.pth\n",
            "\n",
            "episode  803  reward  -3.398754503067979\n",
            "model saved to models_dqn/2023-05-05_18-22-46/q_network_ep_0803.pth\n",
            "\n",
            "episode  804  reward  -8.414800066410674\n",
            "model saved to models_dqn/2023-05-05_18-22-47/q_network_ep_0804.pth\n",
            "\n",
            "episode  805  reward  -11.658007207193883\n",
            "model saved to models_dqn/2023-05-05_18-22-47/q_network_ep_0805.pth\n",
            "\n",
            "episode  806  reward  -9.767294134785713\n",
            "model saved to models_dqn/2023-05-05_18-22-48/q_network_ep_0806.pth\n",
            "\n",
            "episode  807  reward  -27.092754958291334\n",
            "model saved to models_dqn/2023-05-05_18-22-49/q_network_ep_0807.pth\n",
            "\n",
            "episode  808  reward  -7.136744501213793\n",
            "model saved to models_dqn/2023-05-05_18-22-50/q_network_ep_0808.pth\n",
            "\n",
            "episode  809  reward  -8.264135542299215\n",
            "model saved to models_dqn/2023-05-05_18-22-51/q_network_ep_0809.pth\n",
            "\n",
            "episode  810  reward  -6.952372551436458\n",
            "model saved to models_dqn/2023-05-05_18-22-52/q_network_ep_0810.pth\n",
            "\n",
            "episode  811  reward  -5.23300343855954\n",
            "model saved to models_dqn/2023-05-05_18-22-53/q_network_ep_0811.pth\n",
            "\n",
            "episode  812  reward  -13.268992208046914\n",
            "model saved to models_dqn/2023-05-05_18-22-53/q_network_ep_0812.pth\n",
            "\n",
            "episode  813  reward  -9.902155194679416\n",
            "model saved to models_dqn/2023-05-05_18-22-54/q_network_ep_0813.pth\n",
            "\n",
            "episode  814  reward  -6.465639845443543\n",
            "model saved to models_dqn/2023-05-05_18-22-55/q_network_ep_0814.pth\n",
            "\n",
            "episode  815  reward  -22.97802044652466\n",
            "model saved to models_dqn/2023-05-05_18-22-55/q_network_ep_0815.pth\n",
            "\n",
            "episode  816  reward  -5.712348641915688\n",
            "model saved to models_dqn/2023-05-05_18-22-56/q_network_ep_0816.pth\n",
            "\n",
            "episode  817  reward  -22.28023223723244\n",
            "model saved to models_dqn/2023-05-05_18-22-57/q_network_ep_0817.pth\n",
            "\n",
            "episode  818  reward  -8.55877078132885\n",
            "model saved to models_dqn/2023-05-05_18-22-58/q_network_ep_0818.pth\n",
            "\n",
            "episode  819  reward  -5.498428540084545\n",
            "model saved to models_dqn/2023-05-05_18-22-58/q_network_ep_0819.pth\n",
            "\n",
            "episode  820  reward  -12.703797927543297\n",
            "model saved to models_dqn/2023-05-05_18-22-59/q_network_ep_0820.pth\n",
            "\n",
            "episode  821  reward  -5.7758503217804\n",
            "model saved to models_dqn/2023-05-05_18-23-00/q_network_ep_0821.pth\n",
            "\n",
            "episode  822  reward  -7.395676829142599\n",
            "model saved to models_dqn/2023-05-05_18-23-00/q_network_ep_0822.pth\n",
            "\n",
            "episode  823  reward  -8.531304034363684\n",
            "model saved to models_dqn/2023-05-05_18-23-01/q_network_ep_0823.pth\n",
            "\n",
            "episode  824  reward  -11.702987365048694\n",
            "model saved to models_dqn/2023-05-05_18-23-02/q_network_ep_0824.pth\n",
            "\n",
            "episode  825  reward  -9.317082538900173\n",
            "model saved to models_dqn/2023-05-05_18-23-03/q_network_ep_0825.pth\n",
            "\n",
            "episode  826  reward  -6.7468860725314075\n",
            "model saved to models_dqn/2023-05-05_18-23-04/q_network_ep_0826.pth\n",
            "\n",
            "episode  827  reward  -6.400391985796537\n",
            "model saved to models_dqn/2023-05-05_18-23-05/q_network_ep_0827.pth\n",
            "\n",
            "episode  828  reward  -5.288133112506421\n",
            "model saved to models_dqn/2023-05-05_18-23-06/q_network_ep_0828.pth\n",
            "\n",
            "episode  829  reward  -5.5469285262411\n",
            "model saved to models_dqn/2023-05-05_18-23-06/q_network_ep_0829.pth\n",
            "\n",
            "episode  830  reward  -11.271105058205494\n",
            "model saved to models_dqn/2023-05-05_18-23-07/q_network_ep_0830.pth\n",
            "\n",
            "episode  831  reward  -12.56368884535786\n",
            "model saved to models_dqn/2023-05-05_18-23-08/q_network_ep_0831.pth\n",
            "\n",
            "episode  832  reward  -32.76512653029786\n",
            "model saved to models_dqn/2023-05-05_18-23-09/q_network_ep_0832.pth\n",
            "\n",
            "episode  833  reward  -7.543862797932647\n",
            "model saved to models_dqn/2023-05-05_18-23-09/q_network_ep_0833.pth\n",
            "\n",
            "episode  834  reward  -6.429951800047824\n",
            "model saved to models_dqn/2023-05-05_18-23-10/q_network_ep_0834.pth\n",
            "\n",
            "episode  835  reward  -5.480616509909885\n",
            "model saved to models_dqn/2023-05-05_18-23-11/q_network_ep_0835.pth\n",
            "\n",
            "episode  836  reward  -5.843493525103442\n",
            "model saved to models_dqn/2023-05-05_18-23-12/q_network_ep_0836.pth\n",
            "\n",
            "episode  837  reward  -5.890731312632034\n",
            "model saved to models_dqn/2023-05-05_18-23-12/q_network_ep_0837.pth\n",
            "\n",
            "episode  838  reward  -9.075234714216942\n",
            "model saved to models_dqn/2023-05-05_18-23-13/q_network_ep_0838.pth\n",
            "\n",
            "episode  839  reward  -13.304572073077848\n",
            "model saved to models_dqn/2023-05-05_18-23-14/q_network_ep_0839.pth\n",
            "\n",
            "episode  840  reward  -12.875383055190866\n",
            "model saved to models_dqn/2023-05-05_18-23-14/q_network_ep_0840.pth\n",
            "\n",
            "episode  841  reward  -4.321022743171037\n",
            "model saved to models_dqn/2023-05-05_18-23-15/q_network_ep_0841.pth\n",
            "\n",
            "episode  842  reward  -9.290751539990273\n",
            "model saved to models_dqn/2023-05-05_18-23-16/q_network_ep_0842.pth\n",
            "\n",
            "episode  843  reward  -7.008851655848711\n",
            "model saved to models_dqn/2023-05-05_18-23-17/q_network_ep_0843.pth\n",
            "\n",
            "episode  844  reward  -8.285192803743495\n",
            "model saved to models_dqn/2023-05-05_18-23-18/q_network_ep_0844.pth\n",
            "\n",
            "episode  845  reward  -5.3270856689855925\n",
            "model saved to models_dqn/2023-05-05_18-23-19/q_network_ep_0845.pth\n",
            "\n",
            "episode  846  reward  -7.7354099016917495\n",
            "model saved to models_dqn/2023-05-05_18-23-20/q_network_ep_0846.pth\n",
            "\n",
            "episode  847  reward  -5.567717347331759\n",
            "model saved to models_dqn/2023-05-05_18-23-20/q_network_ep_0847.pth\n",
            "\n",
            "episode  848  reward  -10.840259151102183\n",
            "model saved to models_dqn/2023-05-05_18-23-21/q_network_ep_0848.pth\n",
            "\n",
            "episode  849  reward  -21.18784198970684\n",
            "model saved to models_dqn/2023-05-05_18-23-22/q_network_ep_0849.pth\n",
            "\n",
            "episode  850  reward  -4.168263087570241\n",
            "model saved to models_dqn/2023-05-05_18-23-22/q_network_ep_0850.pth\n",
            "\n",
            "episode  851  reward  -3.411940562068136\n",
            "model saved to models_dqn/2023-05-05_18-23-23/q_network_ep_0851.pth\n",
            "\n",
            "episode  852  reward  -8.232275110023522\n",
            "model saved to models_dqn/2023-05-05_18-23-24/q_network_ep_0852.pth\n",
            "\n",
            "episode  853  reward  -6.185084274930368\n",
            "model saved to models_dqn/2023-05-05_18-23-25/q_network_ep_0853.pth\n",
            "\n",
            "episode  854  reward  -13.70630048589842\n",
            "model saved to models_dqn/2023-05-05_18-23-25/q_network_ep_0854.pth\n",
            "\n",
            "episode  855  reward  -11.849975486528528\n",
            "model saved to models_dqn/2023-05-05_18-23-26/q_network_ep_0855.pth\n",
            "\n",
            "episode  856  reward  -3.7301602935504676\n",
            "model saved to models_dqn/2023-05-05_18-23-27/q_network_ep_0856.pth\n",
            "\n",
            "episode  857  reward  -4.788896913274022\n",
            "model saved to models_dqn/2023-05-05_18-23-27/q_network_ep_0857.pth\n",
            "\n",
            "episode  858  reward  -6.488028706801999\n",
            "model saved to models_dqn/2023-05-05_18-23-28/q_network_ep_0858.pth\n",
            "\n",
            "episode  859  reward  -3.54028916927859\n",
            "model saved to models_dqn/2023-05-05_18-23-29/q_network_ep_0859.pth\n",
            "\n",
            "episode  860  reward  -8.805846965898061\n",
            "model saved to models_dqn/2023-05-05_18-23-30/q_network_ep_0860.pth\n",
            "\n",
            "episode  861  reward  -8.286151430288994\n",
            "model saved to models_dqn/2023-05-05_18-23-31/q_network_ep_0861.pth\n",
            "\n",
            "episode  862  reward  -4.695230046275502\n",
            "model saved to models_dqn/2023-05-05_18-23-32/q_network_ep_0862.pth\n",
            "\n",
            "episode  863  reward  -3.518134790580735\n",
            "model saved to models_dqn/2023-05-05_18-23-33/q_network_ep_0863.pth\n",
            "\n",
            "episode  864  reward  -4.648884114347855\n",
            "model saved to models_dqn/2023-05-05_18-23-33/q_network_ep_0864.pth\n",
            "\n",
            "episode  865  reward  -7.751668093150129\n",
            "model saved to models_dqn/2023-05-05_18-23-34/q_network_ep_0865.pth\n",
            "\n",
            "episode  866  reward  -6.6593918781882735\n",
            "model saved to models_dqn/2023-05-05_18-23-35/q_network_ep_0866.pth\n",
            "\n",
            "episode  867  reward  -6.7215802892381\n",
            "model saved to models_dqn/2023-05-05_18-23-35/q_network_ep_0867.pth\n",
            "\n",
            "episode  868  reward  -7.657851992872776\n",
            "model saved to models_dqn/2023-05-05_18-23-36/q_network_ep_0868.pth\n",
            "\n",
            "episode  869  reward  -4.257027237162264\n",
            "model saved to models_dqn/2023-05-05_18-23-37/q_network_ep_0869.pth\n",
            "\n",
            "episode  870  reward  -8.921748185456305\n",
            "model saved to models_dqn/2023-05-05_18-23-37/q_network_ep_0870.pth\n",
            "\n",
            "episode  871  reward  -6.3381504639715915\n",
            "model saved to models_dqn/2023-05-05_18-23-38/q_network_ep_0871.pth\n",
            "\n",
            "episode  872  reward  -6.099502866225065\n",
            "model saved to models_dqn/2023-05-05_18-23-39/q_network_ep_0872.pth\n",
            "\n",
            "episode  873  reward  -17.726745989914154\n",
            "model saved to models_dqn/2023-05-05_18-23-40/q_network_ep_0873.pth\n",
            "\n",
            "episode  874  reward  -2.6490642147904944\n",
            "model saved to models_dqn/2023-05-05_18-23-40/q_network_ep_0874.pth\n",
            "\n",
            "episode  875  reward  -5.458318930238963\n",
            "model saved to models_dqn/2023-05-05_18-23-41/q_network_ep_0875.pth\n",
            "\n",
            "episode  876  reward  -4.77530968963238\n",
            "model saved to models_dqn/2023-05-05_18-23-42/q_network_ep_0876.pth\n",
            "\n",
            "episode  877  reward  -7.502074954511133\n",
            "model saved to models_dqn/2023-05-05_18-23-42/q_network_ep_0877.pth\n",
            "\n",
            "episode  878  reward  -2.807701800518772\n",
            "model saved to models_dqn/2023-05-05_18-23-43/q_network_ep_0878.pth\n",
            "\n",
            "episode  879  reward  -5.095179869358292\n",
            "model saved to models_dqn/2023-05-05_18-23-44/q_network_ep_0879.pth\n",
            "\n",
            "episode  880  reward  -7.352636469344431\n",
            "model saved to models_dqn/2023-05-05_18-23-45/q_network_ep_0880.pth\n",
            "\n",
            "episode  881  reward  -7.676246222174109\n",
            "model saved to models_dqn/2023-05-05_18-23-46/q_network_ep_0881.pth\n",
            "\n",
            "episode  882  reward  -3.4641823996668\n",
            "model saved to models_dqn/2023-05-05_18-23-47/q_network_ep_0882.pth\n",
            "\n",
            "episode  883  reward  -13.10255199868147\n",
            "model saved to models_dqn/2023-05-05_18-23-48/q_network_ep_0883.pth\n",
            "\n",
            "episode  884  reward  -10.614132598090906\n",
            "model saved to models_dqn/2023-05-05_18-23-48/q_network_ep_0884.pth\n",
            "\n",
            "episode  885  reward  -5.272536329921542\n",
            "model saved to models_dqn/2023-05-05_18-23-49/q_network_ep_0885.pth\n",
            "\n",
            "episode  886  reward  -13.5210952502025\n",
            "model saved to models_dqn/2023-05-05_18-23-50/q_network_ep_0886.pth\n",
            "\n",
            "episode  887  reward  -8.535361910947795\n",
            "model saved to models_dqn/2023-05-05_18-23-50/q_network_ep_0887.pth\n",
            "\n",
            "episode  888  reward  -5.926649394773255\n",
            "model saved to models_dqn/2023-05-05_18-23-51/q_network_ep_0888.pth\n",
            "\n",
            "episode  889  reward  -6.808065240304679\n",
            "model saved to models_dqn/2023-05-05_18-23-52/q_network_ep_0889.pth\n",
            "\n",
            "episode  890  reward  -5.275083684931788\n",
            "model saved to models_dqn/2023-05-05_18-23-53/q_network_ep_0890.pth\n",
            "\n",
            "episode  891  reward  -10.920271076486737\n",
            "model saved to models_dqn/2023-05-05_18-23-53/q_network_ep_0891.pth\n",
            "\n",
            "episode  892  reward  -6.392546481477449\n",
            "model saved to models_dqn/2023-05-05_18-23-54/q_network_ep_0892.pth\n",
            "\n",
            "episode  893  reward  -4.835578474403837\n",
            "model saved to models_dqn/2023-05-05_18-23-55/q_network_ep_0893.pth\n",
            "\n",
            "episode  894  reward  -43.555853605639506\n",
            "model saved to models_dqn/2023-05-05_18-23-55/q_network_ep_0894.pth\n",
            "\n",
            "episode  895  reward  -13.729934128035495\n",
            "model saved to models_dqn/2023-05-05_18-23-56/q_network_ep_0895.pth\n",
            "\n",
            "episode  896  reward  -3.5695196528606985\n",
            "model saved to models_dqn/2023-05-05_18-23-57/q_network_ep_0896.pth\n",
            "\n",
            "episode  897  reward  -7.1541286396314705\n",
            "model saved to models_dqn/2023-05-05_18-23-58/q_network_ep_0897.pth\n",
            "\n",
            "episode  898  reward  -10.904791240492823\n",
            "model saved to models_dqn/2023-05-05_18-23-59/q_network_ep_0898.pth\n",
            "\n",
            "episode  899  reward  -2.3823270257782587\n",
            "model saved to models_dqn/2023-05-05_18-24-00/q_network_ep_0899.pth\n",
            "\n",
            "episode  900  reward  -4.170278410046466\n",
            "model saved to models_dqn/2023-05-05_18-24-01/q_network_ep_0900.pth\n",
            "\n",
            "episode  901  reward  -2.202279720792218\n",
            "model saved to models_dqn/2023-05-05_18-24-01/q_network_ep_0901.pth\n",
            "\n",
            "episode  902  reward  -5.3169025205120715\n",
            "model saved to models_dqn/2023-05-05_18-24-02/q_network_ep_0902.pth\n",
            "\n",
            "episode  903  reward  -4.521915333687509\n",
            "model saved to models_dqn/2023-05-05_18-24-03/q_network_ep_0903.pth\n",
            "\n",
            "episode  904  reward  -891.688732875184\n",
            "model saved to models_dqn/2023-05-05_18-24-04/q_network_ep_0904.pth\n",
            "\n",
            "episode  905  reward  -773.7304263177053\n",
            "model saved to models_dqn/2023-05-05_18-24-04/q_network_ep_0905.pth\n",
            "\n",
            "episode  906  reward  -861.7489788175811\n",
            "model saved to models_dqn/2023-05-05_18-24-05/q_network_ep_0906.pth\n",
            "\n",
            "episode  907  reward  -645.3834205653483\n",
            "model saved to models_dqn/2023-05-05_18-24-06/q_network_ep_0907.pth\n",
            "\n",
            "episode  908  reward  -12.602191854611137\n",
            "model saved to models_dqn/2023-05-05_18-24-06/q_network_ep_0908.pth\n",
            "\n",
            "episode  909  reward  -578.5497723445185\n",
            "model saved to models_dqn/2023-05-05_18-24-07/q_network_ep_0909.pth\n",
            "\n",
            "episode  910  reward  -277.4860245148597\n",
            "model saved to models_dqn/2023-05-05_18-24-08/q_network_ep_0910.pth\n",
            "\n",
            "episode  911  reward  -495.1141602089501\n",
            "model saved to models_dqn/2023-05-05_18-24-09/q_network_ep_0911.pth\n",
            "\n",
            "episode  912  reward  -1.8202039859535704\n",
            "model saved to models_dqn/2023-05-05_18-24-09/q_network_ep_0912.pth\n",
            "\n",
            "episode  913  reward  -7.2654909491090525\n",
            "model saved to models_dqn/2023-05-05_18-24-10/q_network_ep_0913.pth\n",
            "\n",
            "episode  914  reward  -9.457736870596039\n",
            "model saved to models_dqn/2023-05-05_18-24-11/q_network_ep_0914.pth\n",
            "\n",
            "episode  915  reward  -485.67381316746525\n",
            "model saved to models_dqn/2023-05-05_18-24-12/q_network_ep_0915.pth\n",
            "\n",
            "episode  916  reward  -9.96092009870746\n",
            "model saved to models_dqn/2023-05-05_18-24-13/q_network_ep_0916.pth\n",
            "\n",
            "episode  917  reward  -40.76784837290768\n",
            "model saved to models_dqn/2023-05-05_18-24-14/q_network_ep_0917.pth\n",
            "\n",
            "episode  918  reward  -574.9446672874564\n",
            "model saved to models_dqn/2023-05-05_18-24-15/q_network_ep_0918.pth\n",
            "\n",
            "episode  919  reward  -4.58975133328794\n",
            "model saved to models_dqn/2023-05-05_18-24-15/q_network_ep_0919.pth\n",
            "\n",
            "episode  920  reward  -5.022954130864174\n",
            "model saved to models_dqn/2023-05-05_18-24-16/q_network_ep_0920.pth\n",
            "\n",
            "episode  921  reward  -24.12638168682597\n",
            "model saved to models_dqn/2023-05-05_18-24-17/q_network_ep_0921.pth\n",
            "\n",
            "episode  922  reward  -6.464826854374214\n",
            "model saved to models_dqn/2023-05-05_18-24-17/q_network_ep_0922.pth\n",
            "\n",
            "episode  923  reward  -17.323881639010324\n",
            "model saved to models_dqn/2023-05-05_18-24-18/q_network_ep_0923.pth\n",
            "\n",
            "episode  924  reward  -4.296703107965908\n",
            "model saved to models_dqn/2023-05-05_18-24-19/q_network_ep_0924.pth\n",
            "\n",
            "episode  925  reward  -8.630653993259406\n",
            "model saved to models_dqn/2023-05-05_18-24-20/q_network_ep_0925.pth\n",
            "\n",
            "episode  926  reward  -8.454542191501933\n",
            "model saved to models_dqn/2023-05-05_18-24-20/q_network_ep_0926.pth\n",
            "\n",
            "episode  927  reward  -11.369706472820031\n",
            "model saved to models_dqn/2023-05-05_18-24-21/q_network_ep_0927.pth\n",
            "\n",
            "episode  928  reward  -3.983026595279231\n",
            "model saved to models_dqn/2023-05-05_18-24-22/q_network_ep_0928.pth\n",
            "\n",
            "episode  929  reward  -114.62575138633419\n",
            "model saved to models_dqn/2023-05-05_18-24-22/q_network_ep_0929.pth\n",
            "\n",
            "episode  930  reward  -3.8225896052708737\n",
            "model saved to models_dqn/2023-05-05_18-24-23/q_network_ep_0930.pth\n",
            "\n",
            "episode  931  reward  -2.8021995994884663\n",
            "model saved to models_dqn/2023-05-05_18-24-24/q_network_ep_0931.pth\n",
            "\n",
            "episode  932  reward  -4.43282784672491\n",
            "model saved to models_dqn/2023-05-05_18-24-25/q_network_ep_0932.pth\n",
            "\n",
            "episode  933  reward  -7.781969362903069\n",
            "model saved to models_dqn/2023-05-05_18-24-26/q_network_ep_0933.pth\n",
            "\n",
            "episode  934  reward  -777.1506577643645\n",
            "model saved to models_dqn/2023-05-05_18-24-27/q_network_ep_0934.pth\n",
            "\n",
            "episode  935  reward  -7.196947638855356\n",
            "model saved to models_dqn/2023-05-05_18-24-28/q_network_ep_0935.pth\n",
            "\n",
            "episode  936  reward  -5.096870528942087\n",
            "model saved to models_dqn/2023-05-05_18-24-28/q_network_ep_0936.pth\n",
            "\n",
            "episode  937  reward  -13.994486365663722\n",
            "model saved to models_dqn/2023-05-05_18-24-29/q_network_ep_0937.pth\n",
            "\n",
            "episode  938  reward  -11.03352113219377\n",
            "model saved to models_dqn/2023-05-05_18-24-30/q_network_ep_0938.pth\n",
            "\n",
            "episode  939  reward  -9.704136681493955\n",
            "model saved to models_dqn/2023-05-05_18-24-30/q_network_ep_0939.pth\n",
            "\n",
            "episode  940  reward  -61.008434863033955\n",
            "model saved to models_dqn/2023-05-05_18-24-31/q_network_ep_0940.pth\n",
            "\n",
            "episode  941  reward  -8.622302756358408\n",
            "model saved to models_dqn/2023-05-05_18-24-32/q_network_ep_0941.pth\n",
            "\n",
            "episode  942  reward  -3.136708640157752\n",
            "model saved to models_dqn/2023-05-05_18-24-32/q_network_ep_0942.pth\n",
            "\n",
            "episode  943  reward  -83.17845907423118\n",
            "model saved to models_dqn/2023-05-05_18-24-33/q_network_ep_0943.pth\n",
            "\n",
            "episode  944  reward  -7.837727895395764\n",
            "model saved to models_dqn/2023-05-05_18-24-34/q_network_ep_0944.pth\n",
            "\n",
            "episode  945  reward  -257.7974438394877\n",
            "model saved to models_dqn/2023-05-05_18-24-34/q_network_ep_0945.pth\n",
            "\n",
            "episode  946  reward  -32.6273695663111\n",
            "model saved to models_dqn/2023-05-05_18-24-35/q_network_ep_0946.pth\n",
            "\n",
            "episode  947  reward  -6.060990348308752\n",
            "model saved to models_dqn/2023-05-05_18-24-36/q_network_ep_0947.pth\n",
            "\n",
            "episode  948  reward  -3.09404469223925\n",
            "model saved to models_dqn/2023-05-05_18-24-37/q_network_ep_0948.pth\n",
            "\n",
            "episode  949  reward  -5.885321812427301\n",
            "model saved to models_dqn/2023-05-05_18-24-37/q_network_ep_0949.pth\n",
            "\n",
            "episode  950  reward  -9.676689293071162\n",
            "model saved to models_dqn/2023-05-05_18-24-38/q_network_ep_0950.pth\n",
            "\n",
            "episode  951  reward  -9.984304896769343\n",
            "model saved to models_dqn/2023-05-05_18-24-39/q_network_ep_0951.pth\n",
            "\n",
            "episode  952  reward  -8.128953709031956\n",
            "model saved to models_dqn/2023-05-05_18-24-40/q_network_ep_0952.pth\n",
            "\n",
            "episode  953  reward  -91.02476283118537\n",
            "model saved to models_dqn/2023-05-05_18-24-41/q_network_ep_0953.pth\n",
            "\n",
            "episode  954  reward  -4.359303796972877\n",
            "model saved to models_dqn/2023-05-05_18-24-42/q_network_ep_0954.pth\n",
            "\n",
            "episode  955  reward  -2.347652225643859\n",
            "model saved to models_dqn/2023-05-05_18-24-42/q_network_ep_0955.pth\n",
            "\n",
            "episode  956  reward  -23.517605612923393\n",
            "model saved to models_dqn/2023-05-05_18-24-43/q_network_ep_0956.pth\n",
            "\n",
            "episode  957  reward  -4.024302904790332\n",
            "model saved to models_dqn/2023-05-05_18-24-44/q_network_ep_0957.pth\n",
            "\n",
            "episode  958  reward  -3.0448072680681237\n",
            "model saved to models_dqn/2023-05-05_18-24-45/q_network_ep_0958.pth\n",
            "\n",
            "episode  959  reward  -2.749089526717072\n",
            "model saved to models_dqn/2023-05-05_18-24-45/q_network_ep_0959.pth\n",
            "\n",
            "episode  960  reward  -2.9957003814332044\n",
            "model saved to models_dqn/2023-05-05_18-24-46/q_network_ep_0960.pth\n",
            "\n",
            "episode  961  reward  -15.5349890709087\n",
            "model saved to models_dqn/2023-05-05_18-24-47/q_network_ep_0961.pth\n",
            "\n",
            "episode  962  reward  -3.3737766201255237\n",
            "model saved to models_dqn/2023-05-05_18-24-47/q_network_ep_0962.pth\n",
            "\n",
            "episode  963  reward  -22.462316976336247\n",
            "model saved to models_dqn/2023-05-05_18-24-48/q_network_ep_0963.pth\n",
            "\n",
            "episode  964  reward  -29.041910094649463\n",
            "model saved to models_dqn/2023-05-05_18-24-49/q_network_ep_0964.pth\n",
            "\n",
            "episode  965  reward  -12.016328108180856\n",
            "model saved to models_dqn/2023-05-05_18-24-49/q_network_ep_0965.pth\n",
            "\n",
            "episode  966  reward  -9.071573408199297\n",
            "model saved to models_dqn/2023-05-05_18-24-50/q_network_ep_0966.pth\n",
            "\n",
            "episode  967  reward  -4.5408586104297415\n",
            "model saved to models_dqn/2023-05-05_18-24-51/q_network_ep_0967.pth\n",
            "\n",
            "episode  968  reward  -8.496925012799725\n",
            "model saved to models_dqn/2023-05-05_18-24-52/q_network_ep_0968.pth\n",
            "\n",
            "episode  969  reward  -2.510272190668364\n",
            "model saved to models_dqn/2023-05-05_18-24-53/q_network_ep_0969.pth\n",
            "\n",
            "episode  970  reward  -5.5006547699542265\n",
            "model saved to models_dqn/2023-05-05_18-24-54/q_network_ep_0970.pth\n",
            "\n",
            "episode  971  reward  -5.45446478582448\n",
            "model saved to models_dqn/2023-05-05_18-24-55/q_network_ep_0971.pth\n",
            "\n",
            "episode  972  reward  -6.880444220264827\n",
            "model saved to models_dqn/2023-05-05_18-24-55/q_network_ep_0972.pth\n",
            "\n",
            "episode  973  reward  -14.881601886038599\n",
            "model saved to models_dqn/2023-05-05_18-24-56/q_network_ep_0973.pth\n",
            "\n",
            "episode  974  reward  -3.4020956914943747\n",
            "model saved to models_dqn/2023-05-05_18-24-57/q_network_ep_0974.pth\n",
            "\n",
            "episode  975  reward  -16.209036424083703\n",
            "model saved to models_dqn/2023-05-05_18-24-57/q_network_ep_0975.pth\n",
            "\n",
            "episode  976  reward  -2.910016984994453\n",
            "model saved to models_dqn/2023-05-05_18-24-58/q_network_ep_0976.pth\n",
            "\n",
            "episode  977  reward  -3.8185718882017596\n",
            "model saved to models_dqn/2023-05-05_18-24-59/q_network_ep_0977.pth\n",
            "\n",
            "episode  978  reward  -6.379098585743456\n",
            "model saved to models_dqn/2023-05-05_18-24-59/q_network_ep_0978.pth\n",
            "\n",
            "episode  979  reward  -1.8034980448108784\n",
            "model saved to models_dqn/2023-05-05_18-25-00/q_network_ep_0979.pth\n",
            "\n",
            "episode  980  reward  -17.530556801350144\n",
            "model saved to models_dqn/2023-05-05_18-25-01/q_network_ep_0980.pth\n",
            "\n",
            "episode  981  reward  -4.5073460471531135\n",
            "model saved to models_dqn/2023-05-05_18-25-02/q_network_ep_0981.pth\n",
            "\n",
            "episode  982  reward  -5.45844714718173\n",
            "model saved to models_dqn/2023-05-05_18-25-02/q_network_ep_0982.pth\n",
            "\n",
            "episode  983  reward  -2.553273093559172\n",
            "model saved to models_dqn/2023-05-05_18-25-03/q_network_ep_0983.pth\n",
            "\n",
            "episode  984  reward  -4.502444929780337\n",
            "model saved to models_dqn/2023-05-05_18-25-04/q_network_ep_0984.pth\n",
            "\n",
            "episode  985  reward  -3.019677757178039\n",
            "model saved to models_dqn/2023-05-05_18-25-04/q_network_ep_0985.pth\n",
            "\n",
            "episode  986  reward  -4.302011245675931\n",
            "model saved to models_dqn/2023-05-05_18-25-05/q_network_ep_0986.pth\n",
            "\n",
            "episode  987  reward  -3.060754395283359\n",
            "model saved to models_dqn/2023-05-05_18-25-07/q_network_ep_0987.pth\n",
            "\n",
            "episode  988  reward  -3.824139333055127\n",
            "model saved to models_dqn/2023-05-05_18-25-08/q_network_ep_0988.pth\n",
            "\n",
            "episode  989  reward  -3.7349797536876865\n",
            "model saved to models_dqn/2023-05-05_18-25-08/q_network_ep_0989.pth\n",
            "\n",
            "episode  990  reward  -3.599738755420671\n",
            "model saved to models_dqn/2023-05-05_18-25-09/q_network_ep_0990.pth\n",
            "\n",
            "episode  991  reward  -6.3087062092508175\n",
            "model saved to models_dqn/2023-05-05_18-25-10/q_network_ep_0991.pth\n",
            "\n",
            "episode  992  reward  -14.413805878272417\n",
            "model saved to models_dqn/2023-05-05_18-25-10/q_network_ep_0992.pth\n",
            "\n",
            "episode  993  reward  -3.2181455161723025\n",
            "model saved to models_dqn/2023-05-05_18-25-11/q_network_ep_0993.pth\n",
            "\n",
            "episode  994  reward  -1.7654682731021132\n",
            "model saved to models_dqn/2023-05-05_18-25-12/q_network_ep_0994.pth\n",
            "\n",
            "episode  995  reward  -5.850524893205998\n",
            "model saved to models_dqn/2023-05-05_18-25-13/q_network_ep_0995.pth\n",
            "\n",
            "episode  996  reward  -1.8075678612025003\n",
            "model saved to models_dqn/2023-05-05_18-25-13/q_network_ep_0996.pth\n",
            "\n",
            "episode  997  reward  -3.8585953112085707\n",
            "model saved to models_dqn/2023-05-05_18-25-14/q_network_ep_0997.pth\n",
            "\n",
            "episode  998  reward  -10.690252376896135\n",
            "model saved to models_dqn/2023-05-05_18-25-15/q_network_ep_0998.pth\n",
            "\n",
            "episode  999  reward  -4.584288844664204\n",
            "model saved to models_dqn/2023-05-05_18-25-15/q_network_ep_0999.pth\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# ---------------\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "env = ArmEnv(arm, gui=False)\n",
        "tqdn = TrainDQN(env)\n",
        "# ---------------\n",
        "\n",
        "# Call your trin function here\n",
        "tqdn.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep track of your experiments, it is good practice to plot and check how well is your model trained based on the returns vs episodes plot. With a large number of episodes, this  plot may look very jagged making it difficult to ascertain how well you are doing. We are proving code to smoothen out the plot by. This will take a large list of returns in every episode and plot a smoothened version of the list. Feel free to use it if it helps.\n",
        "```\n",
        "import seaborn as sns\n",
        "returns = __\n",
        "smoothing = 10\n",
        "\n",
        "smoothened = [sum(returns[i:i+smoothing])/smoothing for i in range(0, len(returns), smoothing)]\n",
        "sns.lineplot(smoothened)\n",
        "```"
      ],
      "metadata": {
        "id": "jAlaOcVJsn5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcBDbeTRNZI"
      },
      "source": [
        "### Load your model and test its performance\n",
        "Change your model path and the goal to see how well your learnt model is performing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gTRNKhRQQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c703ff56-861c-4006-c74c-d758bbd55e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return:  -3.4231788190484593\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "import os\n",
        "from math import dist\n",
        "import seaborn as sns\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from geometry import polar2cartesian\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "\"\"\"\n",
        "\n",
        "by intuition intead of lowest value, around 595 episode the values still seem to in the lower ranges so it is chosen as the best model\n",
        "\n",
        "episode  595  reward  -0.8190010605977223\n",
        "model saved to models_dqn/2023-05-05_18-20-10/q_network_ep_0595.pth\n",
        "\n",
        "episode  596  reward  -0.9297720951654287\n",
        "model saved to models_dqn/2023-05-05_18-20-10/q_network_ep_0596.pth\n",
        "\n",
        "episode  597  reward  -3.1944489778013\n",
        "model saved to models_dqn/2023-05-05_18-20-11/q_network_ep_0597.pth\n",
        "\n",
        "episode  598  reward  -1.9837620717889426\n",
        "model saved to models_dqn/2023-05-05_18-20-12/q_network_ep_0598.pth\n",
        "\n",
        "instead of the expected lowest reward episode - 439 around which the adjacent episodes seem to be not doing as good.\n",
        "\n",
        "episode  437  reward  -33.44507891301291\n",
        "model saved to models_dqn/2023-05-05_18-18-11/q_network_ep_0437.pth\n",
        "\n",
        "episode  438  reward  -3.148843096325622\n",
        "model saved to models_dqn/2023-05-05_18-18-11/q_network_ep_0438.pth\n",
        "\n",
        "episode  439  reward  -0.2692806549995741\n",
        "best return so far is  -0.2692806549995741  in episode  439\n",
        "model saved to models_dqn/2023-05-05_18-18-12/q_network_ep_0439.pth\n",
        "\n",
        "episode  440  reward  -18.199466018558656\n",
        "model saved to models_dqn/2023-05-05_18-18-13/q_network_ep_0440.pth\n",
        "\n",
        "episode  441  reward  -11.805314167507813\n",
        "model saved to models_dqn/2023-05-05_18-18-13/q_network_ep_0441.pth\n",
        "\n",
        "\"\"\"\n",
        "model_path = 'models_dqn/2023-05-05_18-20-10/q_network_ep_0595.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "goal = polar2cartesian(1.6, 0.25 - np.pi/2.0)\n",
        "done = False\n",
        "obs = env.reset(goal)\n",
        "\n",
        "episode_return = 0\n",
        "while not done:\n",
        "  action = qnet.select_discrete_action(obs, device)\n",
        "  action = qnet.action_discrete_to_continuous(action)\n",
        "  new_obs, reward, done, info = env.step(action)\n",
        "  episode_return += reward\n",
        "\n",
        "  pos_ee = info['pos_ee']\n",
        "  vel_ee = info['vel_ee']\n",
        "  dist = np.linalg.norm(pos_ee - goal)\n",
        "\n",
        "  obs = new_obs\n",
        "print('Episode return: ', episode_return)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading and Evaluation\n",
        "You will be evaluated on 5 different goal positions worth 1.5 points each. You must pass the best `model_path` for your network. The scoring function will run one episode for every goal position and find the total reward (aka return) for the episode. For every goal you get:\n",
        "\n",
        "* 1 Point if `easy target < total reward < hard target`\n",
        "* 1.5 Points if `hard target < total reward`"
      ],
      "metadata": {
        "id": "pUDEYLsZLSp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import compute_score\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "#\n",
        "model_path = 'models_dqn/2023-05-05_18-20-10/q_network_ep_0595.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "score = compute_score(qnet, env, device)"
      ],
      "metadata": {
        "id": "OhPD-u6TIxdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7744195-fea5-406a-dfc7-ebac6456c014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -3.1450429782842284\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -3.4231788190484593\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -2.93148500530119\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -4.722733512233445\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -4.807430618233105\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 1.5\n",
            "\n",
            "\n",
            "Final score: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: PPO with an open source RL library\n",
        "\n",
        "In this part, you will use one of the most popular open source RL libraries ([Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)) to solve the same goal reaching problem as Part 1. We will use the same `ArmEnv` gym environment. The algorithm you should choose to use is PPO."
      ],
      "metadata": {
        "id": "vkCvO0-05XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO training\n",
        "\n",
        "We provide the code to construct parallel environments. Parallel environments can be very useful if you have good CPUs and it can speed up training."
      ],
      "metadata": {
        "id": "UBK89P2B8CgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
        "from copy import deepcopy\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from arm_env import ArmEnv\n",
        "\n",
        "class EnvMaker:\n",
        "    def __init__(self,  arm, seed):\n",
        "        self.seed = seed\n",
        "        self.arm = arm\n",
        "\n",
        "    def __call__(self):\n",
        "        arm = deepcopy(self.arm)\n",
        "        env = ArmEnv(arm)\n",
        "        env.seed(self.seed)\n",
        "        return env\n",
        "\n",
        "def make_vec_env(arm, nenv, seed):\n",
        "    return VecMonitor(SubprocVecEnv([EnvMaker(arm, seed  + 100 * i) for i in range(nenv)]))\n",
        "\n",
        "# conveniet function to create a robot arm\n",
        "def make_arm():\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01\n",
        "        )\n",
        "    )\n",
        "    arm.reset()\n",
        "    return arm\n"
      ],
      "metadata": {
        "id": "2RTqfmpVwMja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to complete the code to train the policy using the [PPO class](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) from stable_baselines3. We provide the code to generate the name of the directory to save the checkpoint, an example is `ppo_models/2023-04-13_01-14-13`. Your checkpoint model should be named `ppo_network.zip`. See the [save](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.save) function. Training should take less than 40 minutes."
      ],
      "metadata": {
        "id": "Bniz2TouwM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo import PPO\n",
        "import os\n",
        "import time\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "\n",
        "# Default parameters\n",
        "timesteps = 900000\n",
        "nenv = 8  # number of parallel environments. This can speed up training when you have good CPUs\n",
        "seed = 8\n",
        "batch_size = 2048\n",
        "\n",
        "# Generate path of the directory to save the checkpoint\n",
        "timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_dir = os.path.join('ppo_models', timestr)\n",
        "\n",
        "# Set random seed\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Create arm\n",
        "arm = make_arm()\n",
        "\n",
        "# Create parallel envs\n",
        "vec_env = make_vec_env(arm=arm, nenv=nenv, seed=seed)\n",
        "\n",
        "# ------ IMPLEMENT YOUR TRAINING CODE HERE ------------\n",
        "model = PPO(\"MlpPolicy\", vec_env, batch_size=batch_size, n_epochs=18,seed=seed, verbose=1)\n",
        "model.learn(total_timesteps=timesteps)\n",
        "model.save(os.path.join(save_dir, \"ppo_network\"))\n",
        "#raise NotImplementedError\n",
        "\n",
        "# Do not forget to save your model at the end of training"
      ],
      "metadata": {
        "id": "FHoSWOnG-2sH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6faba48f-44ca-4336-9ee6-0f212eff865b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "-----------------------------------\n",
            "| rollout/           |            |\n",
            "|    ep_len_mean     | 200        |\n",
            "|    ep_rew_mean     | -200.53925 |\n",
            "| time/              |            |\n",
            "|    fps             | 834        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 19         |\n",
            "|    total_timesteps | 16384      |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -145.82999  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 813         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 40          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004575022 |\n",
            "|    clip_fraction        | 0.0402      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.84       |\n",
            "|    explained_variance   | 0.00222     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 212         |\n",
            "|    n_updates            | 18          |\n",
            "|    policy_gradient_loss | -0.00478    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 481         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -112.84083   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 812          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 60           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034336376 |\n",
            "|    clip_fraction        | 0.0202       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | -0.0117      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 61           |\n",
            "|    n_updates            | 36           |\n",
            "|    policy_gradient_loss | -0.00289     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 145          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -70.595345   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 795          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 82           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037753512 |\n",
            "|    clip_fraction        | 0.0244       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | -0.0506      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 55.1         |\n",
            "|    n_updates            | 54           |\n",
            "|    policy_gradient_loss | -0.00315     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 105          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -57.329395   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 798          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 102          |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054514036 |\n",
            "|    clip_fraction        | 0.0481       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | -0.00374     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18           |\n",
            "|    n_updates            | 72           |\n",
            "|    policy_gradient_loss | -0.00518     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 39.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -43.065777   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 789          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 124          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041104616 |\n",
            "|    clip_fraction        | 0.0426       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | -0.00491     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.7         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00496     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 25.5         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -33.785885  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 784         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 146         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003416121 |\n",
            "|    clip_fraction        | 0.0234      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.86       |\n",
            "|    explained_variance   | -0.00105    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.2         |\n",
            "|    n_updates            | 108         |\n",
            "|    policy_gradient_loss | -0.00256    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 16.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -27.499424 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 777        |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 168        |\n",
            "|    total_timesteps      | 131072     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00333424 |\n",
            "|    clip_fraction        | 0.027      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.86      |\n",
            "|    explained_variance   | -0.000682  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.98       |\n",
            "|    n_updates            | 126        |\n",
            "|    policy_gradient_loss | -0.00185   |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 11.5       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -23.498476   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 775          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 190          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046363384 |\n",
            "|    clip_fraction        | 0.02         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | -0.000814    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.22         |\n",
            "|    n_updates            | 144          |\n",
            "|    policy_gradient_loss | -0.00216     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -21.906713   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 761          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 215          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042088632 |\n",
            "|    clip_fraction        | 0.0258       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0.000595     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.84         |\n",
            "|    n_updates            | 162          |\n",
            "|    policy_gradient_loss | -0.00233     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 8.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -19.89693    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 741          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 242          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033819187 |\n",
            "|    clip_fraction        | 0.0167       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0.000787     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.94         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00148     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 6.2          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -20.561802   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 741          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 265          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041025924 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | 0.00284      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.35         |\n",
            "|    n_updates            | 198          |\n",
            "|    policy_gradient_loss | -0.00301     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.12         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -24.38187    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 730          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 291          |\n",
            "|    total_timesteps      | 212992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047878884 |\n",
            "|    clip_fraction        | 0.0289       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.83        |\n",
            "|    explained_variance   | 0.00587      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.21         |\n",
            "|    n_updates            | 216          |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    std                  | 0.997        |\n",
            "|    value_loss           | 4.43         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -24.739368  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 723         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 317         |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002882786 |\n",
            "|    clip_fraction        | 0.0209      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.83       |\n",
            "|    explained_variance   | 0.0045      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.6        |\n",
            "|    n_updates            | 234         |\n",
            "|    policy_gradient_loss | -0.00399    |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 30.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -19.02746    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 716          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 343          |\n",
            "|    total_timesteps      | 245760       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043664556 |\n",
            "|    clip_fraction        | 0.0204       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.82        |\n",
            "|    explained_variance   | 0.00127      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.92         |\n",
            "|    n_updates            | 252          |\n",
            "|    policy_gradient_loss | -0.00163     |\n",
            "|    std                  | 0.991        |\n",
            "|    value_loss           | 3.74         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -18.72779   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 369         |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004557014 |\n",
            "|    clip_fraction        | 0.0386      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.82       |\n",
            "|    explained_variance   | 0.0247      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.55        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.00268    |\n",
            "|    std                  | 0.988       |\n",
            "|    value_loss           | 3.18        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -19.56647    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 711          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 391          |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039372295 |\n",
            "|    clip_fraction        | 0.0266       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.82        |\n",
            "|    explained_variance   | 0.055        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.48         |\n",
            "|    n_updates            | 288          |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    std                  | 0.99         |\n",
            "|    value_loss           | 3.13         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -29.933949  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 415         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003636607 |\n",
            "|    clip_fraction        | 0.0295      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.81       |\n",
            "|    explained_variance   | 0.0279      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.31        |\n",
            "|    n_updates            | 306         |\n",
            "|    policy_gradient_loss | -0.00263    |\n",
            "|    std                  | 0.986       |\n",
            "|    value_loss           | 4.52        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -20.785395   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 703          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 442          |\n",
            "|    total_timesteps      | 311296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026095028 |\n",
            "|    clip_fraction        | 0.0126       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.81        |\n",
            "|    explained_variance   | 0.0116       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.6         |\n",
            "|    n_updates            | 324          |\n",
            "|    policy_gradient_loss | -0.0021      |\n",
            "|    std                  | 0.988        |\n",
            "|    value_loss           | 47.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -15.921745   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 699          |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 468          |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044712042 |\n",
            "|    clip_fraction        | 0.0275       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.8         |\n",
            "|    explained_variance   | 0.0761       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.83         |\n",
            "|    n_updates            | 342          |\n",
            "|    policy_gradient_loss | -0.0024      |\n",
            "|    std                  | 0.979        |\n",
            "|    value_loss           | 3.4          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -14.472262  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 700         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 491         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004098176 |\n",
            "|    clip_fraction        | 0.0272      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.79       |\n",
            "|    explained_variance   | 0.115       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.37        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00222    |\n",
            "|    std                  | 0.973       |\n",
            "|    value_loss           | 2.89        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -14.65132   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 696         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 517         |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004726529 |\n",
            "|    clip_fraction        | 0.0252      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.77       |\n",
            "|    explained_variance   | 0.149       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.28        |\n",
            "|    n_updates            | 378         |\n",
            "|    policy_gradient_loss | -0.00214    |\n",
            "|    std                  | 0.963       |\n",
            "|    value_loss           | 2.45        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -11.674791   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 700          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 538          |\n",
            "|    total_timesteps      | 376832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046392055 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.75        |\n",
            "|    explained_variance   | 0.13         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.03         |\n",
            "|    n_updates            | 396          |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    std                  | 0.957        |\n",
            "|    value_loss           | 4.23         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -10.036149   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 702          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 559          |\n",
            "|    total_timesteps      | 393216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039186934 |\n",
            "|    clip_fraction        | 0.0337       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.74        |\n",
            "|    explained_variance   | 0.245        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.663        |\n",
            "|    n_updates            | 414          |\n",
            "|    policy_gradient_loss | -0.00257     |\n",
            "|    std                  | 0.947        |\n",
            "|    value_loss           | 1.35         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -10.322985   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 704          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 581          |\n",
            "|    total_timesteps      | 409600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047854423 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.71        |\n",
            "|    explained_variance   | 0.323        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.54         |\n",
            "|    n_updates            | 432          |\n",
            "|    policy_gradient_loss | -0.00322     |\n",
            "|    std                  | 0.932        |\n",
            "|    value_loss           | 1.08         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -8.642435   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 602         |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004959253 |\n",
            "|    clip_fraction        | 0.044       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.68       |\n",
            "|    explained_variance   | 0.352       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.475       |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00318    |\n",
            "|    std                  | 0.921       |\n",
            "|    value_loss           | 1.03        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -8.671527   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 623         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004148899 |\n",
            "|    clip_fraction        | 0.043       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.66       |\n",
            "|    explained_variance   | 0.38        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.422       |\n",
            "|    n_updates            | 468         |\n",
            "|    policy_gradient_loss | -0.00362    |\n",
            "|    std                  | 0.916       |\n",
            "|    value_loss           | 0.9         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -8.146843    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 711          |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 644          |\n",
            "|    total_timesteps      | 458752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029476907 |\n",
            "|    clip_fraction        | 0.0233       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.66        |\n",
            "|    explained_variance   | 0.478        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.341        |\n",
            "|    n_updates            | 486          |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    std                  | 0.915        |\n",
            "|    value_loss           | 0.687        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -7.101455    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 714          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 664          |\n",
            "|    total_timesteps      | 475136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034242503 |\n",
            "|    clip_fraction        | 0.0218       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.65        |\n",
            "|    explained_variance   | 0.515        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.247        |\n",
            "|    n_updates            | 504          |\n",
            "|    policy_gradient_loss | -0.00276     |\n",
            "|    std                  | 0.905        |\n",
            "|    value_loss           | 0.531        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -6.629575    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 715          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 686          |\n",
            "|    total_timesteps      | 491520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045403014 |\n",
            "|    clip_fraction        | 0.0451       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.63        |\n",
            "|    explained_variance   | 0.607        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.157        |\n",
            "|    n_updates            | 522          |\n",
            "|    policy_gradient_loss | -0.0038      |\n",
            "|    std                  | 0.903        |\n",
            "|    value_loss           | 0.372        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -6.0636654  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 718         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 706         |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004337473 |\n",
            "|    clip_fraction        | 0.0382      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.62       |\n",
            "|    explained_variance   | 0.647       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.143       |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00306    |\n",
            "|    std                  | 0.894       |\n",
            "|    value_loss           | 0.339       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -5.6923757   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 719          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 728          |\n",
            "|    total_timesteps      | 524288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033876686 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.6         |\n",
            "|    explained_variance   | 0.681        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0946       |\n",
            "|    n_updates            | 558          |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    std                  | 0.888        |\n",
            "|    value_loss           | 0.231        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -5.683853   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 721         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 749         |\n",
            "|    total_timesteps      | 540672      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005267233 |\n",
            "|    clip_fraction        | 0.0383      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.59       |\n",
            "|    explained_variance   | 0.673       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0945      |\n",
            "|    n_updates            | 576         |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    std                  | 0.884       |\n",
            "|    value_loss           | 0.188       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -5.503865    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 721          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 772          |\n",
            "|    total_timesteps      | 557056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046148445 |\n",
            "|    clip_fraction        | 0.0367       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.57        |\n",
            "|    explained_variance   | 0.72         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0806       |\n",
            "|    n_updates            | 594          |\n",
            "|    policy_gradient_loss | -0.0037      |\n",
            "|    std                  | 0.866        |\n",
            "|    value_loss           | 0.185        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -5.5678697   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 722          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 793          |\n",
            "|    total_timesteps      | 573440       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055959392 |\n",
            "|    clip_fraction        | 0.0419       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.53        |\n",
            "|    explained_variance   | 0.756        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0529       |\n",
            "|    n_updates            | 612          |\n",
            "|    policy_gradient_loss | -0.00403     |\n",
            "|    std                  | 0.854        |\n",
            "|    value_loss           | 0.145        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -7.3570514   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 724          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 814          |\n",
            "|    total_timesteps      | 589824       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050896006 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.51        |\n",
            "|    explained_variance   | 0.78         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0386       |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.00564     |\n",
            "|    std                  | 0.849        |\n",
            "|    value_loss           | 0.106        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -20.25363    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 726          |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 834          |\n",
            "|    total_timesteps      | 606208       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014039611 |\n",
            "|    clip_fraction        | 0.0109       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.51        |\n",
            "|    explained_variance   | 0.062        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.99         |\n",
            "|    n_updates            | 648          |\n",
            "|    policy_gradient_loss | -0.00167     |\n",
            "|    std                  | 0.851        |\n",
            "|    value_loss           | 24.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -16.521751   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 727          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 855          |\n",
            "|    total_timesteps      | 622592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030012913 |\n",
            "|    clip_fraction        | 0.00918      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.51        |\n",
            "|    explained_variance   | 0.241        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.2         |\n",
            "|    n_updates            | 666          |\n",
            "|    policy_gradient_loss | -0.0016      |\n",
            "|    std                  | 0.85         |\n",
            "|    value_loss           | 36           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -15.415462   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 729          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 876          |\n",
            "|    total_timesteps      | 638976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020165085 |\n",
            "|    clip_fraction        | 0.00849      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.51        |\n",
            "|    explained_variance   | 0.297        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.1         |\n",
            "|    n_updates            | 684          |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    std                  | 0.851        |\n",
            "|    value_loss           | 32.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -10.964039   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 729          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 897          |\n",
            "|    total_timesteps      | 655360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027439222 |\n",
            "|    clip_fraction        | 0.0128       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.52        |\n",
            "|    explained_variance   | 0.314        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.74         |\n",
            "|    n_updates            | 702          |\n",
            "|    policy_gradient_loss | -0.00212     |\n",
            "|    std                  | 0.853        |\n",
            "|    value_loss           | 28.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -4.758587    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 732          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 917          |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021194853 |\n",
            "|    clip_fraction        | 0.0131       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.52        |\n",
            "|    explained_variance   | 0.34         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.43         |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | -0.00209     |\n",
            "|    std                  | 0.856        |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -5.3912716 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 732        |\n",
            "|    iterations           | 42         |\n",
            "|    time_elapsed         | 939        |\n",
            "|    total_timesteps      | 688128     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00409858 |\n",
            "|    clip_fraction        | 0.0375     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.51      |\n",
            "|    explained_variance   | 0.589      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0932     |\n",
            "|    n_updates            | 738        |\n",
            "|    policy_gradient_loss | -0.00289   |\n",
            "|    std                  | 0.843      |\n",
            "|    value_loss           | 0.24       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -4.4733596   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 733          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 959          |\n",
            "|    total_timesteps      | 704512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035919873 |\n",
            "|    clip_fraction        | 0.0351       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.49        |\n",
            "|    explained_variance   | 0.715        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0829       |\n",
            "|    n_updates            | 756          |\n",
            "|    policy_gradient_loss | -0.00241     |\n",
            "|    std                  | 0.838        |\n",
            "|    value_loss           | 0.224        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -4.451964    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 733          |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 982          |\n",
            "|    total_timesteps      | 720896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031210142 |\n",
            "|    clip_fraction        | 0.0212       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.47        |\n",
            "|    explained_variance   | 0.827        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0461       |\n",
            "|    n_updates            | 774          |\n",
            "|    policy_gradient_loss | -0.00175     |\n",
            "|    std                  | 0.825        |\n",
            "|    value_loss           | 0.0976       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -5.101742    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 735          |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 1002         |\n",
            "|    total_timesteps      | 737280       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048518106 |\n",
            "|    clip_fraction        | 0.0519       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.43        |\n",
            "|    explained_variance   | 0.855        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0259       |\n",
            "|    n_updates            | 792          |\n",
            "|    policy_gradient_loss | -0.0045      |\n",
            "|    std                  | 0.811        |\n",
            "|    value_loss           | 0.0698       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -4.7821302   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 735          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 1025         |\n",
            "|    total_timesteps      | 753664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048955795 |\n",
            "|    clip_fraction        | 0.0453       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.4         |\n",
            "|    explained_variance   | 0.884        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0244       |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00478     |\n",
            "|    std                  | 0.799        |\n",
            "|    value_loss           | 0.073        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -4.157231   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 735         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 1046        |\n",
            "|    total_timesteps      | 770048      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004727496 |\n",
            "|    clip_fraction        | 0.0312      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.38       |\n",
            "|    explained_variance   | 0.911       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0184      |\n",
            "|    n_updates            | 828         |\n",
            "|    policy_gradient_loss | -0.00329    |\n",
            "|    std                  | 0.794       |\n",
            "|    value_loss           | 0.0492      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -4.2820363   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 735          |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 1069         |\n",
            "|    total_timesteps      | 786432       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042381613 |\n",
            "|    clip_fraction        | 0.0377       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.36        |\n",
            "|    explained_variance   | 0.881        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0136       |\n",
            "|    n_updates            | 846          |\n",
            "|    policy_gradient_loss | -0.00372     |\n",
            "|    std                  | 0.78         |\n",
            "|    value_loss           | 0.0411       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -4.2873654  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 736         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 1090        |\n",
            "|    total_timesteps      | 802816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004688822 |\n",
            "|    clip_fraction        | 0.0457      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.33       |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0187      |\n",
            "|    n_updates            | 864         |\n",
            "|    policy_gradient_loss | -0.00411    |\n",
            "|    std                  | 0.772       |\n",
            "|    value_loss           | 0.0475      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.6329029  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 736         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 1112        |\n",
            "|    total_timesteps      | 819200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003490952 |\n",
            "|    clip_fraction        | 0.03        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.31       |\n",
            "|    explained_variance   | 0.893       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0173      |\n",
            "|    n_updates            | 882         |\n",
            "|    policy_gradient_loss | -0.00278    |\n",
            "|    std                  | 0.762       |\n",
            "|    value_loss           | 0.0428      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.6665046  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 736         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1134        |\n",
            "|    total_timesteps      | 835584      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003543976 |\n",
            "|    clip_fraction        | 0.0252      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.27       |\n",
            "|    explained_variance   | 0.911       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00621     |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.00305    |\n",
            "|    std                  | 0.746       |\n",
            "|    value_loss           | 0.026       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.745476   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 736         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1156        |\n",
            "|    total_timesteps      | 851968      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004941358 |\n",
            "|    clip_fraction        | 0.04        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.23       |\n",
            "|    explained_variance   | 0.919       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00657     |\n",
            "|    n_updates            | 918         |\n",
            "|    policy_gradient_loss | -0.00268    |\n",
            "|    std                  | 0.735       |\n",
            "|    value_loss           | 0.0304      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -6.1107774   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 738          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 1176         |\n",
            "|    total_timesteps      | 868352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054877037 |\n",
            "|    clip_fraction        | 0.0558       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.21        |\n",
            "|    explained_variance   | 0.934        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.00305      |\n",
            "|    n_updates            | 936          |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    std                  | 0.729        |\n",
            "|    value_loss           | 0.0218       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -3.5751333   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 738          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 1198         |\n",
            "|    total_timesteps      | 884736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020477958 |\n",
            "|    clip_fraction        | 0.0213       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.21        |\n",
            "|    explained_variance   | 0.35         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16         |\n",
            "|    n_updates            | 954          |\n",
            "|    policy_gradient_loss | -0.00162     |\n",
            "|    std                  | 0.734        |\n",
            "|    value_loss           | 5.77         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.1052017  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 738         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 1219        |\n",
            "|    total_timesteps      | 901120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005827612 |\n",
            "|    clip_fraction        | 0.0461      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.19       |\n",
            "|    explained_variance   | 0.845       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0113      |\n",
            "|    n_updates            | 972         |\n",
            "|    policy_gradient_loss | -0.00288    |\n",
            "|    std                  | 0.719       |\n",
            "|    value_loss           | 0.0321      |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'ppo_models/2023-05-05_18-34-36' does not exist. Will create it.\n",
            "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and evaluation\n",
        "\n",
        "The total number of points for Part 2 is 7.5. We will evaluate your trained model on 5 random goal locations. For each test, we assign points based on the distance between the end effector and the goal location at the end of the episode.\n",
        "\n",
        "- If 0 < distance < 0.05, you get 1.5 points.\n",
        "- If 0.05 <= distance < 0.1, you get 1 point.\n",
        "- If distance >= 0.1, you get 0 point.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9N2falIz9rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import score_policy\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from render import Renderer\n",
        "import time\n",
        "\n",
        "# Set the path to your model\n",
        "model_path = '/content/ppo_models/2023-05-05_18-34-36/ppo_network.zip'\n",
        "\n",
        "set_random_seed(seed=100)\n",
        "\n",
        "# Create arm robot\n",
        "arm = make_arm()\n",
        "\n",
        "# Create environment\n",
        "env = ArmEnv(arm, gui=False)\n",
        "env.seed(100)\n",
        "\n",
        "# Load and test policy\n",
        "policy = PPO.load(model_path)\n",
        "score_policy(policy, env)"
      ],
      "metadata": {
        "id": "X6eQ2mzglwd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56e61bd-570a-45e3-e279-49bdc23fd2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Computing score ---\n",
            "\n",
            "Goal 1: 1.5\n",
            "\n",
            "Goal 2: 1.5\n",
            "\n",
            "Goal 3: 1.5\n",
            "\n",
            "Goal 4: 1.5\n",
            "\n",
            "Goal 5: 1.5\n",
            "\n",
            "\n",
            "---\n",
            "Final score: 7.5/7.5\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.5"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2z_ROdXlULe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}